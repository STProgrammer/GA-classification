{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded assignment 1 - text classification using Genetic Algorithms\n",
    "In this assignment we'll make a binary text classifier using neural networks and train it using genetic algorithms. We will classify movie reviews from IMDB as either negative or positive. \n",
    "\n",
    "The assignment text contain 3 steps, so I divided the assignment in 3 sections:\n",
    "\n",
    "1. Preprocessing of the text\n",
    "2. Genetich Algorithm\n",
    "3. Testing\n",
    "\n",
    "**Preprocessing text**\n",
    "\n",
    "First I uploaded all data and put them together. Then I explore the dataset, explore the use of vocabulary distribution, review length distribution.\n",
    "\n",
    "After analyzing data I split them into training, validation and testing set. Then I preprocessed the text using different methods like removing stop words, lemmatization etc.\n",
    "\n",
    "Training set will be used to build vocabulary and IDF values. Validation set will be used when assigning fitness in Genetic Algorithm which I'll get into more. Testing set will be used at the end in validation part for a final evaluation.\n",
    "\n",
    "What we basically doing here is to try to predict what class a text belongs to based on how many of which words the text contains. Our classifier must recognize that some words convey positive sentiment, some convey negative sentiment and some make no difference. The latter words don't help with predictions. That's why we do preprocessing and turn all the reviews into equally sized vectors representing the frequency and importance of each word which can be proceeded by our neural network. How we do the preprocessing will be further explained later.\n",
    "\n",
    "\n",
    "I have built two classes for preprocessing stage. One is TextPreprocessor which processes contents with methods like removing stop words, lemmatization etc.\n",
    "\n",
    "The other is TF-IDF vectorizer class which is used to build a Numpy array of TF-IDF vectors. Each document is turned into a TF-IDF vector.\n",
    "\n",
    "**Genetic Algorithm**\n",
    "Genetic algorithm  (often referred as GA) is an algorithm inspired by evolution theory. Here rather than species we have solutions to a problem. We call those solutions for Chromosomes. In this case Chromosomes are just weights for a neural network. We use methods like \"cross-over\" and \"mutation\" to make changes in those Chromosomes, and we try to pick the best Chromosome. In here the best Chromosome is \n",
    "\n",
    "The classifier I used is a simple neural network. I considered decision tree, but I'm not familiar with decision trees while I'm a bit familiar with neural network. Thus I decided to go with neural network. \n",
    "\n",
    "The neural net has only 2 hidden layers, 16 and 8 hidden nodes. I kept it simple because I had to take hardware limitations into account too. In early trials, this architecture seemed more than capable of learning from the training set. In the Genetic Algorithm I will train with the trainign dataset but I will calculate fitness using the validation set for each classifier. This will avoid overfitting the training set.\n",
    "\n",
    "**Testing**\n",
    "The validation data is simply using the optimizied weights to predict all reviews in the testing set to see how our GA performs - that is, to determine its fitness. In the assignment text they suggested to use testing set for assigning fitness in GA and in validation part. But I thought that using the same data for GA improvement and evaluation would led to overfitting. Thus, other than training set, I used two separate sets: validation set and testing set. Validation set will be used when assigning fitness while optimizing weights in GA. Testing set will be used at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "The preprocessing consists of two parts. Preproceesing the texts in the dataframe, and then converting the texts into vectors that can be used in classifier.\n",
    "\n",
    "I have built twp seperate classes for each one. TextPreprocessor class and TF-IDF vectorizer.\n",
    "\n",
    "I searched online about different vectorizartion methods in text classifying. I found TF-IDF vectorization contains more data than others, which could help in classifying. With TF-IDF vectors one can see which words are more \"special\" than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Uploading data\n",
    "\n",
    "All the data is placed in four seperate folders. I found PlainTextCorpusReader from nlkt.corpus to be more effective in reading multiple text files. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Upload the text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from scipy import special\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used for loop iteration to extract the info as array. Then I made a dataframe where I put all the samples together.\n",
    "\n",
    "The data contains 3 columns (among index): reviews (the content of TXT files), labels (positive or negative, 1 or 0), and rates (extracted from file names).\n",
    "\n",
    "I haven't used rate values anywhere in the assignment. I planned to use them but didn't use them. But still I didn't remove them so the code can be further developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File directories\n",
    "corpus_train_pos_root = 'aclImdb/train/pos/'\n",
    "corpus_train_neg_root = 'aclImdb/train/neg/'\n",
    "corpus_test_pos_root = 'aclImdb/test/pos/'\n",
    "corpus_test_neg_root = 'aclImdb/test/neg/'\n",
    "\n",
    "# Corpus file objects\n",
    "files_train_pos = PlaintextCorpusReader(corpus_train_pos_root, '.*')\n",
    "files_train_neg = PlaintextCorpusReader(corpus_train_neg_root, '.*')\n",
    "files_test_pos = PlaintextCorpusReader(corpus_test_pos_root, '.*')\n",
    "files_test_neg = PlaintextCorpusReader(corpus_test_neg_root, '.*')\n",
    "\n",
    "\n",
    "# Getting review texts, labels and rates all in arrays\n",
    "reviews_train_pos = [files_train_pos.open(n).read() for n in files_train_pos.fileids()]\n",
    "rates_train_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_pos.fileids()]\n",
    "labels_train_pos = [1] * len(reviews_train_pos)\n",
    "\n",
    "reviews_train_neg = [files_train_neg.open(n).read() for n in files_train_neg.fileids()]\n",
    "rates_train_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_neg.fileids()]\n",
    "labels_train_neg = [0] * len(reviews_train_neg)\n",
    "\n",
    "reviews_test_pos = [files_test_pos.open(n).read() for n in files_test_pos.fileids()]\n",
    "rates_test_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_pos.fileids()]\n",
    "labels_test_pos = [1] * len(reviews_test_pos)\n",
    "\n",
    "reviews_test_neg = [files_test_neg.open(n).read() for n in files_test_neg.fileids()]\n",
    "rates_test_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_neg.fileids()]\n",
    "labels_test_neg = [0] * len(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_set = pd.DataFrame()\n",
    "\n",
    "# Puttin all data into whole set\n",
    "whole_set['review'] = reviews_train_pos + reviews_test_pos + reviews_train_neg + reviews_test_neg\n",
    "whole_set['rate'] = rates_train_pos + rates_test_pos + rates_train_neg + rates_test_neg\n",
    "whole_set['label'] = labels_train_pos + labels_test_pos + labels_train_neg + labels_test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data exploration\n",
    "\n",
    "Here we explore how the data looks like. That is to figure out how to preprocess the data, what to eliminate etc. We start it by looking at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18988</th>\n",
       "      <td>We just saw this movie in Austin Texas at the ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31848</th>\n",
       "      <td>I love a good sappy love story (and I'm a guy)...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Trio's vignettes were insightful and quite enj...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606</th>\n",
       "      <td>Legendary movie producer Walt Disney brought t...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36620</th>\n",
       "      <td>First off, I knew nothing about 'Mazes and Mon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label\n",
       "18988  We just saw this movie in Austin Texas at the ...    10      1\n",
       "31848  I love a good sappy love story (and I'm a guy)...     1      0\n",
       "365    Trio's vignettes were insightful and quite enj...     8      1\n",
       "4606   Legendary movie producer Walt Disney brought t...    10      1\n",
       "36620  First off, I knew nothing about 'Mazes and Mon...     1      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_set.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Most common words\n",
    "\n",
    "We print the most common words both in terms of total frequency and frequency in terms of number of reviews it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 20 counted by appearance in nr of reviews:  [('the', 49298), ('a', 48048), ('and', 47886), ('of', 47322), ('to', 46839), ('is', 44366), ('in', 43163), ('this', 41446), ('that', 38943), ('it', 37931), ('I', 35924), ('for', 34503), ('with', 33960), ('but', 32072), ('was', 31866), ('The', 31164), ('as', 30498), ('on', 29824), ('/><br', 29200), ('have', 28109)]\n",
      "\n",
      "Most common 20 counted by word count total:  [('the', 568735), ('a', 306960), ('and', 301919), ('of', 283625), ('to', 261850), ('is', 203056), ('in', 169981), ('I', 132498), ('that', 126818), ('this', 113726), ('it', 107916), ('/><br', 100974), ('was', 92658), ('as', 83130), ('with', 82569), ('for', 80919), ('The', 67317), ('but', 66282), ('on', 61197), ('movie', 60762)]\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "# Most common words\n",
    "\n",
    "cnt = Counter()\n",
    "cnt2 = Counter()\n",
    "for text in whole_set[\"review\"].values:\n",
    "    # Counting the words\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "    # Counting in how many reviews the word appears\n",
    "    for word in set(text.split()):\n",
    "        cnt2[word] += 1\n",
    "\n",
    "print(\"Most common 20 counted by appearance in nr of reviews: \", cnt2.most_common(20))\n",
    "print(\"\\nMost common 20 counted by word count total: \", cnt.most_common(20))\n",
    "\n",
    "# print(\"Least common 20 counted by appearance in nr of reviews: \", cnt2.most_common()[:-20])\n",
    "# print(\"\\nLeast common 20 counted by word count total: \", cnt.most_common()[:-20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen there are a lot of stop words. There are HTML tags in the set too. The stop words and HTML tags will be removed in with TextPreprocessor class.\n",
    "\n",
    "Vocabulary here means all the words used in the training set. The vocabulary will be based on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Plot vocabulary distribution\n",
    "\n",
    "Here we plot what % of the reviews use what % of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '% of documents')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What % of the reviews use what % of the vocab\n",
    "vocab_size = len(cnt2)\n",
    "sample_size = len(whole_set)\n",
    "\n",
    "y = [c/sample_size * 100 for (w, c) in cnt2.most_common()]\n",
    "x = [c/vocab_size * 100 for c in range(1, vocab_size+1)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"% of the words used in % of the documents\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"% of vocabolary\")\n",
    "ax.set_ylabel(\"% of documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows how much % of the vocabulary is used by how much % of the documents.\n",
    "\n",
    "We see that only a tiny fraction of the vocubulary are used in most of the documents. \n",
    "Most of the words in vocabulary are rare words, also they're used only in a small fraction of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Reviews length distribution\n",
    "\n",
    "Here we plot histogram of lengthts of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length distribution of the reviews\n",
    "whole_set['rev_lens_raw'] = whole_set['review'].str.len()\n",
    "whole_set['rev_lens_words'] = whole_set['review'].str.split().apply(len)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "ax[0].hist(whole_set['rev_lens_raw'], bins='auto')\n",
    "ax[0].set_xlabel(\"Review length in characters\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[1].hist(whole_set['rev_lens_words'], bins='auto')\n",
    "ax[1].set_xlabel(\"Review length in words\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above shows distribution of frequency and review length. The longest reviews are 1000 words which is not very unusual. \n",
    "\n",
    "We want to see if short reviews contain garbage, and to check they look like normal movie reviews.\n",
    "\n",
    "We might consider removing the very long reviews as these use many words of positive and negative sentiment and can be very nebulous and therefore offer poor training signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. See some samples\n",
    "\n",
    "We want to see if short reviews contain garbage, and see they look like normal movie reviews.\n",
    "\n",
    "We want to see some samples too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Primary plot!Primary direction!Poor interpretation.',\n",
       "       'Read the book, forget the movie!',\n",
       "       'I hope this group of film-makers never re-unites.',\n",
       "       'More suspenseful, more subtle, much, much more disturbing....',\n",
       "       'What a script, what a story, what a mess!',\n",
       "       'Brilliant and moving performances by Tom Courtenay and Peter Finch.',\n",
       "       'This movie is terrible but it has some good effects.',\n",
       "       \"I wouldn't rent this one even on dollar rental night.\",\n",
       "       \"You'd better choose Paul Verhoeven's even if you have watched it.\",\n",
       "       'Adrian Pasdar is excellent is this film. He makes a fascinating woman.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shortest reviews\n",
    "whole_set.nsmallest(10, 'rev_lens_words')['review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop reviews that are very long\n",
    "#train_set = train_set.loc[train_set['rev_lens_word'] < 1800]\n",
    "#train_set.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Splitting data\n",
    "\n",
    "The original sample was split 50% was testing and 50% for training. That's too little for training and too much for testing. And there's no validation dataset.\n",
    "\n",
    "Thus I made the split into three: \n",
    "* 70 % on training, \n",
    "* 15 % on testing\n",
    "* 15 % on validation.\n",
    "\n",
    "Splitting the data into three sets where training set is significantly bigger is common in machine learning.\n",
    "\n",
    "TF-IDF vectors will be based on training set. Thus we need training set to be in bigger size.\n",
    "\n",
    "Validation set will be used in Genetic Algorithm when assigning fitness to classifiers.\n",
    "Too big validation set could slow it the GA much.\n",
    "\n",
    "Testing set will be used at the end to test how much the \"trained classifier\" performs.\n",
    "\n",
    "Then I shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "      <th>rev_lens_raw</th>\n",
       "      <th>rev_lens_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I did not watch the entire movie. I could not ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1624</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes, it can be done. John De Bello and Costa D...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>762</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Too bad Chuck Norris has gone to TV. He made s...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kubrick proved his brilliantness again, now in...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1704</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sorry, gave it a 1, which is the rating I give...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>I was expecting a B-Movie French musical. Afte...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>729</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>This is one of my all time favorites.&lt;br /&gt;&lt;br...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>639</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>This movie completely ran laps around the orig...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>885</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>\"Convicts\" is very much a third act sort of fi...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1701</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>Be warned: Neither Zeta-Jones nor McGregor pla...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35000 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label  \\\n",
       "0      I did not watch the entire movie. I could not ...     3      0   \n",
       "1      Yes, it can be done. John De Bello and Costa D...     2      0   \n",
       "2      Too bad Chuck Norris has gone to TV. He made s...     4      0   \n",
       "3      Kubrick proved his brilliantness again, now in...    10      1   \n",
       "4      Sorry, gave it a 1, which is the rating I give...     2      0   \n",
       "...                                                  ...   ...    ...   \n",
       "34995  I was expecting a B-Movie French musical. Afte...     1      0   \n",
       "34996  This is one of my all time favorites.<br /><br...    10      1   \n",
       "34997  This movie completely ran laps around the orig...    10      1   \n",
       "34998  \"Convicts\" is very much a third act sort of fi...     7      1   \n",
       "34999  Be warned: Neither Zeta-Jones nor McGregor pla...     2      0   \n",
       "\n",
       "       rev_lens_raw  rev_lens_words  \n",
       "0              1624             283  \n",
       "1               762             139  \n",
       "2               231              46  \n",
       "3              1704             291  \n",
       "4               213              44  \n",
       "...             ...             ...  \n",
       "34995           729             133  \n",
       "34996           639             111  \n",
       "34997           885             161  \n",
       "34998          1701             287  \n",
       "34999           595             102  \n",
       "\n",
       "[35000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting all into two Pandas dataframes - training set and testing set\n",
    "train_set = pd.DataFrame()\n",
    "test_set = pd.DataFrame()\n",
    "valid_set = pd.DataFrame()\n",
    "\n",
    "# Dividing reviews into negative and positive to make sure data is always balanced\n",
    "negatives = whole_set.loc[whole_set['label'] == 0]\n",
    "positives = whole_set.loc[whole_set['label'] == 1]\n",
    "\n",
    "# Splitting positive and negative reviews\n",
    "train_set, valid_set, test_set = np.split(positives, [int(0.7*len(positives)), int(0.85*len(positives))])\n",
    "tr_neg, vl_neg, ts_neg = np.split(negatives, [int(0.7*len(negatives)), int(0.85*len(negatives))])\n",
    "\n",
    "# Appending negatives to positives\n",
    "train_set = train_set.append(tr_neg)\n",
    "valid_set = valid_set.append(vl_neg)\n",
    "test_set = test_set.append(ts_neg)\n",
    "\n",
    "# Shuffle and reset the index\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "valid_set = valid_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocessing and and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerocessing and vectorization serves differ goals. The goal of preprocessing is to clean and normalize the data to best reflect its sentiment by eliminating noise. The purpose of vectorization is to present a uniform numerical representation of text data that is fit for consumption by our neural network classifier. \n",
    "\n",
    "What we basically want to do here is to try to predict what class a text belongs to by making some calculations based on how many of which words in the vocabulary that particular text contains. \n",
    "\n",
    "So we want to build a vocabulary first, a list of words in the training set. But we don't want to include absolutely all words used in the training set to be included in the vocabulary. Thus we make some preprocessing, some filtering etc. and that's where TextPreprocessor comes in.\n",
    "\n",
    "E.g. stopwords like \"the\", \"a\", \"and\" etc. don't say anything about whether the review is positive or negative. Thus in the preprocessing stage we eliminate them. We eliminate numbers and other words too that don't give meaning.\n",
    "\n",
    "Very common words, words that are used in almost all reviews, don't say much about if the review is negative or positive. So we eliminate most common words too. But very rare words also don't say much about \n",
    "\n",
    "To keep variations of the same word inflating the vocabulary size (and reducing the performance of our classifier) we will use lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 TextPreprocessor class\n",
    "\n",
    "Most of the functions are in this class are taken from a code posted on Canvas. I just put hem into a class, made some changes and built some additional functions.\n",
    "\n",
    "Preprocessing is done with a function in TextProcessor class named def `preprocess_imdb_reviews()` which uses other functions in the class. It processes all the text in a dataframe under column \"review\" and returns a dataframe with additional column named \"processed\" where all processed text are saved. I keep both of them to be able to repeat code and to make a comparison later.\n",
    "\n",
    "The functions are applied by using .apply() function of the dataframe, which apply a function to each row in a chosen column.\n",
    "\n",
    "As mentioned we don't want to include all words used in the training set to be included in the vocabulary. Thus we make some changes. Here is how we process the text:\n",
    "1. We make all lower cases. Some people use upper case to highlight their words, and first letter of words are in upper case after periods. All all those words mean the same thing, so we don't want to discriminate between those. The only problem here is that movie names and person names would be processed just like some words. E.g. the movie \"Cars\" would be interpreted as noun in plural form cars, and eventually be lemmatized into car or removed. But it's very difficult to write a code to determine what is movie / person name or not, and they don't say much about whether a movie review is positive or negative. Thus for now we just ignore this problem. Lower casing all words also makes other processing steps easier.\n",
    "2. We remove HTML tags. They are not words with meaning, just some clutter to be removed.\n",
    "5. We remove URLs. They are not words with meaning, so just remove them.\n",
    "3. We convert emoticons into text. We do this before removing numbers or anything else as emoticons may contain numbers too. We don't want to remove them as they may contain useful information about the review.\n",
    "4. We remove numbers. They can be dates, ages etc. which don't give much relevant info about the review.\n",
    "6. We expand contractions. Alos \"don't\" means same as \"do not\". So we don't want to distinguish between those. Expanding contractions also will make removing stop words easier. It's important to do it early as it will generate new stop words.\n",
    "7. We remove emojis\n",
    "8. We remove punctuations, like dots and slashes. In computers \"something\" and \"something,\" are perceived as two different words, we don't want that.\n",
    "9. We lemmatize words. Lemmatizing means converting words to base forms.E.g. \"boring\" and \"bored\" are same words, \"child\" and \"children\" are actually same word. We want the computer to perceive them as same words too. Words with same base should be considered as same word.\n",
    "10. We remove the stop words. Words like \"the\", \"a\", \"and\" may be used in all reviews and don't give any information about review. So we just remove them.\n",
    "11. Curbing. We remove the words that exists in more than 85 % of the documents. They are some common words that don't give much context. And we remove the words that exists in less than 0.05 % of the documents. Rare words are words like special movie names, actor names, weirdo words, misspesllings etc. Words that are used only here and there or only in one review can't be used in calculating which class a reviews belongs to. The rates 0.85 and 0.0005 are paramaters to the function. I experimented with different values, I don't want to end up with vocabulary that's too little, but nor too big. Most of the words in vocabulary are rare words that are used in tiny fraction of the set. Thus the bottom curb is just 0.0005. Too little vocabulary give too little information. Too big vocabulary is too heavy for the neural network, and may contain a lot of words that give little information. A vocabulary of 10k-20k is enough. After experiemting with different values I did choose 0.85 and 0.0005 as curbing values.\n",
    "12. At the end I found some garbage words still existing in the set. I made an array of those and removed those from all the reviews. The array is: [\"\", \"\", \"\", \"\\x96\", \"st\", \"nd\", \"rd\", \"th\"]. \\x96 is just some triangle. st, nd, rd and th are remainders of numerical values like 1st, 2dn, 3rd, 4th etc. which is meaningless.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I think to put this into own .PY file and import from there\n",
    "class TextPreprocessor():\n",
    "    \"\"\"\n",
    "    This is a class for processing text and text in Pandas Dataframes\n",
    "      \n",
    "    Attributes:\n",
    "        stop_words (set): list of stop words from nltk library\n",
    "        punctuation: list of punctuations from 'strin' library\n",
    "        emoji_pattern (list): a compiled emoji patternt for processing emojis\n",
    "        emoticons (list): a dictionary of emoticons and descriptive text\n",
    "        url_pattern: re.compile of URL pattern for removing URLs\n",
    "        html_pattern: re.compile of HTML pattern for removing HTML tags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor for TextPreprocessor class\n",
    "        \"\"\"\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.punctuation = string.punctuation\n",
    "        \n",
    "        self.emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        self.emoticons = {\n",
    "            u\":\\)\":\"Happy face or smiley\",\n",
    "            u\":\\)\":\"Happy face or smiley\",\n",
    "            u\":-\\]\":\"Happy face or smiley\",\n",
    "            u\":\\]\":\"Happy face or smiley\",\n",
    "            u\":-3\":\"Happy face smiley\",\n",
    "            u\":3\":\"Happy face smiley\",\n",
    "            u\":->\":\"Happy face smiley\",\n",
    "            u\":>\":\"Happy face smiley\",\n",
    "            u\"8-\\)\":\"Happy face smiley\",\n",
    "            u\":o\\)\":\"Happy face smiley\",\n",
    "            u\":-\\}\":\"Happy face smiley\",\n",
    "            u\":\\}\":\"Happy face smiley\",\n",
    "            u\":-\\)\":\"Happy face smiley\",\n",
    "            u\":c\\)\":\"Happy face smiley\",\n",
    "            u\":\\^\\)\":\"Happy face smiley\",\n",
    "            u\"=\\]\":\"Happy face smiley\",\n",
    "            u\"=\\)\":\"Happy face smiley\"\n",
    "        }\n",
    "        \n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.html_pattern = re.compile('<.*?>')\n",
    "\n",
    "    def lower_case(self, text):\n",
    "        \"\"\"\n",
    "        Lowercase all characters in the string and returns the string.\n",
    "        \"\"\"\n",
    "        return str.lower(text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"\n",
    "        Removes all punctuations in the string and returns the string.\n",
    "        \"\"\"\n",
    "        return text.translate(str.maketrans('', '', self.punctuation))\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"\n",
    "        Removes all stop words in the string and returns the string.\n",
    "        \"\"\"\n",
    "        return \" \".join([word for word in text.split() if word not in self.stop_words])\n",
    "    \n",
    "    def remove_words(self, text, words_list):\n",
    "        \"\"\"\n",
    "        Removes words determined in the list from the string and returns the string back.\n",
    "  \n",
    "        Parameters:\n",
    "            text (string): a string to be processed\n",
    "            words_list (list): a list that containts list of words that'll be removed from the string\n",
    "          \n",
    "        Returns:\n",
    "            string: the processed text where the words in the list are removed\n",
    "        \"\"\"\n",
    "        \n",
    "        return \" \".join([word for word in text.split() if word not in words_list])\n",
    "    \n",
    "    def remove_emoji(self, text):\n",
    "        \"\"\"\n",
    "        Removes all emoji in the string and returns the string.\n",
    "        \"\"\"\n",
    "        # src: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "        return self.emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    def remove_emoticons(self, text):\n",
    "        \"\"\"\n",
    "        Removes all emoticons in the string and returns the string.\n",
    "        \"\"\"\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in self.emoticons) + u')')\n",
    "        return emoticon_pattern.sub(r'', text)\n",
    "    \n",
    "    def convert_emoticons(self, text):\n",
    "        \"\"\"\n",
    "        Converts all emoticons to descriptive text and returns the text back\n",
    "        \n",
    "        How it converts can be found here: https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        \"\"\"\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        for emot in self.emoticons:\n",
    "            text = re.sub(u'('+emot+')', \"_\".join(self.emoticons[emot].replace(\",\",\"\").split()), text)\n",
    "        return text\n",
    "    \n",
    "    def lemmatization(self, text):\n",
    "        \"\"\"\n",
    "        Lemmatizes each verbs, nouns and adverts in the string and returns the string back\n",
    "        \n",
    "        E.g. it turns \"cars\" into \"car\", \"children\" to \"child\". \n",
    "        All words are converted to their base form.\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = word_tokenize(text)\n",
    "        for i in ['v','n','a']:\n",
    "            tokens = [lemmatizer.lemmatize(word, i) for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"\n",
    "        Expands all contractions in the string and returns the string back\n",
    "        \n",
    "        E.g. \"i'm\" converted to \"i am\". \"haven't\" converted to \"have not\".\n",
    "        All characters must be in lower case for the function to work.\n",
    "        The text needs to be processed with lower_case() function first\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"i'm\", \" i am \", text)\n",
    "        text = re.sub(r\" im \", \" i am \", text)\n",
    "        text = re.sub(r\"\\: p\", \"\", text)\n",
    "        text = re.sub(r\" ive \", \" i have \", text)\n",
    "        text = re.sub(r\" he's \", \" he is \", text)\n",
    "        text = re.sub(r\" she's \", \" she is \", text)\n",
    "        text = re.sub(r\" that's \", \" that is \", text)\n",
    "        text = re.sub(r\" what's \", \" what is \", text)\n",
    "        text = re.sub(r\" where's \", \" where is \", text)\n",
    "        text = re.sub(r\" haven't \", \" have not \", text)\n",
    "        text = re.sub(r\" ur \", \" you are \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\" won't \", \" will not \", text)\n",
    "        text = re.sub(r\" wouldn't \", \" would not \", text)\n",
    "        text = re.sub(r\" can't \", \" cannot \", text)\n",
    "        text = re.sub(r\" couldn't \", \" could not \", text)\n",
    "        text = re.sub(r\" don't \", \" do not \", text)\n",
    "        text = re.sub(r\" didn't \", \" did not \", text)\n",
    "        text = re.sub(r\" doesn't \", \" does not \", text)\n",
    "        text = re.sub(r\" isn't \", \" is not \", text)\n",
    "        text = re.sub(r\" it's \", \" it is \", text)\n",
    "        text = re.sub(r\" who's \", \" who is \", text)\n",
    "        text = re.sub(r\" there's \", \" there is \", text)\n",
    "        text = re.sub(r\" weren't \", \" were not \", text)\n",
    "        text = re.sub(r\" wasn't \", \" was not \", text)\n",
    "        text = re.sub(r\" ok \", \" okay \", text)\n",
    "        text = re.sub(r\" you're \", \" you are \", text)\n",
    "        text = re.sub(r\" c'mon \", \" come on \", text)\n",
    "        text = re.sub(r\"\\'s\", \" s\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        \"\"\"\n",
    "        Removes all the numbers from the string and returns the string back\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"\n",
    "        Removes all HTML tags from the string and returns the string back\n",
    "        \"\"\"\n",
    "        return self.html_pattern.sub(r'', text)\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        \"\"\"\n",
    "        Removes all the URLs from the string and returns the string back\n",
    "        \"\"\"\n",
    "        return self.url_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    # preprocessing IMDB reviews\n",
    "    def preprocess_imdb_reviews(self, df, max_df, min_df):\n",
    "        \"\"\"\n",
    "        Preprocessing IMDB reviews saved in a Pandas Dataframe \n",
    "        and returns a Dataframe with additional column.\n",
    "        \n",
    "        It preprocesses IMDB reviews by lower casing all letter, removing HTML tags, \n",
    "        emojis, numbers, converting emoticons to descriptive text, removes punctioation, \n",
    "        stop words and lemmatization. Then it removes most and least frequent words based\n",
    "        on percentage of appearerance in the dataframe. The percentages are passed as\n",
    "        parameters as rates between 0 and 1. Then it removes some more garbage words.\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pands Dataframe): a Pandas Datframe. The strings must be in column named 'review'\n",
    "            max_df (float): A rate between 0-1. To remove words that appear more than max_df of the rows\n",
    "            min_df (float): A rate between 0-1. To remove words that appear less than min_df of the rows\n",
    "          \n",
    "        Returns:\n",
    "            Pandas Dataframe: The same Pandas Dataframe with additional column named 'processed' where all\n",
    "            processed text are saved.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df['processed'] = df['review'].apply(lambda text: self.lower_case(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_html_tags(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_urls(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.convert_emoticons(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_numbers(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.expand_contractions(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_emoji(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_punctuation(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.lemmatization(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_stopwords(text))\n",
    "\n",
    "\n",
    "        \n",
    "        cnt2 = Counter()\n",
    "        for text in df['processed'].values:\n",
    "            # Counting the words\n",
    "            for word in set(text.split()):\n",
    "                cnt2[word] += 1\n",
    "\n",
    "\n",
    "        # Remove words used in >max_df% and <min_df% of the reviews\n",
    "        curb_max_amount = len(df) * max_df\n",
    "        curb_min_amount = len(df) * min_df\n",
    "        curb_words = set([w for (w, wc) in cnt2.most_common() if wc > curb_max_amount or wc < curb_min_amount])\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, curb_words))\n",
    "        \n",
    "        # Other words to remove\n",
    "        rem_words = [\"\\x96\", \"st\", \"nd\", \"rd\", \"th\"]\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, rem_words))\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.4.3 Preprocessing\n",
    "\n",
    "Here we preprocess training set, testing set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Using the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "train_set_processed = preprocessor.preprocess_imdb_reviews(train_set, 0.85, 0.0005)\n",
    "test_set_processed = preprocessor.preprocess_imdb_reviews(test_set, 0.85, 0.0005)\n",
    "valid_set_processed = preprocessor.preprocess_imdb_reviews(valid_set, 0.85, 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 Checking preprocessed reviews\n",
    "\n",
    "Here we take a quick look into the preprocessed reviews again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "# Most common words\n",
    "\n",
    "cnt = Counter()\n",
    "cnt2 = Counter()\n",
    "for text in train_set_processed[\"processed\"].values:\n",
    "    # Counting the words\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "    # Counting in how many reviews the word appears\n",
    "    for word in set(text.split()):\n",
    "        cnt2[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(\"Most common 50 counted by appearance in nr of reviews: \", cnt1.most_common(50))\n",
    "\n",
    "vocab_size = len(cnt2)\n",
    "sample_size = len(train_set)\n",
    "\n",
    "x = [c/sample_size * 100 for (w, c) in cnt2.most_common()]\n",
    "y = [c/vocab_size * 100 for c in range(1, vocab_size+1)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"% of the words used in % of the documents\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"% of vocabolary\")\n",
    "ax.set_ylabel(\"% of documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above we see that after preprocessing the distribution of vocabulary in the documents are more smooth.\n",
    "\n",
    "Below we see some random samples to check if the data is garbage or not. And we see it looks OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "      <th>rev_lens_raw</th>\n",
       "      <th>rev_lens_words</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>fascinating look at fascist italy and the peop...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>50</td>\n",
       "      <td>fascinate look fascist italy people carve life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32982</th>\n",
       "      <td>FBI Agents Mulder and Scully get assigned to p...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1784</td>\n",
       "      <td>298</td>\n",
       "      <td>fbi agent mulder scully get assign probe myste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15073</th>\n",
       "      <td>As much as I like Japanese movies this one did...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>928</td>\n",
       "      <td>168</td>\n",
       "      <td>much like japanese movie one cut movie suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31243</th>\n",
       "      <td>This is absolutely the best none-animated fami...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>47</td>\n",
       "      <td>absolutely best family film see quite back fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31756</th>\n",
       "      <td>I cannot believe how uneducated this movie is....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>634</td>\n",
       "      <td>111</td>\n",
       "      <td>believe uneducated movie like watch police aca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>question: how do you steal a scene from the ex...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>671</td>\n",
       "      <td>111</td>\n",
       "      <td>question steal scene expert expert scene full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27600</th>\n",
       "      <td>Most people get the luxury of typing in the ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1635</td>\n",
       "      <td>291</td>\n",
       "      <td>people get luxury type title film find film wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20174</th>\n",
       "      <td>Hellraiser: Bloodline is where the sequel medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1241</td>\n",
       "      <td>214</td>\n",
       "      <td>hellraiser sequel mediocrity hellraiser series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10428</th>\n",
       "      <td>Worst DCOM I have seen. Ever. Well, maybe not ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>497</td>\n",
       "      <td>94</td>\n",
       "      <td>bad see ever well maybe bad smart house bad ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>Stu Ungar is considered by many to be the grea...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>833</td>\n",
       "      <td>159</td>\n",
       "      <td>consider many great poker player time extraord...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label  \\\n",
       "9739   fascinating look at fascist italy and the peop...    10      1   \n",
       "32982  FBI Agents Mulder and Scully get assigned to p...     8      1   \n",
       "15073  As much as I like Japanese movies this one did...     3      0   \n",
       "31243  This is absolutely the best none-animated fami...    10      1   \n",
       "31756  I cannot believe how uneducated this movie is....     1      0   \n",
       "27970  question: how do you steal a scene from the ex...     8      1   \n",
       "27600  Most people get the luxury of typing in the ti...     1      0   \n",
       "20174  Hellraiser: Bloodline is where the sequel medi...     1      0   \n",
       "10428  Worst DCOM I have seen. Ever. Well, maybe not ...     4      0   \n",
       "5823   Stu Ungar is considered by many to be the grea...     4      0   \n",
       "\n",
       "       rev_lens_raw  rev_lens_words  \\\n",
       "9739            279              50   \n",
       "32982          1784             298   \n",
       "15073           928             168   \n",
       "31243           268              47   \n",
       "31756           634             111   \n",
       "27970           671             111   \n",
       "27600          1635             291   \n",
       "20174          1241             214   \n",
       "10428           497              94   \n",
       "5823            833             159   \n",
       "\n",
       "                                               processed  \n",
       "9739   fascinate look fascist italy people carve life...  \n",
       "32982  fbi agent mulder scully get assign probe myste...  \n",
       "15073  much like japanese movie one cut movie suppose...  \n",
       "31243  absolutely best family film see quite back fir...  \n",
       "31756  believe uneducated movie like watch police aca...  \n",
       "27970  question steal scene expert expert scene full ...  \n",
       "27600  people get luxury type title film find film wa...  \n",
       "20174  hellraiser sequel mediocrity hellraiser series...  \n",
       "10428  bad see ever well maybe bad smart house bad ac...  \n",
       "5823   consider many great poker player time extraord...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 TF IDF Vectorizer class\n",
    "\n",
    "TF-IDF stands for Term Frequency - Ineverse Document Frequency. TF is a number that shows frequency of term in the document, in this case in a particular reveiw. IDF is calculated by: $log(\\frac{n}{1+DF})$ where N is number of documents in the Corpus, (here we use trainin set as corpus) and DF is document frequency, also number of documents that term appears in. If DF is high, the that must be a more common term, and TF of that term in the document has less weight.\n",
    "\n",
    "TF-IDF values are ${TF \\times IDF}$ or ${TF \\times log(\\frac{n}{1 + DF})}$ It takes into account both how much a term appears, and how unique is that term for that review.\n",
    "\n",
    "There are different wasy to calculate TF-IDF values. I used this to avoud division by zero, as we add 1 in the divisor. In some websites they use 1 + n in the dividend to avoid negative values. But since we have removed all terms that apperas in more than 85 % of the documents, it's no term in the vocab appears in all documetns. So negative values of TF-IDF is not possible in this case.\n",
    "\n",
    "\n",
    "\n",
    "When initializing the TfIdfVectorizer class I pass in dataframe (the training set) as argument. Then using two functions (prepare_idfs and prepare_vocab) it builds two dictionaries and saves them.\n",
    "\n",
    "`self._vocab` is a dictionary where the keys are vocabulary taken from processed reviews in the training set. Also it's all the words used in the processed reviews in the training set. Values are used to determine which index TF-IDF values should be saved on. Each value in the `self._vocab` dictionary determines which index the TF-IDF value of a term in the review sohuld be be saved on.\n",
    "\n",
    "`self._idfs` is a dictionary that has same keys as `self._vocab`. But it will contain the IDF values of each term, where IDF's are based on the training set, also based on who many times that term appears in the training set.\n",
    "\n",
    "\n",
    "When we vectorize any text to put it into classification, we first preprocess it using functions in \"TextPreprocessor\", then we vectorize it using  `tf_idf_vectorize_all()`. That function accepts dataframe as argument. It will use .apply of the Pandas Dataframe and run `tf_idf_vectorize()` for each row in the 'processed'. Then we convert it to Numpy Array and return it.\n",
    "\n",
    "`tf_idf_vectorize_all()` will return a Numpy array that has an array for each review, where size of each array is equal to size of `self._vocab` or vocabulary, with TF-IDF values that saved in the indexes that represent the words in the vocab (what index for what word is determined by using `self._vocab` dictionary).\n",
    "\n",
    "\n",
    "\n",
    "Here's illustration with a small example:\n",
    "\n",
    "For simple illustration let's say our corpus is very small and vocabulary has just 3 words. Then the `self._vocab` would be like:\n",
    "{\"hello\": 0, \"world\": 1, \"python\": 2}.\n",
    "\n",
    "`self._idfs` would be something similar, just it will contain IDF values. E.g. based on their frequencies, document size etc. it could be something like:\n",
    "\n",
    "{\"hello\": 0, \"world\": 0, \"python\": 0.405}.\n",
    "\n",
    "Here we assume the set has 8 documents. \"hello\" and \"world\" appears in 2 documents and \"python\" appears in one. So their values will be as shown above.\n",
    "\n",
    "E.g. if the text is \"hello hello world in python\". We will ignore the word 'in' since it's not in the `self._vocab` dictionary.\n",
    "\n",
    "We see the word \"hello\" exists in the `self._vocab` and has value '0 and its IDF value is 0 in `self._idfs`. It appears two times.\n",
    "So we multiply 2 by 0, and save the value in index 0. The word \"world\" has the value '1' in the `self._vocab` dictionary, 0 in `self._idfs` dictionary, appears once in the document. So we multiply 1 by 0, and save the result in index 1. We repeat the same process for python. Then we return a Numpy array that looks like: [0, 0, 0.405]\n",
    "\n",
    "The `self._idfs` and `self._vocab` is used in the code to make the vector generation more efficient, as we don't have to generate vocabs and IDF values each time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF VECTORIZER CLASS\n",
    "\n",
    "class TfIdfVectorizer:\n",
    "    \"\"\"\n",
    "    This is a class for turning dataframe of text into TF-IDF vectors\n",
    "      \n",
    "    Attributes:\n",
    "        idfs (dictionary): dictionary of words with their IDF values\n",
    "        vocab (dictionary): dictionary of words with their index values as integers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        The constructer of TfIDfVectorizer\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pands Dataframe): Dataframe with all the text that will be used to build \n",
    "            dictionary of IDF values and vocab dictionary\n",
    "        \"\"\"\n",
    "        self._idfs = self.prepare_idfs(df)\n",
    "        self._vocab = self.prepare_vocab(df)\n",
    "    \n",
    "    @property\n",
    "    def idfs(self):\n",
    "        \"\"\" Get IDFS\"\"\"\n",
    "        return self._idfs\n",
    "    \n",
    "    @idfs.setter\n",
    "    def idfs(self, idfs):\n",
    "        \"\"\" Set IDFS \"\"\"\n",
    "        self._idfs = idfs\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        \"\"\" Get vocab\"\"\"\n",
    "        return self._vocab\n",
    "    \n",
    "    @vocab.setter\n",
    "    def vocab(self, vocab):\n",
    "        \"\"\" Set vocab \"\"\"\n",
    "        self._vocab = vocab\n",
    "    \n",
    "    # Prepare and return vocab out of corpus\n",
    "    def prepare_vocab(self, df):\n",
    "        \"\"\"\n",
    "        Prepares the vocabulary to and returns a dictionary of vocabulary with increasing integer values.\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pandas Dataframe): Dataframe of text values to be used to generate vocabulary.\n",
    "            The text values must be in a column named 'processed' in the dataframe.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary that maps each word from vocabulary to index value.\n",
    "        \"\"\"\n",
    "        # Prepare the vocab\n",
    "        self.vocab = set(\" \".join(df['processed'].values).split())\n",
    "        self.vocab = dict.fromkeys(self.vocab, 0)\n",
    "        self.vocab.update((k, i) for i, k in enumerate(self.vocab))\n",
    "        return self.vocab\n",
    "    \n",
    "    # Prepare and return idfs out of corpus\n",
    "    def prepare_idfs(self, df):\n",
    "        \"\"\"\n",
    "        Prepares the IDF values to and returns a dictionary of vocabulary with IDF values.\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pandas Dataframe): Dataframe of text values to be used to generate IDF values.\n",
    "            The text values must be in a column named 'processed' in the dataframe.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary that maps each word from vocabulary to IDF value of that word.\n",
    "        \"\"\"\n",
    "        # Counting how many reviews a word appears ins\n",
    "        cnt = Counter()\n",
    "        for text in df[\"processed\"].values:\n",
    "            for word in set(text.split()):\n",
    "                cnt[word] += 1\n",
    "        # Preparing the IDF vector\n",
    "        size = len(df)\n",
    "        self.idfs = dict()\n",
    "        for w, c in cnt.items():\n",
    "            self.idfs[w] = math.log((1+size) / (1 + c))\n",
    "        return self.idfs\n",
    "\n",
    "\n",
    "    # TF-IDF vectorize a single text, returning an np.array\n",
    "    def tf_idf_vectorize(self, text):\n",
    "        \"\"\"\n",
    "        Takes a text as argument and and returns a Numpy vector of TD-IDF values\n",
    "        based on vocab and idfs attributes from that text.\n",
    "        \"\"\"\n",
    "        \n",
    "        freq_dist = FreqDist(text.split())\n",
    "        vector = np.zeros(len(self.vocab))\n",
    "        for w, c in freq_dist.items():\n",
    "            if w in self.vocab:\n",
    "                vector[self.vocab[w]] = c * self.idfs[w]\n",
    "        return vector\n",
    "\n",
    "\n",
    "    # One hot encode labels\n",
    "    def one_hot_encode(self, label, nr_of_labels):\n",
    "        \"\"\"\n",
    "        Takes integer and returns one-hot encoding numpy array of that\n",
    "        \"\"\"\n",
    "        arr = np.zeros(nr_of_labels, dtype=int)\n",
    "        arr[label] = 1\n",
    "        return arr\n",
    "\n",
    "    # Vectorize all in the dataset\n",
    "    def tf_idf_vectorize_all(self, df):\n",
    "        \"\"\"\n",
    "        Vectorizes all strings in a dataframe and returns numpy array of vectors based on idfs and vocab attributes.\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pandas Dataframe): All the strings must be saved in a column named 'processed'.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: A numpy array which consists of vectors \n",
    "                of all text from all rows under 'processed' in the dataframe.\n",
    "        \"\"\"\n",
    "        vectors = np.array(df['processed'].apply(lambda text: self.tf_idf_vectorize(text)).values.tolist())\n",
    "        return vectors\n",
    "    \n",
    "    # Turn all labels into one hot encoded arrays\n",
    "    def one_hot_encode_all(self, df, nr_of_labels):\n",
    "        \"\"\"\n",
    "        Ane hot encodes all labels in a Pandas Dataframe and returns Numpy array\n",
    "        \n",
    "        Parameters:\n",
    "            df (Pandas Dataframe): Labels must be saved in column named 'label'\n",
    "            nr_of_labels: number of categories, classes or labels minus one.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: A numpy array of all labels that are saved as one-hot encoded arrays each.\n",
    "        \"\"\"\n",
    "        vector = np.array(df['label'].apply(lambda label: self.one_hot_encode(label, nr_of_labels)).values.tolist())\n",
    "        return vector\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 Vectorizing data sets using TF-IDF vectors\n",
    "\n",
    "Here we instantiate TfIdfVector useing processed train set. We save the idfs and vocab in a variable for easier reference.\n",
    "\n",
    "Then we generate Numpy arrays. X_train contains vectors representing reviews, Y_train contains labels in a numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IDF values and VOCAB vector\n",
    "vectorizer = TfIdfVectorizer(train_set_processed)\n",
    "idfs = vectorizer.idfs\n",
    "vocab = vectorizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing train set and test set\n",
    "X_train = vectorizer.tf_idf_vectorize_all(train_set_processed)\n",
    "\n",
    "X_test = vectorizer.tf_idf_vectorize_all(test_set_processed)\n",
    "\n",
    "X_valid = vectorizer.tf_idf_vectorize_all(valid_set_processed)\n",
    "\n",
    "Y_train = train_set_processed['label'].to_numpy()\n",
    "\n",
    "Y_test = test_set_processed['label'].to_numpy()\n",
    "\n",
    "Y_valid = valid_set_processed['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check if index of labels correspond to the vectors in the Numpy arrays. We print the vocab to see some of the words included. While it contains a couple of weird words, most of them looks OK. We see the length of vocab is ca 12k which is big enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "12050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dental': 0,\n",
       " 'weed': 1,\n",
       " 'seek': 2,\n",
       " 'tremendously': 3,\n",
       " 'negotiate': 4,\n",
       " 'literary': 5,\n",
       " 'gackt': 6,\n",
       " 'hunk': 7,\n",
       " 'butch': 8,\n",
       " 'vulcan': 9,\n",
       " 'dis': 10,\n",
       " 'sidewalk': 11,\n",
       " 'haircut': 12,\n",
       " 'rebel': 13,\n",
       " 'investor': 14,\n",
       " 'hindu': 15,\n",
       " 'fbi': 16,\n",
       " 'impossible': 17,\n",
       " 'fulllength': 18,\n",
       " 'coach': 19,\n",
       " 'cross': 20,\n",
       " 'instance': 21,\n",
       " 'behind': 22,\n",
       " 'haul': 23,\n",
       " 'charity': 24,\n",
       " 'gunfight': 25,\n",
       " 'unfortunately': 26,\n",
       " 'elvis': 27,\n",
       " 'couldnt': 28,\n",
       " 'wall': 29,\n",
       " 'herein': 30,\n",
       " 'earth': 31,\n",
       " 'mushroom': 32,\n",
       " 'simultaneously': 33,\n",
       " 'ceo': 34,\n",
       " 'help': 35,\n",
       " 'unremarkable': 36,\n",
       " 'moore': 37,\n",
       " 'filler': 38,\n",
       " 'boo': 39,\n",
       " 'substitute': 40,\n",
       " 'fact': 41,\n",
       " 'therapy': 42,\n",
       " 'rightfully': 43,\n",
       " 'impromptu': 44,\n",
       " 'genitals': 45,\n",
       " 'command': 46,\n",
       " 'numbingly': 47,\n",
       " 'tightly': 48,\n",
       " 'perpetual': 49,\n",
       " 'unearth': 50,\n",
       " 'separately': 51,\n",
       " 'carlyle': 52,\n",
       " 'shuffle': 53,\n",
       " 'primetime': 54,\n",
       " 'newsreel': 55,\n",
       " 'coltrane': 56,\n",
       " 'pole': 57,\n",
       " 'blunder': 58,\n",
       " 'carole': 59,\n",
       " 'monday': 60,\n",
       " 'distinctive': 61,\n",
       " 'criminal': 62,\n",
       " 'suave': 63,\n",
       " 'donna': 64,\n",
       " 'fatigue': 65,\n",
       " 'illiterate': 66,\n",
       " 'charlie': 67,\n",
       " 'people': 68,\n",
       " 'disbelief': 69,\n",
       " 'eg': 70,\n",
       " 'good': 71,\n",
       " 'moment': 72,\n",
       " 'sauce': 73,\n",
       " 'christensen': 74,\n",
       " 'timid': 75,\n",
       " 'enlightenment': 76,\n",
       " 'ocean': 77,\n",
       " 'nowadays': 78,\n",
       " 'wacky': 79,\n",
       " 'powerless': 80,\n",
       " 'ava': 81,\n",
       " 'bogged': 82,\n",
       " 'dimwitted': 83,\n",
       " 'sterling': 84,\n",
       " 'mayhem': 85,\n",
       " 'muse': 86,\n",
       " 'ultra': 87,\n",
       " 'matt': 88,\n",
       " 'ridiculousness': 89,\n",
       " 'politically': 90,\n",
       " 'ernst': 91,\n",
       " 'rotate': 92,\n",
       " 'poise': 93,\n",
       " 'psychopath': 94,\n",
       " 'clayton': 95,\n",
       " 'marjorie': 96,\n",
       " 'bigtime': 97,\n",
       " 'assume': 98,\n",
       " 'inhabitant': 99,\n",
       " 'rapture': 100,\n",
       " 'ii': 101,\n",
       " 'canadian': 102,\n",
       " 'furnish': 103,\n",
       " 'hunky': 104,\n",
       " 'murder': 105,\n",
       " 'daytime': 106,\n",
       " 'poo': 107,\n",
       " 'deteriorate': 108,\n",
       " 'roar': 109,\n",
       " 'loyal': 110,\n",
       " 'suspenseful': 111,\n",
       " 'affiliate': 112,\n",
       " 'collage': 113,\n",
       " 'depp': 114,\n",
       " 'commission': 115,\n",
       " 'lag': 116,\n",
       " 'battlestar': 117,\n",
       " 'englishman': 118,\n",
       " 'dusk': 119,\n",
       " 'acquaint': 120,\n",
       " 'longoria': 121,\n",
       " 'fart': 122,\n",
       " 'retain': 123,\n",
       " 'gerald': 124,\n",
       " 'athletic': 125,\n",
       " 'otherthe': 126,\n",
       " 'copyright': 127,\n",
       " 'pace': 128,\n",
       " 'stylize': 129,\n",
       " 'slide': 130,\n",
       " 'sideline': 131,\n",
       " 'resist': 132,\n",
       " 'vacant': 133,\n",
       " 'victim': 134,\n",
       " 'decoration': 135,\n",
       " 'inconsistent': 136,\n",
       " 'limp': 137,\n",
       " 'foremost': 138,\n",
       " 'description': 139,\n",
       " 'lucille': 140,\n",
       " 'sublime': 141,\n",
       " 'inc': 142,\n",
       " 'prude': 143,\n",
       " 'ian': 144,\n",
       " 'dung': 145,\n",
       " 'wildly': 146,\n",
       " 'hurt': 147,\n",
       " 'wretched': 148,\n",
       " 'lambert': 149,\n",
       " 'flatout': 150,\n",
       " 'seize': 151,\n",
       " 'check': 152,\n",
       " 'britain': 153,\n",
       " 'robin': 154,\n",
       " 'disdain': 155,\n",
       " 'incomparable': 156,\n",
       " 'recorder': 157,\n",
       " 'next': 158,\n",
       " 'daylewis': 159,\n",
       " 'weaken': 160,\n",
       " 'midlife': 161,\n",
       " 'mix': 162,\n",
       " 'survive': 163,\n",
       " 'twin': 164,\n",
       " 'episode': 165,\n",
       " 'cheung': 166,\n",
       " 'brink': 167,\n",
       " 'curve': 168,\n",
       " 'dispel': 169,\n",
       " 'putrid': 170,\n",
       " 'longtime': 171,\n",
       " 'coen': 172,\n",
       " 'kyle': 173,\n",
       " 'applause': 174,\n",
       " 'endearingly': 175,\n",
       " 'melancholic': 176,\n",
       " 'maniac': 177,\n",
       " 'wry': 178,\n",
       " 'sticker': 179,\n",
       " 'extent': 180,\n",
       " 'resent': 181,\n",
       " 'torch': 182,\n",
       " 'gas': 183,\n",
       " 'imprison': 184,\n",
       " 'maltin': 185,\n",
       " 'lancaster': 186,\n",
       " 'il': 187,\n",
       " 'enliven': 188,\n",
       " 'roll': 189,\n",
       " 'game': 190,\n",
       " 'discipline': 191,\n",
       " 'springer': 192,\n",
       " 'bigoted': 193,\n",
       " 'whomever': 194,\n",
       " 'hideous': 195,\n",
       " 'soprano': 196,\n",
       " 'tshirt': 197,\n",
       " 'marble': 198,\n",
       " 'warrant': 199,\n",
       " 'dominic': 200,\n",
       " 'ennio': 201,\n",
       " 'jerome': 202,\n",
       " 'carefree': 203,\n",
       " 'mpaa': 204,\n",
       " 'hometown': 205,\n",
       " 'determination': 206,\n",
       " 'firth': 207,\n",
       " 'abrasive': 208,\n",
       " 'naive': 209,\n",
       " 'ancient': 210,\n",
       " 'knee': 211,\n",
       " 'stitch': 212,\n",
       " 'carbon': 213,\n",
       " 'mythical': 214,\n",
       " 'sporadically': 215,\n",
       " 'nobudget': 216,\n",
       " 'ullman': 217,\n",
       " 'benign': 218,\n",
       " 'remind': 219,\n",
       " 'translator': 220,\n",
       " 'therein': 221,\n",
       " 'difficulty': 222,\n",
       " 'mutilation': 223,\n",
       " 'letter': 224,\n",
       " 'husband': 225,\n",
       " 'patriarch': 226,\n",
       " 'muddle': 227,\n",
       " 'vignette': 228,\n",
       " 'association': 229,\n",
       " 'emmanuelle': 230,\n",
       " 'mgm': 231,\n",
       " 'presentation': 232,\n",
       " 'bootleg': 233,\n",
       " 'term': 234,\n",
       " 'postman': 235,\n",
       " 'handful': 236,\n",
       " 'sorvino': 237,\n",
       " 'harmony': 238,\n",
       " 'edmund': 239,\n",
       " 'handheld': 240,\n",
       " 'gi': 241,\n",
       " 'moviebut': 242,\n",
       " 'chubby': 243,\n",
       " 'donut': 244,\n",
       " 'dramatization': 245,\n",
       " 'pluck': 246,\n",
       " 'william': 247,\n",
       " 'capitalism': 248,\n",
       " 'skilled': 249,\n",
       " 'shirt': 250,\n",
       " 'fiery': 251,\n",
       " 'gender': 252,\n",
       " 'swat': 253,\n",
       " 'nauseous': 254,\n",
       " 'detriment': 255,\n",
       " 'yep': 256,\n",
       " 'precode': 257,\n",
       " 'delicate': 258,\n",
       " 'train': 259,\n",
       " 'protection': 260,\n",
       " 'disco': 261,\n",
       " 'library': 262,\n",
       " 'gabriel': 263,\n",
       " 'western': 264,\n",
       " 'today': 265,\n",
       " 'bureau': 266,\n",
       " 'tempo': 267,\n",
       " 'reply': 268,\n",
       " 'oz': 269,\n",
       " 'superior': 270,\n",
       " 'atlantis': 271,\n",
       " 'severely': 272,\n",
       " 'expression': 273,\n",
       " 'hubby': 274,\n",
       " 'main': 275,\n",
       " 'chief': 276,\n",
       " 'pierre': 277,\n",
       " 'yummy': 278,\n",
       " 'launcher': 279,\n",
       " 'cameron': 280,\n",
       " 'palma': 281,\n",
       " 'eagerly': 282,\n",
       " 'chopper': 283,\n",
       " 'underwear': 284,\n",
       " 'financially': 285,\n",
       " 'monument': 286,\n",
       " 'muscular': 287,\n",
       " 'unsophisticated': 288,\n",
       " 'stimulate': 289,\n",
       " 'fernando': 290,\n",
       " 'dart': 291,\n",
       " 'stage': 292,\n",
       " 'abbott': 293,\n",
       " 'myrna': 294,\n",
       " 'reversal': 295,\n",
       " 'implore': 296,\n",
       " 'like': 297,\n",
       " 'banquet': 298,\n",
       " 'sol': 299,\n",
       " 'neil': 300,\n",
       " 'despise': 301,\n",
       " 'nixon': 302,\n",
       " 'coaster': 303,\n",
       " 'amenabar': 304,\n",
       " 'sawyer': 305,\n",
       " 'poorly': 306,\n",
       " 'johnston': 307,\n",
       " 'rachel': 308,\n",
       " 'ensue': 309,\n",
       " 'definition': 310,\n",
       " 'voight': 311,\n",
       " 'impeccable': 312,\n",
       " 'monica': 313,\n",
       " 'place': 314,\n",
       " 'crumble': 315,\n",
       " 'primary': 316,\n",
       " 'susannah': 317,\n",
       " 'deem': 318,\n",
       " 'trump': 319,\n",
       " 'isnt': 320,\n",
       " 'anticipation': 321,\n",
       " 'suddenly': 322,\n",
       " 'demi': 323,\n",
       " 'singh': 324,\n",
       " 'ironic': 325,\n",
       " 'starr': 326,\n",
       " 'ringo': 327,\n",
       " 'grandma': 328,\n",
       " 'waterfront': 329,\n",
       " 'inferior': 330,\n",
       " 'time': 331,\n",
       " 'induce': 332,\n",
       " 'lucy': 333,\n",
       " 'devastation': 334,\n",
       " 'adapt': 335,\n",
       " 'quaint': 336,\n",
       " 'candid': 337,\n",
       " 'jewelry': 338,\n",
       " 'turner': 339,\n",
       " 'soylent': 340,\n",
       " 'prompt': 341,\n",
       " 'seedy': 342,\n",
       " 'volume': 343,\n",
       " 'nipple': 344,\n",
       " 'turk': 345,\n",
       " 'introspective': 346,\n",
       " 'shoot': 347,\n",
       " 'talkie': 348,\n",
       " 'weary': 349,\n",
       " 'oneill': 350,\n",
       " 'eccentric': 351,\n",
       " 'refresh': 352,\n",
       " 'repay': 353,\n",
       " 'unemployment': 354,\n",
       " 'memento': 355,\n",
       " 'emerge': 356,\n",
       " 'override': 357,\n",
       " 'spoilersi': 358,\n",
       " 'destiny': 359,\n",
       " 'dakota': 360,\n",
       " 'layout': 361,\n",
       " 'disfigure': 362,\n",
       " 'dwell': 363,\n",
       " 'archer': 364,\n",
       " 'oppose': 365,\n",
       " 'stereo': 366,\n",
       " 'cushing': 367,\n",
       " 'hepburn': 368,\n",
       " 'ability': 369,\n",
       " 'bishop': 370,\n",
       " 'wine': 371,\n",
       " 'laura': 372,\n",
       " 'pursuer': 373,\n",
       " 'pitch': 374,\n",
       " 'micheal': 375,\n",
       " 'sentiment': 376,\n",
       " 'splendidly': 377,\n",
       " 'intricacy': 378,\n",
       " 'wing': 379,\n",
       " 'robocop': 380,\n",
       " 'strength': 381,\n",
       " 'brace': 382,\n",
       " 'mistreat': 383,\n",
       " 'autistic': 384,\n",
       " 'embellish': 385,\n",
       " 'harry': 386,\n",
       " 'weakness': 387,\n",
       " 'heritage': 388,\n",
       " 'adrienne': 389,\n",
       " 'lam': 390,\n",
       " 'oscarwinning': 391,\n",
       " 'biko': 392,\n",
       " 'talent': 393,\n",
       " 'newspaper': 394,\n",
       " 'brilliance': 395,\n",
       " 'hunchback': 396,\n",
       " 'stalker': 397,\n",
       " 'ed': 398,\n",
       " 'sure': 399,\n",
       " 'shoulder': 400,\n",
       " 'imperfect': 401,\n",
       " 'demand': 402,\n",
       " 'silly': 403,\n",
       " 'slightly': 404,\n",
       " 'blacklist': 405,\n",
       " '': 406,\n",
       " 'baby': 407,\n",
       " 'choir': 408,\n",
       " 'apology': 409,\n",
       " 'joe': 410,\n",
       " 'kerry': 411,\n",
       " 'audacity': 412,\n",
       " 'widescreen': 413,\n",
       " 'vinnie': 414,\n",
       " 'delusional': 415,\n",
       " 'chart': 416,\n",
       " 'ebay': 417,\n",
       " 'aimee': 418,\n",
       " 'meatball': 419,\n",
       " 'wade': 420,\n",
       " 'deceit': 421,\n",
       " 'proceed': 422,\n",
       " 'croc': 423,\n",
       " 'noble': 424,\n",
       " 'pursuit': 425,\n",
       " 'oriental': 426,\n",
       " 'industry': 427,\n",
       " 'traffic': 428,\n",
       " 'nominee': 429,\n",
       " 'wherein': 430,\n",
       " 'infuriate': 431,\n",
       " 'oc': 432,\n",
       " 'cruelty': 433,\n",
       " 'psychiatric': 434,\n",
       " 'mushy': 435,\n",
       " 'sa': 436,\n",
       " 'keira': 437,\n",
       " 'glossy': 438,\n",
       " 'invade': 439,\n",
       " 'ail': 440,\n",
       " 'mcadams': 441,\n",
       " 'vatican': 442,\n",
       " 'simpleminded': 443,\n",
       " 'turkish': 444,\n",
       " 'tank': 445,\n",
       " 'itthis': 446,\n",
       " 'hallway': 447,\n",
       " 'machinery': 448,\n",
       " 'homage': 449,\n",
       " 'grenade': 450,\n",
       " 'clunker': 451,\n",
       " 'artist': 452,\n",
       " 'sell': 453,\n",
       " 'beget': 454,\n",
       " 'tint': 455,\n",
       " 'different': 456,\n",
       " 'react': 457,\n",
       " 'fourteen': 458,\n",
       " 'ethical': 459,\n",
       " 'thread': 460,\n",
       " 'loach': 461,\n",
       " 'convey': 462,\n",
       " 'bauer': 463,\n",
       " 'atomic': 464,\n",
       " 'brashear': 465,\n",
       " 'gravity': 466,\n",
       " 'halfhearted': 467,\n",
       " 'efficiently': 468,\n",
       " 'minion': 469,\n",
       " 'rural': 470,\n",
       " 'slaughterhouse': 471,\n",
       " 'hastily': 472,\n",
       " 'space': 473,\n",
       " 'wan': 474,\n",
       " 'edinburgh': 475,\n",
       " 'ohara': 476,\n",
       " 'powerfully': 477,\n",
       " 'corporation': 478,\n",
       " 'irresponsible': 479,\n",
       " 'muchthe': 480,\n",
       " 'joey': 481,\n",
       " 'delivery': 482,\n",
       " 'kitchen': 483,\n",
       " 'splatter': 484,\n",
       " 'shortcoming': 485,\n",
       " 'shelley': 486,\n",
       " 'infinitely': 487,\n",
       " 'govinda': 488,\n",
       " 'parlor': 489,\n",
       " 'whats': 490,\n",
       " 'yo': 491,\n",
       " 'diabolical': 492,\n",
       " 'harmless': 493,\n",
       " 'stahl': 494,\n",
       " 'falter': 495,\n",
       " 'dawson': 496,\n",
       " 'concentrate': 497,\n",
       " 'bette': 498,\n",
       " 'purity': 499,\n",
       " 'auntie': 500,\n",
       " 'secular': 501,\n",
       " 'contend': 502,\n",
       " 'probe': 503,\n",
       " 'bone': 504,\n",
       " 'concentration': 505,\n",
       " 'moon': 506,\n",
       " 'bruce': 507,\n",
       " 'avail': 508,\n",
       " 'bonkers': 509,\n",
       " 'exact': 510,\n",
       " 'helena': 511,\n",
       " 'reallife': 512,\n",
       " 'impersonation': 513,\n",
       " 'softcore': 514,\n",
       " 'thereby': 515,\n",
       " 'genie': 516,\n",
       " 'serum': 517,\n",
       " 'intensity': 518,\n",
       " 'seclude': 519,\n",
       " 'farmer': 520,\n",
       " 'nerd': 521,\n",
       " 'gadget': 522,\n",
       " 'weekend': 523,\n",
       " 'catalogue': 524,\n",
       " 'zhang': 525,\n",
       " 'porno': 526,\n",
       " 'allen': 527,\n",
       " 'deniro': 528,\n",
       " 'airline': 529,\n",
       " 'empower': 530,\n",
       " 'insensitive': 531,\n",
       " 'elite': 532,\n",
       " 'credibility': 533,\n",
       " 'storyteller': 534,\n",
       " 'cape': 535,\n",
       " 'captain': 536,\n",
       " 'serpent': 537,\n",
       " 'meaningful': 538,\n",
       " 'faithfully': 539,\n",
       " 'subpar': 540,\n",
       " 'daisy': 541,\n",
       " 'komodo': 542,\n",
       " 'appear': 543,\n",
       " 'leung': 544,\n",
       " 'engulf': 545,\n",
       " 'cleverly': 546,\n",
       " 'lame': 547,\n",
       " 'uncertain': 548,\n",
       " 'wipe': 549,\n",
       " 'supreme': 550,\n",
       " 'tweak': 551,\n",
       " 'antwone': 552,\n",
       " 'rant': 553,\n",
       " 'foreign': 554,\n",
       " 'vhs': 555,\n",
       " 'stint': 556,\n",
       " 'alec': 557,\n",
       " 'prison': 558,\n",
       " 'logo': 559,\n",
       " 'ha': 560,\n",
       " 'throughout': 561,\n",
       " 'doug': 562,\n",
       " 'underwater': 563,\n",
       " 'hogwash': 564,\n",
       " 'growl': 565,\n",
       " 'nail': 566,\n",
       " 'themthe': 567,\n",
       " 'indictment': 568,\n",
       " 'endeavor': 569,\n",
       " 'brass': 570,\n",
       " 'commercially': 571,\n",
       " 'hulk': 572,\n",
       " 'mechanical': 573,\n",
       " 'yearold': 574,\n",
       " 'moviethat': 575,\n",
       " 'tin': 576,\n",
       " 'embarrass': 577,\n",
       " 'lurid': 578,\n",
       " 'cook': 579,\n",
       " 'lastly': 580,\n",
       " 'differ': 581,\n",
       " 'goddess': 582,\n",
       " 'complaint': 583,\n",
       " 'law': 584,\n",
       " 'lindsey': 585,\n",
       " 'crew': 586,\n",
       " 'shrill': 587,\n",
       " 'pt': 588,\n",
       " 'independent': 589,\n",
       " 'vicious': 590,\n",
       " 'societal': 591,\n",
       " 'linger': 592,\n",
       " 'petty': 593,\n",
       " 'eponymous': 594,\n",
       " 'escalate': 595,\n",
       " 'hitchhiker': 596,\n",
       " 'displace': 597,\n",
       " 'anxious': 598,\n",
       " 'fatherson': 599,\n",
       " 'structure': 600,\n",
       " 'town': 601,\n",
       " 'kidman': 602,\n",
       " 'noah': 603,\n",
       " 'criminally': 604,\n",
       " 'much': 605,\n",
       " 'recently': 606,\n",
       " 'queen': 607,\n",
       " 'marital': 608,\n",
       " 'dismal': 609,\n",
       " 'symbolism': 610,\n",
       " 'adolescence': 611,\n",
       " 'awfulness': 612,\n",
       " 'fate': 613,\n",
       " 'google': 614,\n",
       " 'sharon': 615,\n",
       " 'laconic': 616,\n",
       " 'tidy': 617,\n",
       " 'cinematography': 618,\n",
       " 'angelic': 619,\n",
       " 'umbrella': 620,\n",
       " 'immortal': 621,\n",
       " 'isolate': 622,\n",
       " 'duvall': 623,\n",
       " 'multifaceted': 624,\n",
       " 'ingrid': 625,\n",
       " 'account': 626,\n",
       " 'breslin': 627,\n",
       " 'dang': 628,\n",
       " 'rivalry': 629,\n",
       " 'plain': 630,\n",
       " 'middleclass': 631,\n",
       " 'porn': 632,\n",
       " 'naughty': 633,\n",
       " 'denial': 634,\n",
       " 'banjo': 635,\n",
       " 'elliott': 636,\n",
       " 'expensive': 637,\n",
       " 'subversive': 638,\n",
       " 'encompass': 639,\n",
       " 'shin': 640,\n",
       " 'elwes': 641,\n",
       " 'bohemian': 642,\n",
       " 'teach': 643,\n",
       " 'jayne': 644,\n",
       " 'fang': 645,\n",
       " 'racy': 646,\n",
       " 'boast': 647,\n",
       " 'suspense': 648,\n",
       " 'radiant': 649,\n",
       " 'unexciting': 650,\n",
       " 'rabid': 651,\n",
       " 'goo': 652,\n",
       " 'compress': 653,\n",
       " 'fleming': 654,\n",
       " 'sub': 655,\n",
       " 'corbett': 656,\n",
       " 'adequate': 657,\n",
       " 'practical': 658,\n",
       " 'perception': 659,\n",
       " 'amateurish': 660,\n",
       " 'newfound': 661,\n",
       " 'easy': 662,\n",
       " 'gyllenhaal': 663,\n",
       " 'muster': 664,\n",
       " 'relevant': 665,\n",
       " 'rewatching': 666,\n",
       " 'rewatch': 667,\n",
       " 'insist': 668,\n",
       " 'manchu': 669,\n",
       " 'selfcontained': 670,\n",
       " 'sway': 671,\n",
       " 'diet': 672,\n",
       " 'marilyn': 673,\n",
       " 'naked': 674,\n",
       " 'mole': 675,\n",
       " 'havent': 676,\n",
       " 'hadley': 677,\n",
       " 'accuse': 678,\n",
       " 'disrupt': 679,\n",
       " 'sidesplitting': 680,\n",
       " 'omit': 681,\n",
       " 'guess': 682,\n",
       " 'fountain': 683,\n",
       " 'romero': 684,\n",
       " 'danza': 685,\n",
       " 'trench': 686,\n",
       " 'heart': 687,\n",
       " 'toss': 688,\n",
       " 'lifethis': 689,\n",
       " 'vonnegut': 690,\n",
       " 'virginity': 691,\n",
       " 'freakish': 692,\n",
       " 'masala': 693,\n",
       " 'overblown': 694,\n",
       " 'level': 695,\n",
       " 'kooky': 696,\n",
       " 'ham': 697,\n",
       " 'rob': 698,\n",
       " 'hesitant': 699,\n",
       " 'thailand': 700,\n",
       " 'appreciate': 701,\n",
       " 'upheaval': 702,\n",
       " 'sardonic': 703,\n",
       " 'swipe': 704,\n",
       " 'dedication': 705,\n",
       " 'exploitation': 706,\n",
       " 'heavyweight': 707,\n",
       " 'discovery': 708,\n",
       " 'thats': 709,\n",
       " 'costarred': 710,\n",
       " 'descendant': 711,\n",
       " 'sutherland': 712,\n",
       " 'hooper': 713,\n",
       " 'lear': 714,\n",
       " 'levy': 715,\n",
       " 'paul': 716,\n",
       " 'connolly': 717,\n",
       " 'factory': 718,\n",
       " 'circa': 719,\n",
       " 'grandeur': 720,\n",
       " 'offer': 721,\n",
       " 'thisit': 722,\n",
       " 'concise': 723,\n",
       " 'nuclear': 724,\n",
       " 'stake': 725,\n",
       " 'goody': 726,\n",
       " 'hindsight': 727,\n",
       " 'nod': 728,\n",
       " 'culprit': 729,\n",
       " 'clone': 730,\n",
       " 'hardnosed': 731,\n",
       " 'seduction': 732,\n",
       " 'longwinded': 733,\n",
       " 'eloquent': 734,\n",
       " 'line': 735,\n",
       " 'randolph': 736,\n",
       " 'patriotism': 737,\n",
       " 'scrawny': 738,\n",
       " 'perk': 739,\n",
       " 'nemesis': 740,\n",
       " 'cheeky': 741,\n",
       " 'fck': 742,\n",
       " 'finish': 743,\n",
       " 'voiceover': 744,\n",
       " 'lincoln': 745,\n",
       " 'beauty': 746,\n",
       " 'plagiarism': 747,\n",
       " 'cartoon': 748,\n",
       " 'span': 749,\n",
       " 'overweight': 750,\n",
       " 'shiny': 751,\n",
       " 'enthral': 752,\n",
       " 'spring': 753,\n",
       " 'debut': 754,\n",
       " 'stressful': 755,\n",
       " 'repercussion': 756,\n",
       " 'gigantic': 757,\n",
       " 'rotten': 758,\n",
       " 'franchot': 759,\n",
       " 'orleans': 760,\n",
       " 'jackson': 761,\n",
       " 'wrath': 762,\n",
       " 'original': 763,\n",
       " 'shaolin': 764,\n",
       " 'host': 765,\n",
       " 'hackneyed': 766,\n",
       " 'convent': 767,\n",
       " 'sip': 768,\n",
       " 'joss': 769,\n",
       " 'militant': 770,\n",
       " 'sleaze': 771,\n",
       " 'eleniak': 772,\n",
       " 'hilt': 773,\n",
       " 'trim': 774,\n",
       " 'illness': 775,\n",
       " 'grinch': 776,\n",
       " 'jo': 777,\n",
       " 'foot': 778,\n",
       " 'masterfully': 779,\n",
       " 'simon': 780,\n",
       " 'pie': 781,\n",
       " 'cheerfully': 782,\n",
       " 'liven': 783,\n",
       " 'eugene': 784,\n",
       " 'selfproclaimed': 785,\n",
       " 'machine': 786,\n",
       " 'evil': 787,\n",
       " 'craftsman': 788,\n",
       " 'kiddy': 789,\n",
       " 'barton': 790,\n",
       " 'consideration': 791,\n",
       " 'motif': 792,\n",
       " 'boil': 793,\n",
       " 'dealer': 794,\n",
       " 'helen': 795,\n",
       " 'nutty': 796,\n",
       " 'infantile': 797,\n",
       " 'shred': 798,\n",
       " 'flaw': 799,\n",
       " 'todd': 800,\n",
       " 'appoint': 801,\n",
       " 'foible': 802,\n",
       " 'nationalist': 803,\n",
       " 'situate': 804,\n",
       " 'sleuth': 805,\n",
       " 'comic': 806,\n",
       " 'screenwriter': 807,\n",
       " 'abbot': 808,\n",
       " 'somber': 809,\n",
       " 'roth': 810,\n",
       " 'peace': 811,\n",
       " 'dross': 812,\n",
       " 'custom': 813,\n",
       " 'waste': 814,\n",
       " 'stop': 815,\n",
       " 'love': 816,\n",
       " 'occupy': 817,\n",
       " 'shakespearean': 818,\n",
       " 'cesar': 819,\n",
       " 'strand': 820,\n",
       " 'counterpart': 821,\n",
       " 'wolf': 822,\n",
       " 'developer': 823,\n",
       " 'astonishingly': 824,\n",
       " 'doodle': 825,\n",
       " 'carol': 826,\n",
       " 'jessica': 827,\n",
       " 'waver': 828,\n",
       " 'carnal': 829,\n",
       " 'seemingly': 830,\n",
       " 'areal': 831,\n",
       " 'optimistic': 832,\n",
       " 'nell': 833,\n",
       " 'snappy': 834,\n",
       " 'saul': 835,\n",
       " 'wholly': 836,\n",
       " 'strain': 837,\n",
       " 'guffaw': 838,\n",
       " 'cassidy': 839,\n",
       " 'crummy': 840,\n",
       " 'piece': 841,\n",
       " 'faithfulness': 842,\n",
       " 'capote': 843,\n",
       " 'bike': 844,\n",
       " 'hurl': 845,\n",
       " 'kiddie': 846,\n",
       " 'rouse': 847,\n",
       " 'glare': 848,\n",
       " 'abandon': 849,\n",
       " 'mae': 850,\n",
       " 'merchandise': 851,\n",
       " 'dodgy': 852,\n",
       " 'bump': 853,\n",
       " 'conroy': 854,\n",
       " 'penetrate': 855,\n",
       " 'wtf': 856,\n",
       " 'suitable': 857,\n",
       " 'faceless': 858,\n",
       " 'webb': 859,\n",
       " 'fighter': 860,\n",
       " 'inner': 861,\n",
       " 'sho': 862,\n",
       " 'chick': 863,\n",
       " 'turgid': 864,\n",
       " 'monarch': 865,\n",
       " 'affirm': 866,\n",
       " 'pile': 867,\n",
       " '': 868,\n",
       " 'eddy': 869,\n",
       " 'frank': 870,\n",
       " 'troop': 871,\n",
       " 'etch': 872,\n",
       " 'bury': 873,\n",
       " 'celebrate': 874,\n",
       " 'spain': 875,\n",
       " 'last': 876,\n",
       " 'ostensibly': 877,\n",
       " 'vh': 878,\n",
       " 'scholar': 879,\n",
       " 'house': 880,\n",
       " 'herethe': 881,\n",
       " 'mutiny': 882,\n",
       " 'bachchan': 883,\n",
       " 'statistic': 884,\n",
       " 'sermon': 885,\n",
       " 'message': 886,\n",
       " 'office': 887,\n",
       " 'coolness': 888,\n",
       " 'veneer': 889,\n",
       " 'slutty': 890,\n",
       " 'conviction': 891,\n",
       " 'hardship': 892,\n",
       " 'delve': 893,\n",
       " 'fever': 894,\n",
       " 'respite': 895,\n",
       " 'revive': 896,\n",
       " 'attenborough': 897,\n",
       " 'shame': 898,\n",
       " 'strap': 899,\n",
       " 'redefine': 900,\n",
       " 'doctor': 901,\n",
       " 'insufferable': 902,\n",
       " 'injokes': 903,\n",
       " 'miniseries': 904,\n",
       " 'selfishness': 905,\n",
       " 'snob': 906,\n",
       " 'dominique': 907,\n",
       " 'trinity': 908,\n",
       " 'portray': 909,\n",
       " 'gloss': 910,\n",
       " 'avoid': 911,\n",
       " 'kinnear': 912,\n",
       " 'terror': 913,\n",
       " 'residence': 914,\n",
       " 'isi': 915,\n",
       " 'island': 916,\n",
       " 'onedimensional': 917,\n",
       " 'douglas': 918,\n",
       " 'november': 919,\n",
       " 'dawn': 920,\n",
       " 'performer': 921,\n",
       " 'conquest': 922,\n",
       " 'investigator': 923,\n",
       " 'human': 924,\n",
       " 'deal': 925,\n",
       " 'consistently': 926,\n",
       " 'bender': 927,\n",
       " 'buff': 928,\n",
       " 'bettany': 929,\n",
       " 'booth': 930,\n",
       " 'timon': 931,\n",
       " 'jail': 932,\n",
       " 'dopey': 933,\n",
       " 'televise': 934,\n",
       " 'mouse': 935,\n",
       " 'showdown': 936,\n",
       " 'saloon': 937,\n",
       " 'fatherinlaw': 938,\n",
       " 'death': 939,\n",
       " 'passable': 940,\n",
       " 'hilariously': 941,\n",
       " 'mend': 942,\n",
       " 'enter': 943,\n",
       " 'inexperience': 944,\n",
       " 'polly': 945,\n",
       " 'amid': 946,\n",
       " 'identifiable': 947,\n",
       " 'ignite': 948,\n",
       " 'scene': 949,\n",
       " 'engagement': 950,\n",
       " 'vic': 951,\n",
       " 'crime': 952,\n",
       " 'lackluster': 953,\n",
       " 'overshadow': 954,\n",
       " 'ghoul': 955,\n",
       " 'harper': 956,\n",
       " 'kirk': 957,\n",
       " 'easter': 958,\n",
       " 'sociological': 959,\n",
       " 'along': 960,\n",
       " 'retirement': 961,\n",
       " 'confederate': 962,\n",
       " 'progress': 963,\n",
       " 'glitch': 964,\n",
       " 'ewoks': 965,\n",
       " 'summarize': 966,\n",
       " 'sarah': 967,\n",
       " 'knightley': 968,\n",
       " 'egyptian': 969,\n",
       " 'unfathomable': 970,\n",
       " 'davis': 971,\n",
       " 'dangerfield': 972,\n",
       " 'cord': 973,\n",
       " 'heavily': 974,\n",
       " 'surrealist': 975,\n",
       " 'absurdly': 976,\n",
       " 'cutout': 977,\n",
       " 'eli': 978,\n",
       " 'broadcast': 979,\n",
       " 'uninspiring': 980,\n",
       " 'fiennes': 981,\n",
       " 'make': 982,\n",
       " 'scandal': 983,\n",
       " 'subplots': 984,\n",
       " 'client': 985,\n",
       " 'marshal': 986,\n",
       " 'def': 987,\n",
       " 'warmth': 988,\n",
       " 'shipwreck': 989,\n",
       " 'turtle': 990,\n",
       " 'mcdonald': 991,\n",
       " 'gang': 992,\n",
       " 'tepid': 993,\n",
       " 'baggage': 994,\n",
       " 'bald': 995,\n",
       " 'rare': 996,\n",
       " 'journal': 997,\n",
       " 'crusade': 998,\n",
       " 'lsd': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if labels fit with X values\n",
    "index = random.randint(0, len(train_set))\n",
    "print(Y_train[index])\n",
    "print(train_set.loc[index, 'label'])\n",
    "\n",
    "print(len(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up genetic algorithm\n",
    "\n",
    "Genetic Algorithm is an algorithm that's inspired by evolution theory. Here we have a list of solutions that can be anything that keeps some data in some form of data-structures. They \"mutate\" and \"reproduce\" and new generations are produced. We choose the best chromosomes and eliminate bad ones. The goal is to come up with the best solution after some generations.\n",
    "\n",
    "I built 3 classes to apply the genetic algorithm.\n",
    "1. The classifier: It's a simple neural network. It takes vectors as input and classifies it, and calculates Cross Entropy Loss and accuracy.\n",
    "2. The Chromosome: This is a class that essentially just keeps the weights and makes operations on the weights. In GA we are trying to optimize the weights. So it has some functions like mutation, crossover and assigning fitness.\n",
    "3. GAEngine (Genetic Algorithm Engine): This class makes operations on the chromosomes at a higher lever. It has functions like choosing chromosomes to mutate, to do crossover, choosing what chromosomes to keep etc. and it has a function that runs the routine of the genetic algorithm training.\n",
    "\n",
    "The structure where we use two seperate classes, GAEngine and Chromosome, is inspired from an earlier base code on GA assignment. On some guides online they use that structure too. It's very logical, convenient and easy to understand. Chromosome objects represents the \"species\" and GAEngine represents the \"nature\" in evolution theory.\n",
    "\n",
    "While the GA training routine was put outside the GAEngine in the earlier GA assignment, I put it inside in a function here. That is to make the code more clean.\n",
    "\n",
    "I could use both classifier and Chromosome as same class, since both keeps weights. But classifier keeps vocab and idfs. I didn't want to keep same data multiple places. Their functions are distinct, classifier is a classifier, but Chromosome just keeps and opreates on weights. So keeping them as distinct classes makes more sense and gives more clean structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classifier\n",
    "In the assignment text they mention using decision tree. I tried to build one, but I have not much experience with decision trees. There are other classifiers too. I haven't any experience in most of them, just a little familiar with Neural Network thus this is what I did choose.\n",
    "\n",
    "Too simple architecture fails to make prediction. I tried with simple architecture and it was almost untrainable. Too complicated architecture (too many nodes, too much hidden layers) causes overfitting and it's very slow in training and prediction. After experimenting with different models, the model I decided is this:\n",
    "\n",
    "Input is as long as length of the vocab, which is 1250.\n",
    "\n",
    "Then a hidden layer of 16 nodes, using ReLU as activation function.\n",
    "Then a hidden layer of 8 nodes, using ReLU as activation function.\n",
    "Then using one more hidden layer of 2 nodes. No special activation function is used, it's just linear.\n",
    "Then at the end I use \"SoftMax\" to get predictions as output.\n",
    "\n",
    "Number of layers can't be set dynamically, but number of hidden nodes can be set dynamically while instantiating the classifier.\n",
    "\n",
    "The classifier returns both \"Cross Entropy Loss\" and \"accuracy\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"\n",
    "    This is a simple 3 layer neural network classifier.\n",
    "    \n",
    "    It has 2 hidden layers. Output has two nodes and uses softmax at the end.\n",
    "    It's attributes are idfs that show IDF values, vocab that have vocab values.\n",
    "    How many nodes in the input is determined by the attribute in_vector.\n",
    "    Other attributes are 3 Numpy matrices of weights and 3 Numpy arrays of biases.\n",
    "    \"\"\"\n",
    "    def __init__(self, idfs, vocab):\n",
    "        \"\"\"\n",
    "        The constructor for Classifier class.\n",
    "        \n",
    "        Parameters:\n",
    "            idfs (dict): a dictionary that maps words and IDF values\n",
    "            vocab (dict): a dictionary that maps words and integer values (where in the vector the word is presented)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.idfs = idfs\n",
    "        self.vocab = vocab\n",
    "        self.ws_1 = 0\n",
    "        self.ws_2 = 0\n",
    "        self.ws_3 = 0\n",
    "        self.bias_1 = 0\n",
    "        self.bias_2 = 0\n",
    "        self.bias_3 = 0\n",
    "        self.hn_1 = 0\n",
    "        self.hn_2 = 0\n",
    "        self.in_vector = 0\n",
    "\n",
    "    # Initializing weights\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializing weights and biases and returning them.\n",
    "        \n",
    "        Returns:\n",
    "            (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray): Weights and biases\n",
    "                for the neural net\n",
    "        \"\"\"\n",
    "        self.ws_1 = np.random.rand(self.in_vector, self.hn_1) - 0.5\n",
    "        self.ws_2 = np.random.rand(self.hn_1, self.hn_2) - 0.5\n",
    "        self.ws_3 = np.random.rand(self.hn_2, 2) - 0.5\n",
    "        self.bias_1 = np.random.rand(self.hn_1) - 0.5\n",
    "        self.bias_2 = np.random.rand(self.hn_2) - 0.5\n",
    "        self.bias_3 = np.random.rand(2) - 0.5\n",
    "        return self.ws_1, self.ws_2, self.ws_3, self.bias_1, self.bias_2, self.bias_3\n",
    "        \n",
    "    def set_weights(self, ws_1, ws_2, ws_3, bias_1, bias_2, bias_3):\n",
    "        \"\"\"\n",
    "        Setting weights and biases for the neural net.\n",
    "        \n",
    "        Parameters:\n",
    "            ws_1 (numpy matrice / array ): weights for first layer.\n",
    "            ws_2 (numpy matrice / array ): weights for second layer.\n",
    "            ws_3 (numpy matrice / array ): weights for thrid layer.\n",
    "            bias_1 (numpy array ): bias for first layer.\n",
    "            bias_2 (numpy array ): bias for second layer.\n",
    "            bias_3 (numpy array ): bias for third layer.\n",
    "        \"\"\"\n",
    "        self.ws_1 = ws_1\n",
    "        self.ws_2 = ws_2\n",
    "        self.ws_3 = ws_3\n",
    "        self.bias_1 = bias_1\n",
    "        self.bias_2 = bias_2\n",
    "        self.bias_3 = bias_3\n",
    "    \n",
    "    def set_hidden_nodes(self, in_vector, hn_1, hn_2):\n",
    "        \"\"\"\n",
    "        Setting how many nodes in input layer and on hidden layers.\n",
    "        \n",
    "        Parameters:\n",
    "            in_vector (int): nr of hidden nodes\n",
    "            hn_1 (int): nr of hidden nodes on first hidden layer\n",
    "            hn_2 (int): nr of hidden nodes on second hidden layer\n",
    "        \"\"\"\n",
    "        self.hn_1 = hn_1\n",
    "        self.hn_2 = hn_2\n",
    "        self.in_vector = in_vector\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"\n",
    "        Applies ReLU activating function.\n",
    "        \"\"\"\n",
    "        return (x > 0) * x\n",
    "    \n",
    "    def relu_grad(self, x):\n",
    "        \"\"\"\n",
    "        Applies ReLU gradient function and returns its result.\n",
    "        \"\"\"\n",
    "        return x > 0\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Applies Sigmoid function and returns its result.\n",
    "        \"\"\"\n",
    "        return special.expit(x)\n",
    "    \n",
    "    def sigmoid_grad(self, x):\n",
    "        \"\"\"\n",
    "        Applies Sigmoid gradient function and returns its result.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Applies Softmax function and returns its result.\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "        \n",
    "\n",
    "    def cross_entropy(self, p, y):\n",
    "        \"\"\"\n",
    "        A function that calculates Cross Entropy Loss and returns the result\n",
    "        \"\"\"\n",
    "        return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n",
    "    \n",
    "    def predict(self, x, get_all=False):\n",
    "        \"\"\"\n",
    "        Forward pass, prediction class of data.\n",
    "        \n",
    "        Parameters:\n",
    "            x (Numpy array): the vector preresenting the data.\n",
    "            get_all (boolean): \n",
    "                Set True if you want to return output of all layers.\n",
    "                Set false if you only want to return output of last layer.\n",
    "        Returns:\n",
    "            numpy.ndarray: output of last layer \n",
    "            OR\n",
    "            (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray): output of all layers.\n",
    "        \"\"\"\n",
    "        #forward pass/prediction\n",
    "        layer_1 = self.relu(x.dot(self.ws_1))\n",
    "        layer_2 = self.relu(layer_1.dot(self.ws_2))\n",
    "        layer_3 = layer_2.dot(self.ws_3)\n",
    "        layer_out = self.softmax(layer_3)\n",
    "        if get_all:\n",
    "            return layer_out, layer_3, layer_2, layer_1\n",
    "        else:\n",
    "            return layer_out\n",
    "    \n",
    "    def get_accuracy(self, y, p):\n",
    "        \"\"\"\n",
    "        Function that calculates accuracy after predicting all dataset\n",
    "        \"\"\"\n",
    "\n",
    "        acc = np.sum((y == np.argmax(p, axis=1))) / len(y)\n",
    "        return acc\n",
    "    \n",
    "\n",
    "    def predict_whole_set(self, x, y):\n",
    "        \"\"\"\n",
    "        Makes prediction on whole dataset and returns CEL, last layout outputs and accuracy.\n",
    "        \n",
    "        Parameters:\n",
    "            x (Numpy matrice): A Numpy matrice that contains vectors.\n",
    "            y (Numpy array): a Numpy array that containts corresponding label values.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: mean Cross Entropy Loss of each prediction.\n",
    "            numpy.ndarray: output of last layout for each prediction.\n",
    "            float: Accuracy of the prediction of the data set.\n",
    "        \"\"\"\n",
    "        output = np.apply_along_axis(self.predict, 1, x)\n",
    "        cel = self.cross_entropy(output, y)\n",
    "        acc = self.get_accuracy(y, output)\n",
    "        return cel, output, acc\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize and test our classifier to see how it predicts, if it works as expected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "classifier = Classifier(idfs, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.422734799603507\n",
      "0.5\n",
      "[[9.99995596e-01 4.40379506e-06]\n",
      " [9.99989505e-01 1.04947816e-05]\n",
      " [9.99983909e-01 1.60913010e-05]\n",
      " ...\n",
      " [9.94659878e-01 5.34012166e-03]\n",
      " [9.99873338e-01 1.26661609e-04]\n",
      " [9.99988110e-01 1.18898215e-05]]\n",
      "(12050, 16) (16, 8) (8, 2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "classifier.set_hidden_nodes(len(vocab), 16, 8)\n",
    "classifier.init_weights()\n",
    "cel_pre, output, acc_pre = classifier.predict_whole_set(X_valid, Y_valid)\n",
    "print(cel_pre)\n",
    "print(acc_pre)\n",
    "\n",
    "print(output)\n",
    "print(classifier.ws_1.shape, classifier.ws_2.shape, classifier.ws_3.shape)\n",
    "print(len(classifier.bias_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Chromosome\n",
    "This is the Chromosome. Chromosome keeps the weights and has some methods for mutation, cross.over and fitness calculation\n",
    "\n",
    "I have written own operator functions like`__eq__`. The comparisons will be made based on value of `self.fitness`. In the GAEngine I do compare the fitness values and sort the lists, so rather than calling lambda each time I cal max() or sorte(), I did implement those functions to make the code more clean and robust.\n",
    "\n",
    "The Chromosome has three essential functions:\n",
    "1. Assign fitness. This uses the classifier class above. It is based on accuracy after predicting the data on validation set. The goal is to increase the accuracy as much as possible.\n",
    "2. Cross over: it accepts another chromosome as parameter. New weights are produced from calculations which made based on weight values of both chromosomes (self and other). Then a new chromosome is iniitalized and returned. It's as parents give offspring to a child where childs genes are combination of parent's genes. But in this algorithm the Chromosomes has no genders, so anyone can make offspring from anyone.\n",
    "3. Mutation: here we make som random changes on the weights of the Chromosome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chromosome:\n",
    "    \"\"\"\n",
    "    This is a class Chromosome to use in Genetic Algorithm.\n",
    "    \n",
    "    It keeps weights and biases for a simple neural network.\n",
    "    It's attributes are weights, biases and fitness. Fitness is \n",
    "    measured by accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ws_1, ws_2, ws_3, bias_1, bias_2, bias_3):\n",
    "        \"\"\"\n",
    "        The constructor for Chromosome class.\n",
    "        \n",
    "        Parameters:\n",
    "            ws_1 (Numpy array / matrice): weights for first layer\n",
    "            ws_2 (Numpy array / matrice): weights for second layer\n",
    "            ws_3 (Numpy array / matrice): weights for third layer\n",
    "            bias_1 (Numpy array): bias for first layer\n",
    "            bias_2 (Numpy array): bias for second layer\n",
    "            bias_3 (Numpy array): bias for third layer\n",
    "        \"\"\"\n",
    "        self._fitness = 0\n",
    "        self._ws_1 = ws_1\n",
    "        self._ws_2 = ws_2\n",
    "        self._ws_3 = ws_3\n",
    "        self._bias_1 = bias_1\n",
    "        self._bias_2 = bias_2\n",
    "        self._bias_3 = bias_3\n",
    "    \n",
    "    # Getters and setters\n",
    "    @property\n",
    "    def ws_1(self):\n",
    "        \"\"\"\n",
    "        Get weights for first layer.\n",
    "        \"\"\"\n",
    "        return self._ws_1\n",
    "    \n",
    "    @ws_1.setter\n",
    "    def ws_1(self, value):\n",
    "        \"\"\"\n",
    "        Set weights for first layer.\n",
    "        \"\"\"\n",
    "        self._ws_1 = value\n",
    "    \n",
    "    @property\n",
    "    def ws_2(self):\n",
    "        \"\"\"\n",
    "        Get weights for second layer.\n",
    "        \"\"\"\n",
    "        return self._ws_2\n",
    "    \n",
    "    @ws_2.setter\n",
    "    def ws_2(self, value):\n",
    "        \"\"\"\n",
    "        Set weights for second layer.\n",
    "        \"\"\"\n",
    "        self._ws_2 = value\n",
    "    \n",
    "    @property\n",
    "    def ws_3(self):\n",
    "        \"\"\"\n",
    "        Get weights for third layer.\n",
    "        \"\"\"\n",
    "        return self._ws_3\n",
    "    \n",
    "    @ws_3.setter\n",
    "    def ws_3(self, value):\n",
    "        \"\"\"\n",
    "        Set weights for third layer.\n",
    "        \"\"\"\n",
    "        self._ws_3 = value\n",
    "    \n",
    "    @property\n",
    "    def bias_1(self):\n",
    "        \"\"\"\n",
    "        Get bias for first layer.\n",
    "        \"\"\"\n",
    "        return self._bias_1\n",
    "    \n",
    "    @bias_1.setter\n",
    "    def bias_1(self, value):\n",
    "        \"\"\"\n",
    "        Set bias for first layer.\n",
    "        \"\"\"\n",
    "        self._bias_1 = value\n",
    "    \n",
    "    @property\n",
    "    def bias_2(self):\n",
    "        \"\"\"\n",
    "        Get bias for second layer.\n",
    "        \"\"\"\n",
    "        return self._bias_2\n",
    "    \n",
    "    @bias_2.setter\n",
    "    def bias_2(self, value):\n",
    "        \"\"\"\n",
    "        Set bias for second layer.\n",
    "        \"\"\"\n",
    "        self._bias_2 = value\n",
    "    \n",
    "    @property\n",
    "    def bias_3(self):\n",
    "        \"\"\"\n",
    "        Get bias for third layer.\n",
    "        \"\"\"\n",
    "        return self._bias_3\n",
    "    \n",
    "    @bias_3.setter\n",
    "    def bias_3(self, value):\n",
    "        \"\"\"\n",
    "        Set bias for third layer.\n",
    "        \"\"\"\n",
    "        self._bias_3 = value\n",
    "    \n",
    "    @property\n",
    "    def fitness(self):\n",
    "        \"\"\"\n",
    "        Get fitness of the chromosome.\n",
    "        \"\"\"\n",
    "        return self._fitness\n",
    "    \n",
    "    @fitness.setter\n",
    "    def fitness(self, value):\n",
    "        \"\"\"\n",
    "        Set fitness of the chromosome.\n",
    "        \"\"\"\n",
    "        self._fitness = value\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is less than other.\n",
    "        \"\"\"\n",
    "        return self.fitness < other.fitness\n",
    "\n",
    "    def __le__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is less than or equal to other.\n",
    "        \"\"\"\n",
    "        return self.fitness <= other.fitness\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is equal to other.\n",
    "        \"\"\"\n",
    "        return self.fitness == other.fitness\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is not equal to other.\n",
    "        \"\"\"\n",
    "        return self.fitness != other.fitness\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is greater than or equal to other.\n",
    "        \"\"\"\n",
    "        return self.fitness >= other.fitness\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"\n",
    "        Check if fitness is greater than other.\n",
    "        \"\"\"\n",
    "        return self.fitness > other.fitness\n",
    "    \n",
    "    def assign_fitness(self, classifier, x, y):\n",
    "        \"\"\"\n",
    "        Function to assign / update fitness of the chromosome.\n",
    "        \n",
    "        Parameters:\n",
    "            classifier (Classifier): Classifier class to test weights on.\n",
    "            x (Numpy matrice): data set vectors\n",
    "            y (Numpy array): data set labels\n",
    "        \"\"\"\n",
    "        classifier.set_weights(self.ws_1, self.ws_2, self.ws_3,\n",
    "                               self.bias_1, self.bias_2, self.bias_3)\n",
    "        loss, _, acc = classifier.predict_whole_set(x, y)\n",
    "        self.fitness = acc #0 if loss <= 0 or loss == float('inf') else -math.log(1 / loss)\n",
    "    \n",
    "    # produce a new offspring from 2 parents\n",
    "    def crossover(self, other):\n",
    "        \"\"\"\n",
    "        Function to do crossover with another chromosome returning new chromosome.\n",
    "        \n",
    "        Parameters:\n",
    "            other (Chromosome): the chromosome to do cross-over with.\n",
    "        \n",
    "        Returns:\n",
    "            Chromosome: new chromosome object.\n",
    "        \"\"\"\n",
    "        r = 0.01\n",
    "        \n",
    "        ## SINGLE POINT\n",
    "#         i = random.randint(0, len(self.ws_1) - 1) \n",
    "#         ws_1_1 = np.append(self.ws_1[i:], other.ws_1[:i], axis=0)\n",
    "#         ws_1_2 = np.append(self.ws_1[:i], other.ws_1[i:], axis=0)\n",
    "        \n",
    "#         i = random.randint(0, len(self.ws_2) - 1) \n",
    "#         ws_2_1 = np.append(self.ws_2[i:], other.ws_2[:i], axis=0)\n",
    "#         ws_2_2 = np.append(self.ws_2[:i], other.ws_2[i:], axis=0)\n",
    "        \n",
    "#         i = random.randint(0, len(self.ws_3) - 1) \n",
    "#         ws_3_1 = np.append(self.ws_3[i:], other.ws_3[:i], axis=0)\n",
    "#         ws_3_2 = np.append(self.ws_3[:i], other.ws_3[i:], axis=0)\n",
    "        \n",
    "#         i = random.randint(0, len(self.bias_1) - 1) \n",
    "#         bias_1_1 = np.append(self.bias_1[i:], other.bias_1[:i], axis=0)\n",
    "#         bias_1_2 = np.append(self.bias_1[:i], other.bias_1[i:], axis=0)\n",
    "        \n",
    "        \n",
    "#         i = random.randint(0, len(self.bias_2) - 1) \n",
    "#         bias_2_1 = np.append(self.bias_2[i:], other.bias_2[:i], axis=0)\n",
    "#         bias_2_2 = np.append(self.bias_2[:i], other.bias_2[i:], axis=0)\n",
    "        \n",
    "#         i = random.randint(0, len(self.bias_3) - 1) \n",
    "#         bias_3_1 = np.append(self.bias_3[i:], other.bias_3[:i], axis=0)\n",
    "#         bias_3_2 = np.append(self.bias_3[:i], other.bias_3[i:], axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "         ## MULTI POINT\n",
    "#         ws_1 = self.ws_1.copy()\n",
    "#         n = int(self.ws_1.size / 2 + 1)\n",
    "#         index1 = np.random.choice(ws_1.shape[0], n, replace=True)\n",
    "#         index2 = np.random.choice(ws_1.shape[1], n, replace=True)\n",
    "#         ws_1[index1, index2] = other.ws_1[index1, index2]\n",
    "        \n",
    "#         self.ws_1[index1, index2]\n",
    "        \n",
    "#         ws_2 = self.ws_2.copy()\n",
    "#         n = int(self.ws_2.size / 2 + 1)\n",
    "#         index1 = np.random.choice(ws_2.shape[0], n, replace=True)\n",
    "#         index2 = np.random.choice(ws_2.shape[1], n, replace=True)\n",
    "#         ws_2[index1, index2] = other.ws_2[index1, index2]\n",
    "        \n",
    "#         ws_3 = self.ws_3.copy()\n",
    "#         n = int(self.ws_3.size / 2 + 1)\n",
    "#         index1 = np.random.choice(ws_3.shape[0], n, replace=True)\n",
    "#         index2 = np.random.choice(ws_3.shape[1], n, replace=True)\n",
    "#         ws_3[index1, index2] = other.ws_3[index1, index2]\n",
    "        \n",
    "#         bias_1 = self.bias_1.copy()\n",
    "#         n = int(self.bias_1.size / 2 + 1)\n",
    "#         index1 = random.sample(range(bias_1.size), n)\n",
    "#         bias_1[index1] = other.bias_1[index1]\n",
    "        \n",
    "#         bias_2 = self.bias_2.copy()\n",
    "#         n = int(self.bias_2.size / 2 + 1)\n",
    "#         index1 = random.sample(range(bias_2.size), n)\n",
    "#         bias_2[index1] = other.bias_2[index1]\n",
    "        \n",
    "#         bias_3 = self.bias_3.copy()\n",
    "#         n = int(self.bias_3.size / 2 + 1)\n",
    "#         index1 = random.sample(range(bias_3.size), n)\n",
    "#         bias_3[index1] = other.bias_1[index1]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## RANDOM UNIFORM\n",
    "        min_mat_1 = np.minimum(self.ws_1, other.ws_1)\n",
    "        max_mat_1 = np.maximum(self.ws_1, other.ws_1)\n",
    "        min_mat_2 = np.minimum(self.ws_2, other.ws_2)\n",
    "        max_mat_2 = np.maximum(self.ws_2, other.ws_2)\n",
    "        min_mat_3 = np.minimum(self.ws_3, other.ws_3)\n",
    "        max_mat_3 = np.maximum(self.ws_3, other.ws_3)\n",
    "        min_mat_4 = np.minimum(self.bias_1, other.bias_1)\n",
    "        max_mat_4 = np.maximum(self.bias_1, other.bias_1)\n",
    "        min_mat_5 = np.minimum(self.bias_2, other.bias_2)\n",
    "        max_mat_5 = np.maximum(self.bias_2, other.bias_2)\n",
    "        min_mat_6 = np.minimum(self.bias_3, other.bias_3)\n",
    "        max_mat_6 = np.maximum(self.bias_3, other.bias_3)\n",
    "        \n",
    "        ws_1 = np.random.uniform(min_mat_1-r, max_mat_1+r)\n",
    "        ws_2 = np.random.uniform(min_mat_2-r, max_mat_2+r)\n",
    "        ws_3 = np.random.uniform(min_mat_3-r, max_mat_3+r)\n",
    "        bias_1 = np.random.uniform(min_mat_4-r, max_mat_4+r)\n",
    "        bias_2 = np.random.uniform(min_mat_5-r, max_mat_5+r)\n",
    "        bias_3 = np.random.uniform(min_mat_6-r, max_mat_6+r)\n",
    "        \n",
    "        offspring1 = Chromosome(ws_1, ws_2, ws_3, bias_1, bias_2, bias_3)\n",
    "#         offspring1 = Chromosome(ws_1_1, ws_2_1, ws_3_1, bias_1_1, bias_2_1, bias_3_1)\n",
    "#         offspring2 = Chromosome(ws_1_2, ws_2_2, ws_3_2, bias_1_2, bias_2_2, bias_3_2)\n",
    "        return offspring1#, offspring2\n",
    "\n",
    "    # mutate the individual\n",
    "    def mutate(self):\n",
    "        \"\"\"\n",
    "        Function for mutation, changint the weights and bias value of the chromosome.\n",
    "        \"\"\"\n",
    "        ### Change random weights\n",
    "        n = np.random.randint(1, int(self.ws_1.size / 4))\n",
    "        index1 = np.random.choice(self.ws_1.shape[0], n, replace=True)\n",
    "        index2 = np.random.choice(self.ws_1.shape[1], n, replace=True)\n",
    "        self.ws_1[index1, index2] = np.random.uniform(-1,1, n)\n",
    "\n",
    "        n = np.random.randint(1, int(self.ws_2.size / 4))\n",
    "        index1 = np.random.choice(self.ws_2.shape[0], n, replace=True)\n",
    "        index2 = np.random.choice(self.ws_2.shape[1], n, replace=True)\n",
    "        self.ws_2[index1, index2] = np.random.uniform(-1,1, n)\n",
    "        \n",
    "        n = np.random.randint(1, int(self.ws_3.size / 4))\n",
    "        index1 = np.random.choice(self.ws_3.shape[0], n, replace=True)\n",
    "        index2 = np.random.choice(self.ws_3.shape[1], n, replace=True)\n",
    "        self.ws_3[index1, index2] = np.random.uniform(-1,1, n)\n",
    "        \n",
    "        n = np.random.randint(1, int(self.bias_1.size / 4))\n",
    "        index1 = random.sample(range(self.bias_1.size ), n)\n",
    "        self.bias_1[index1] = np.random.uniform(-1, 1, n)\n",
    "        \n",
    "        n = np.random.randint(1, int(self.bias_2.size / 2))\n",
    "        index1 = random.sample(range(self.bias_2.size ), n)\n",
    "        self.bias_2[index1] = np.random.uniform(-1, 1, n)\n",
    "        \n",
    "        n = np.random.randint(1, int(self.bias_3.size + 1))\n",
    "        index1 = random.sample(range(self.bias_3.size ), n)\n",
    "        self.bias_3[index1] = np.random.uniform(-1, 1, n)\n",
    "        \n",
    "        \n",
    "        ## INCREMENTING DECREMENTING\n",
    "#         self.ws_1 = self.ws_1 + np.random.uniform(-0.1, 0.1, size=self.ws_1.shape)*np.random.randint(0, 2, size=self.ws_1.shape)\n",
    "#         self.ws_2 = self.ws_2 + np.random.uniform(-0.1, 0.1, size=self.ws_2.shape)*np.random.randint(0, 2, size=self.ws_2.shape)\n",
    "#         self.ws_3 = self.ws_3 + np.random.uniform(-0.1, 0.1, size=self.ws_3.shape)*np.random.randint(0, 2, size=self.ws_3.shape)\n",
    "#         self.bias_1 = self.bias_1 + np.random.uniform(-0.1, 0.1, size=self.bias_1.shape)*np.random.randint(0, 2, size=self.bias_1.shape)\n",
    "#         self.bias_2 = self.bias_2 + np.random.uniform(-0.1, 0.1, size=self.bias_2.shape)*np.random.randint(0, 2, size=self.bias_2.shape)\n",
    "#         self.bias_3 = self.bias_3 + np.random.uniform(-0.1, 0.1, size=self.bias_3.shape)*np.random.randint(0, 2, size=self.bias_3.shape)\n",
    "\n",
    "        ## RANDOM UNIFORM\n",
    "#         self.ws_1 = np.random.uniform(-1, 1, size=self.ws_1.shape)\n",
    "#         self.ws_2 = np.random.uniform(-1, 1, size=self.ws_2.shape)\n",
    "#         self.ws_3 = np.random.uniform(-1, 1, size=self.ws_3.shape)\n",
    "#         self.bias_1 = np.random.uniform(-1, 1, size=self.bias_1.shape)\n",
    "#         self.bias_2 = np.random.uniform(-1, 1, size=self.bias_2.shape)\n",
    "#         self.bias_3 = np.random.uniform(-1, 1, size=self.bias_3.shape)\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Genetic Algorithm Engine\n",
    "\n",
    "Here I have divided the chromosome in two groups: elite and population.\n",
    "Elite contains the population with highest fitness. This is to make sure if we find a very fit chromosome we don't lose it again because of cross overs or mutations.\n",
    "\n",
    "And we make the mutations first. We make mutations on the least fit chromosomes. We don't want to mutate highest fit chromosomes. How many to mutate is also passed as a parameter. After mutations we determine the mutated chromosomes new fitness values and update elite list again.\n",
    "\n",
    "In the cross-over I use something similar to roulette wheel. How much of the crossover will be from elite will be passed as a parameter. \n",
    "Based on that value, I produce M number of offsprings only from the elites (chosen randomly). Elites are the list of most fit chromosomes. How many to keep in elites is also passed as a parmater to training function.\n",
    "\n",
    "And I produce N number of offsprings from the chromosomes randomly chosen from population. But here the random choice is weighted one, also it's higher probabilty to choose chromosomes with higher fitness values. After all the cross overs the old population is thrown out and new generation is produced from the offsprings. The number of the offsprings is equal to the number of the population to avoid decrease or increase in the population. Since we keep the most fit ones in the elite list, we don't lose winners by replacing the old generation with the new geneartion.\n",
    "\n",
    "After the cross-overs it determines the fitness values of the new population and update the elite list again.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAEngine:\n",
    "    \"\"\"\n",
    "    This is a class that runs Genetic Algorithm on higher level, operates on Chromosome objects.\n",
    "      \n",
    "    Attributes:\n",
    "        population (list): list of chromosomes.\n",
    "        generations (int): number of generations to run.\n",
    "        classifier (Classifier): the classifier to test the chromosomes (weights) on.\n",
    "        elite (list): a list of chromosomes with highest fitness so far.\n",
    "        nr_of_elites (int): how many elite chromosomes to keep in elite list.\n",
    "        x: data set to use when testing chromsomes with the classifier while assigning  fitness.\n",
    "        y: labels to use when testing chromsomes with the classifier while assigning  fitness.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        \"\"\"\n",
    "        The constructor for GAEngine class\n",
    "        \n",
    "        Parameters:\n",
    "            classifier (Classifier): the classifier to use to test the chromosomes (weights) on.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._population = []\n",
    "        self._generations = 0\n",
    "        self._classifier = classifier\n",
    "        self._elite = []\n",
    "        self.nr_of_elites = 0\n",
    "        self._x = []\n",
    "        self._y = []\n",
    "\n",
    "      \n",
    "    @property\n",
    "    def generations(self):\n",
    "        \"\"\"\n",
    "        Get number of generations.\n",
    "        \"\"\"\n",
    "        return self._generations\n",
    "    \n",
    "    @generations.setter\n",
    "    def generations(self, g):\n",
    "        \"\"\"\n",
    "        Set number of generations.\n",
    "        \"\"\"\n",
    "        self._generations = g\n",
    "    \n",
    "    @property\n",
    "    def population(self):\n",
    "        \"\"\"\n",
    "        Get population.\n",
    "        \"\"\"\n",
    "        return self._population\n",
    "    \n",
    "    @population.setter\n",
    "    def population(self, p):\n",
    "        \"\"\"\n",
    "        Set population.\n",
    "        \"\"\"\n",
    "        self._population = p\n",
    "    \n",
    "    @property\n",
    "    def elite(self):\n",
    "        \"\"\"\n",
    "        Get elite.\n",
    "        \"\"\"\n",
    "        return self._elite\n",
    "    \n",
    "    @elite.setter\n",
    "    def elite(self, e):\n",
    "        \"\"\"\n",
    "        Set elite.\n",
    "        \"\"\"\n",
    "        self._elite = e\n",
    "    \n",
    "    @property\n",
    "    def classifier(self):\n",
    "        \"\"\"\n",
    "        Get classifier.\n",
    "        \"\"\"\n",
    "        return self._classifier\n",
    "    \n",
    "    @classifier.setter\n",
    "    def classifier(self, cl):\n",
    "        \"\"\"\n",
    "        Set classifier.\n",
    "        \"\"\"\n",
    "        self._classifier = cl\n",
    "        \n",
    "    @property\n",
    "    def x(self):\n",
    "        \"\"\"\n",
    "        Get x (dataset vectors to test chromosomes on).\n",
    "        \"\"\"\n",
    "        return self._x\n",
    "    \n",
    "    @x.setter\n",
    "    def x(self, x):\n",
    "        \"\"\"\n",
    "        Set x (dataset vectors to test chromosomes on).\n",
    "        \"\"\"\n",
    "        self._x = x\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"\n",
    "        Get y (dataset labels to test chromosomes on).\n",
    "        \"\"\"\n",
    "        return self._y\n",
    "    \n",
    "    @y.setter\n",
    "    def y(self, y):\n",
    "        \"\"\"\n",
    "        Set y (dataset labels to test chromosomes on).\n",
    "        \"\"\"\n",
    "        self._y = y\n",
    "    \n",
    "    def make_initial_population(self, population_size):\n",
    "        \"\"\"\n",
    "        Making the initial population with random weight and bias values\n",
    "        \n",
    "        Parameters:\n",
    "            population_size: number of chromosomes to start with\n",
    "        \"\"\"\n",
    "        for i in range(population_size):\n",
    "            ws_1, ws_2, ws_3, bias_1, bias_2, bias_3 = self.classifier.init_weights()\n",
    "            self.population.append(Chromosome(ws_1, ws_2, ws_3, bias_1, bias_2, bias_3))\n",
    "    \n",
    "    # selection code goes here...\n",
    "    def do_crossover(self, elite_crossover_rate):\n",
    "        \"\"\"\n",
    "        Function to do crossover between chromosomes\n",
    "        \n",
    "        Parameters:\n",
    "            elite_crossover_rate: the rate of population size to produce offsprings from chromosomes in elite list.\n",
    "            \n",
    "        Produces new chromosomes for new generation, as many as size of population list. \n",
    "        Based on elite_cross_over rate some of the offsprings are produced from randomly chosen chromosomes\n",
    "        from elite list, others are produced from randomly chosen chromosomes from population list, but in this case\n",
    "        it's weighted random selection.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_population_size = int(len(self.population) / 1)\n",
    "        \n",
    "                \n",
    "        # Here we combine elitism selection with roulette wheel\n",
    "        # We carry some of the most fit over to the next generation.\n",
    "        # We do cross over with both the elite and other population\n",
    "        # Then we use roulette wheel because we want diversity too.\n",
    "        # We want diversity because it's hard to predict optimal weights\n",
    "        \n",
    "        no_of_elite_crossovers = int(elite_crossover_rate * new_population_size)\n",
    "        \n",
    "        other_offspring = new_population_size - no_of_elite_crossovers\n",
    "\n",
    "        new_generation = list()\n",
    "        \n",
    "        # Offsprings from the elite\n",
    "        for i in range(no_of_elite_crossovers):\n",
    "            parent1, parent2 = random.choices(self.elite, k=2)\n",
    "            offspring1 = parent1.crossover(parent2)\n",
    "            new_generation.append(offspring1)\n",
    "            #new_generation.append(offspring2)\n",
    "        \n",
    "        # Weighted random choice\n",
    "        fitness_values = [x.fitness for x in self.population]\n",
    "        \n",
    "        # Offsprings from other population\n",
    "        for i in range(other_offspring):\n",
    "            parent1, parent2 = random.choices(self.population, weights=fitness_values, k=2)\n",
    "            offspring1 = parent1.crossover(parent2)\n",
    "            new_generation.append(offspring1)\n",
    "            #new_generation.append(offspring2)\n",
    "            \n",
    "        \n",
    "        # The population is the new generation\n",
    "        self.population = new_generation\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def do_mutation(self, no_of_mutation):\n",
    "        \"\"\"\n",
    "        Function to do mutation on Chromosomes with lowest fitness.\n",
    "        \n",
    "        After mutation the fitness values of the mutated chromosomes are updated again.\n",
    "        \n",
    "        Parameters:\n",
    "            no_of_mutation: number of chromosomes to mutate.\n",
    "            x: dataset to test run\n",
    "        \"\"\"\n",
    "        for i in range(no_of_mutation):\n",
    "            ch = np.random.choice(self.population)\n",
    "            ch.mutate()\n",
    "            ch.assign_fitness(self.classifier, self.x, self.y)\n",
    "        self.population = sorted(self.population, reverse=True)\n",
    "    \n",
    "    \n",
    "    # fitness calculation goes here...\n",
    "    def assign_fitness(self):\n",
    "        \"\"\"\n",
    "        Function to assign / update fitness to all chromosomes.\n",
    "        \"\"\"\n",
    "        for ch in self.population:\n",
    "            ch.assign_fitness(self.classifier, self.x, self.y)\n",
    "        self.population = sorted(self.population, reverse=True)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_elite(self):\n",
    "        \"\"\"\n",
    "        Function to update elite list.\n",
    "        \"\"\"\n",
    "        if len(self.elite) == 0:\n",
    "            self.elite = self.population[-self.nr_of_elites:]\n",
    "            self.elite = sorted(self.elite, reverse=True)\n",
    "            return\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < self.nr_of_elites and j < len(self.population):\n",
    "            if self.elite[i] < self.population[j]:\n",
    "                self.elite[i] = self.population[j]\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        self.elite = sorted(self.elite, reverse=True)\n",
    "\n",
    "    \n",
    "    def get_best_chromosome(self):\n",
    "        \"\"\"\n",
    "        Function to get best elite from elite list.\n",
    "        \"\"\"\n",
    "        return max(self.elite)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING ROUTINE\n",
    "    def training_routine(self, init_population, nr_of_generations, \n",
    "                        nr_of_mutation, nr_of_elites, elite_crossover_rate, x, y):\n",
    "        \"\"\"\n",
    "        Function implementing Genetic Algorithm on higher level, the optimization routine.\n",
    "        \n",
    "        Parameters:\n",
    "            init_population (int): size of inital population\n",
    "            nr_of_generations (int): number of generations to run this algorithm on\n",
    "            nr_of_mutation (int): number of chromosomes to apply mutation on\n",
    "            nr_of_elites (int): number of chromosomes to keep in elite list\n",
    "            elite_crossover_rate (float): Rate of the offsprings that will be produced from\n",
    "                chromosomes in elite list.\n",
    "            x: data set to use when testing chromsomes with the classifier while assigning  fitness.\n",
    "            y: labels to use when testing chromsomes with the classifier while assigning  fitness.\n",
    "        \n",
    "        Returns:\n",
    "            list: Fitness value of the best chromosome on each generation.\n",
    "            Chromosome: Best chromosome after all the generations.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.make_initial_population(init_population)\n",
    "        self.generations = nr_of_generations\n",
    "        self.nr_of_elites = nr_of_elites\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        fits = list()\n",
    "\n",
    "\n",
    "        for i in range(self.generations):\n",
    "            self.assign_fitness()\n",
    "            self.update_elite()\n",
    "            fits.append(self.elite[-1].fitness)\n",
    "            \n",
    "            self.do_mutation(nr_of_mutation)\n",
    "            self.update_elite()\n",
    "            \n",
    "            self.do_crossover(elite_crossover_rate)\n",
    "            \n",
    "            \n",
    "\n",
    "        # Assign fitness last time before getting the best chromosome\n",
    "        self.assign_fitness()\n",
    "        self.update_elite()\n",
    "        fits.append(self.get_best_chromosome().fitness)\n",
    "        return self.get_best_chromosome(), fits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Optimizing classifier with Genetic Algorithm\n",
    "\n",
    "Here I just run the training_routine function of the GAEngine.\n",
    "\n",
    "It has following parameters:\n",
    "* init_population: The population to start with.\n",
    "* nr_of_generations: how many generations to run this\n",
    "* nr_of_mutation: how many mutations\n",
    "* nr_of_elites: how many to keep in self.elite.\n",
    "* elite_crossover_rate: the offspring rate is constant (equal to size of population). This determines how much of that offspring should be produced from chromosomes in self.elite, and how much from self.population.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Here I run the GA engine\n",
    "classifier = Classifier(idfs, vocab)\n",
    "classifier.set_hidden_nodes(len(vocab), 16, 8)\n",
    "\n",
    "ga = GAEngine(classifier)\n",
    "\n",
    "init_population = 40\n",
    "nr_of_generations = 20\n",
    "nr_of_mutation = 8\n",
    "nr_of_elites = 30\n",
    "elite_crossover_rate = 0.2\n",
    "\n",
    "ch, cels = ga.training_routine(init_population, \n",
    "                               nr_of_generations, \n",
    "                               nr_of_mutation,\n",
    "                               nr_of_elites, \n",
    "                               elite_crossover_rate, \n",
    "                               X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting increase of fitness in training\n",
    "x = np.linspace(0, len(cels), len(cels))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, cels, label='fitness')\n",
    "\n",
    "plt.xlabel(\"Generations\")\n",
    "plt.ylabel(\"fitness\")\n",
    "plt.title(\"Change of fitness (fitness = accuracy)\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation\n",
    "\n",
    "Here we use the weights of the most fit chromosome after the optimization using GA in our classifier.\n",
    "We classify the whole test set and print the results both in terms of accuracy and CEL to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.set_weights(ch.ws_1, ch.ws_2, ch.ws_3, ch.bias_1, ch.bias_2, ch.bias_3)\n",
    "cel_post, output_post, acc_post = classifier.predict_whole_set(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training:  0.5\n",
      "Accuracy after training:  0.5356\n",
      "CEL before training 6.422734799603507\n",
      "CEL after training 2.1492307597562395\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy before training: \", acc_pre)\n",
    "print(\"Accuracy after training: \", acc_post)\n",
    "print(\"CEL before training\", cel_pre)\n",
    "print(\"CEL after training\", cel_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12859395 0.87140605]\n",
      " [0.92903263 0.07096737]\n",
      " [0.4692415  0.5307585 ]\n",
      " ...\n",
      " [0.51756871 0.48243129]\n",
      " [0.7417245  0.2582755 ]\n",
      " [0.20196091 0.79803909]]\n"
     ]
    }
   ],
   "source": [
    "print(output_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.50573363e-02  2.49006506e-01  1.00921911e-01 ... -1.68969283e-01\n",
      "   1.90766101e-01  7.16182785e-02]\n",
      " [ 1.08101057e-02  2.60611186e-01  1.47620163e-01 ...  1.77777810e-01\n",
      "  -3.30098800e-01  3.41583497e-04]\n",
      " [ 7.00772645e-02  2.91268502e-02 -8.90053026e-02 ... -3.77387863e-02\n",
      "  -8.38042218e-02 -1.64415362e-02]\n",
      " ...\n",
      " [ 5.12031417e-02 -2.50931768e-01 -1.14750035e-01 ...  5.09964245e-02\n",
      "   1.22898352e-01  3.48265685e-01]\n",
      " [ 3.30192286e-01 -6.88468314e-02 -2.97932042e-01 ... -5.01929741e-02\n",
      "  -2.43995298e-01  7.06645100e-02]\n",
      " [-1.27097465e-01  1.39104670e-01 -2.45763281e-01 ... -1.42467772e-01\n",
      "   4.57803825e-02 -6.28404951e-02]] [[-0.07317585 -0.13908267  0.03349777 -0.07302634  0.24870912 -0.14010279\n",
      "   0.01895544  0.0488529 ]\n",
      " [ 0.23687829 -0.07198451 -0.07947411 -0.30854593 -0.02466758 -0.30523101\n",
      "  -0.04987234  0.19287442]\n",
      " [ 0.07726066 -0.06018223  0.03785584  0.15336374  0.28459646 -0.16827788\n",
      "   0.1743199   0.06526453]\n",
      " [-0.11619147 -0.05301942  0.13560473  0.12546606 -0.00195059  0.18668376\n",
      "   0.16783666  0.16764849]\n",
      " [-0.03094066  0.35452531  0.06739504 -0.15325804 -0.20174805 -0.00293963\n",
      "  -0.19144769 -0.37072216]\n",
      " [ 0.01981871  0.11981076  0.24737002  0.05085373 -0.3047779  -0.06587786\n",
      "  -0.18717841  0.04329938]\n",
      " [ 0.15428482  0.14292071 -0.04232616  0.18577531 -0.13957189 -0.22066604\n",
      "   0.14120677 -0.21635439]\n",
      " [ 0.37083903  0.14233451  0.1614599   0.0416527  -0.24175643  0.04553028\n",
      "   0.005267   -0.19094167]\n",
      " [ 0.23475864  0.10723764  0.15240674  0.42243968 -0.05512387  0.22530975\n",
      "   0.07542257 -0.03299546]\n",
      " [-0.01585289 -0.294575   -0.32972158  0.02316102 -0.17221947 -0.01599196\n",
      "   0.04382504  0.0421428 ]\n",
      " [-0.07840385  0.27827469  0.00131098  0.0418137   0.01757408  0.01635471\n",
      "  -0.27741747 -0.09516908]\n",
      " [-0.15051092  0.01314895  0.08926148  0.14579951 -0.00807542 -0.00646271\n",
      "   0.09155871 -0.16826807]\n",
      " [ 0.17410376 -0.15915622 -0.16895662  0.36729289  0.0599362   0.09664376\n",
      "   0.11911835 -0.05520802]\n",
      " [-0.10640232  0.02453997  0.12395201  0.00604766  0.02134066 -0.14203935\n",
      "  -0.21724647 -0.23268704]\n",
      " [ 0.07251033 -0.29751607 -0.14161526  0.18428591  0.2115578  -0.17879654\n",
      "   0.13404504 -0.086066  ]\n",
      " [ 0.08190256 -0.0085005   0.25403849  0.15394465 -0.10323598  0.01893091\n",
      "   0.00762405 -0.14693886]] [[-0.19597422  0.2157668 ]\n",
      " [-0.05622153  0.08735056]\n",
      " [ 0.1170651   0.37358748]\n",
      " [ 0.18228843 -0.19629393]\n",
      " [ 0.1968049   0.28972459]\n",
      " [ 0.04090493  0.13872808]\n",
      " [-0.10568935 -0.10660436]\n",
      " [ 0.28954828  0.19248504]]\n"
     ]
    }
   ],
   "source": [
    "print(ch.ws_1, ch.ws_2, ch.ws_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.init_weights()\n",
    "\n",
    "classifier.ws_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel_post, output_post, acc_post = classifier.predict_whole_set(X_test, Y_test)\n",
    "\n",
    "print(\"Accuracy before training: \", acc_pre)\n",
    "print(\"Accuracy after training: \", acc_post)\n",
    "print(\"CEL before training\", cel_pre)\n",
    "print(\"CEL after training\", cel_post)\n",
    "\n",
    "print(classifier.ws_1, classifier.ws_2, classifier.ws_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 PyTorch test\n",
    "\n",
    "The assignment asks to build own classifier in Python without using libraries like PyTorch or Tensorflow. Thus I build my own classifier class as seen above.\n",
    "\n",
    "But I wanted to compare training weights using GA vs using SDG. I didn't want to build SDG training algorithm without libraries since the assignment doesn't ask for building a SDG trainer. Thus to make the comparison I used PyTorch here.\n",
    "\n",
    "Below is the \"training with SDG\" routine using PyTorch, and the results shown in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def training(num_epochs, model, optimizer, criterion, train_loader, valid_loader, vocab_length):\n",
    "    results = list()\n",
    "    accuracies = list()\n",
    "    valid_results = list()\n",
    "    valid_accuracies = list()\n",
    "\n",
    "    #model.float()\n",
    "    for ep in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        valid_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_valid = 0\n",
    "        correct_valid = 0\n",
    "        model.train() # Set model into training mode\n",
    "        for batch in train_loader:\n",
    "            # We extract the images and labels from the batch\n",
    "            vector = batch[:, :vocab_length]\n",
    "            labels = batch[:, vocab_length]\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            # This will prevent the gradient descents from the previous batches to accumulate. Without this the weight will get updated with the sum of\n",
    "            # all previos gradient descents, instead of the gradient descents on the current batch.\n",
    "            optimizer.zero_grad() \n",
    "            output = model(vector) # prediction / output from the model\n",
    "\n",
    "            loss = criterion(output, labels)  # We calculate the loss here\n",
    "            loss.backward() # Computes the derivative of the loss using backpropagation.\n",
    "            optimizer.step() # We update the weights\n",
    "\n",
    "            running_loss += loss.item() # Sum the loss here\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        else:\n",
    "            results.append(running_loss / len(train_loader))  # We append the mean loss into a list\n",
    "            accuracies.append(correct / total)\n",
    "    \n",
    "        # Validation part\n",
    "        model.eval()\n",
    "        with torch.no_grad(): # This will prevent calculation of gradient descents\n",
    "            for batch in valid_loader:\n",
    "                vector = batch[:, :vocab_length]\n",
    "                labels = batch[:, vocab_length]\n",
    "                valid_output = model(vector) # Prediciton\n",
    "                loss = criterion(valid_output, labels.long()) # Calculation of loss\n",
    "                valid_loss += loss.item() # Sum the loss\n",
    "\n",
    "                _, predicted = torch.max(valid_output.data, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "            else:\n",
    "                valid_results.append(valid_loss / len(valid_loader)) # Here we calculate the mean loss\n",
    "                valid_accuracies.append(correct_valid / total_valid)\n",
    "    return results, accuracies, valid_results, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing function\n",
    "\n",
    "def testing(model, criterion, test_loader, vocab_length):\n",
    "    model.eval() # Set model into evaluation model\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # This will prevent calculation of gradient descents\n",
    "        for batch in test_loader:\n",
    "            vector = batch[:, :vocab_length]\n",
    "            labels = batch[:, vocab_length]\n",
    "            valid_output = model(vector) # Prediciton\n",
    "            loss = criterion(valid_output, labels.long()) # Calculation of loss\n",
    "            test_loss += loss.item() # Sum the loss\n",
    "            \n",
    "            _, predicted = torch.max(valid_output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        else:\n",
    "            mean_test_loss = test_loss / len(test_loader) # Here we calculate the mean loss\n",
    "            accuracy = correct / total\n",
    "    return mean_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the learning rate and epoch.\n",
    "learning_rate = 0.00003\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-9f93445c3650>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_torch = torch.tensor(train_torch, dtype=torch.float)\n",
      "<ipython-input-56-9f93445c3650>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_torch = torch.tensor(valid_torch, dtype=torch.float)\n",
      "<ipython-input-56-9f93445c3650>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_torch = torch.tensor(test_torch, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Buildint torch values\n",
    "\n",
    "train_torch = np.c_[X_train, Y_train]\n",
    "\n",
    "train_torch = torch.from_numpy(train_torch)\n",
    "train_torch = torch.tensor(train_torch, dtype=torch.float)\n",
    "\n",
    "valid_torch = np.c_[X_valid, Y_valid]\n",
    "\n",
    "valid_torch = torch.from_numpy(valid_torch)\n",
    "valid_torch = torch.tensor(valid_torch, dtype=torch.float)\n",
    "\n",
    "test_torch = np.c_[X_test, Y_test]\n",
    "\n",
    "test_torch = torch.from_numpy(test_torch)\n",
    "test_torch = torch.tensor(test_torch, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_torch, batch_size=1, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_torch, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_torch, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model which is same as the model we'll use in GA\n",
    "model = nn.Sequential(nn.Linear(len(vocab), 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, 8),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(8,2)\n",
    "                     )\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-14579e24792d>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Training with SDG using the model\n",
    "sgd_results, accuracies, sgd_results_valid, accuracies_valid = training(num_epochs, model, optimizer, criterion, train_loader, valid_loader, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sgd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAE9CAYAAACcH89FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABNyklEQVR4nO3dd3hUVf7H8feZSS+kkkISCAklhBqIVJEACqgIKlgQFVbXtrp2d3XLT3ddu+ta14Io6qqACCIKKNKb9N6rEHrvLeT8/rghJCGJgEkm5fN6nnmSuXPune9cIHxy7jnnGmstIiIiIlL5uDxdgIiIiIiUDgU9ERERkUpKQU9ERESkklLQExEREamkFPREREREKikFPREREZFKysvTBZRXkZGRNjExsVTf48iRIwQGBpbqe1QkOh9n6Vzkp/ORn87HWToX+el85FeVzse8efN2W2urF9yuoFeExMRE5s6dW6rvMWnSJDIyMkr1PSoSnY+zdC7y0/nIT+fjLJ2L/HQ+8qtK58MY80th23XpVkRERKSSUtATERERqaQU9EREREQqKY3RExERkVJx6tQpMjMzOX78uEfePyQkhBUrVnjkvUuLn58f8fHxeHt7n1d7BT0REREpFZmZmQQHB5OYmIgxpszf/9ChQwQHB5f5+5YWay179uwhMzOT2rVrn9c+unQrIiIipeL48eNERER4JORVRsYYIiIiLqiHVEFPRERESo1CXsm60POpoCciIiKV0p49e2jWrBnNmjUjJiaGuLi43OcnT54sdt+5c+fy4IMP/up7tG3btqTKLRUaoyciIiKVUkREBAsXLgTgmWeeISgoiMcffzz39aysLLy8Co9C6enppKen/+p7zJgxo0RqLS3q0SvAGHONMeaDAwcOlOr7TF2ziznbs9h24Fipvo+IiIic1b9/fx599FE6duzIn//8Z2bPnk3btm1JS0ujbdu2rFq1CnDuqtG9e3fACYl33HEHGRkZJCUl8eabb+YeLygoKLd9RkYGvXv3JiUlhb59+2KtBWD06NGkpKRw6aWX8uCDD+YetyyoR68Aa+0oYFR6evpdpfk+g6ZvZPzKE7yzcAIx1fxIqxlKs4RQ0mqG0TguBH8fd2m+vYiISJW1evVqfvrpJ9xuNwcPHmTKlCl4eXnx008/8Ze//IWvv/76nH1WrlzJxIkTOXToEPXr1+e+++47Z4mTBQsWsGzZMmrUqEG7du2YPn066enp3HPPPUyZMoXatWvTp0+fsvqYgIKex7x7awv+9/1EXJFJLNi8nwWb9jNm6XYA3C5Dg9hg0hLCcgNg7chADWgVEZEK6x+jlrF868ESPWZqjWo8fU3DC97vhhtuwO12OlQOHDhAv379WLNmDcYYTp06Veg+V199Nb6+vvj6+hIVFcWOHTuIj4/P16Zly5a525o1a8bGjRsJCgoiKSkpdzmUPn368MEHH1xwzRdLQc9DfLYvoBlrad66A/3bOX/4ew6fYGFO6FuweR8jFmzhs5+dexSHBng7PX4JYTSrGUqz+FBCAs5vsUQRERE5KzAwMPf7v//973Ts2JERI0awceNGMjIyCt3H19c393u3201WVtZ5tTlz+dZTFPQ8ZcorNF89BpY9C7XaQlIHImpfRuf6DencIBqA09mWtTsPs3DzPif8bdrP5NWrOfN3Jrl6IGk1nV6/tIQw6kUH4eXWsEsRESl/LqbnrSwcOHCAuLg4AAYNGlTix09JSWH9+vVs3LiRxMREhgwZUuLvURwFPU/p+TbLvn+fhgG7YcMUWPODs90/HGq3h9qX4a7dgfrRdagfE8xNl9QE4NDxUyzOPJDT87ePiSt3MmxeprOrt5sm8SF5wl8oUdX8PPUJRUREyr0//elP9OvXj9dee41OnTqV+PH9/f3573//S7du3YiMjKRly5Yl/h7FUdDzlMBIdkW1gzNdxAe2OIFvwxTYMBmWj3S2B9eA2pflPoJDE2hXJ5J2dSIB53Yom/ceY8GZXr/N+xk4bT2nTjvdfnGh/jTLCX1pNcNoWKMaft6a6CEiIlXLM888U+j2Nm3asHr16tznzz77LAAZGRm5l3EL7rt06dLc7w8fPnxOe4C333479/uOHTuycuVKrLXcf//957VsS0lR0CsvQuKgWR/nYS3sXX82+K39CRYPdtqF1c4X/ExQFDUjAqgZEUDPZk7X8/FTp1m29SALNu3LHfP3/eJtAHi7Damx1fJd8k0I99dEDxERkVIyYMAAPvnkE06ePElaWhr33HNPmb23gl55ZAxEJDuP9N9BdjbsWnE2+C0bAfM/cdpGpZ4NfrXagX8oft5uWtQKo0WtsNxD7jx4PHd278LN+xgyZzODZmwEICLQh2YJoVxSO5weTWtQI9TfAx9aRESkcnrkkUd45JFHPPLeCnoVgcsF0Q2dR+v74HQWbF8E6yc7wW/eJzDrPTAuiG2aE/w6QM3W4OPMLIqq5kfXhjF0bRgDQNbpbFbvOJx7yXfh5v2MX7mTl8aupH3d6tyYHs8VqdH4eukyr4iISEWloFcRub0groXzaP8oZJ2AzLlne/xm/hemvwEub4i/5GyPX3w6eDlTv73cLlJrVCO1RjX6tqoFwOa9Rxk2L5Nh8zJ54IsFhAZ4c22zOG5Ij6dhjRBPfmIRERG5CAp6lYGXLyS2cx4dn4KTR2DTzLPBb/JLMPlF8PJ3evmSOjjBL7YZuM722CWEB/DIFfV4qHNdZqzbw9C5m/li9iYGzdhIwxrVuDE9gZ7NahAa4OO5zyoiIiLnTUGvMvIJhDqXOw+AY/vglxlO6Fs/GX56xtnuG+KEwzM9flGpYAwul+HSupFcWjeSA0dP8e2iLQydm8nT3y7jue9X0KVhNDemO7N/3S5N4hARESmvtLpuVeAfBilXw5Uvwf0/w+NroNdAaHgt7FwBY5+Ed9vCaw1gyitwdG/uriEB3tzWJpFRf7yU0Q+2p2/rmkxfu5vbP5pN+5cm8NqPq9i056jnPpuIiEgRrrrqKn744Yd8215//XX+8Ic/FNo+IyODuXPn5u67f//+c9o888wzvPrqq8W+7zfffMPy5ctzn//f//0fP/300wVWXzLUo1cVBUVB497OA2D/JtgwFZZ+DRP+BVNfg2Z9nYkfEcm5u6XWqMbTNRry5JUpjF+xk6FzN/P2xLW8OWEtbZIiuPGSeLo1jMXfRxM4RETE83r37s3gwYPp2rVr7rbBgwfzyiuv/Oq+o0ePvuj3/eabb+jevTupqakA/POf/7zoY/1W6tETCK0JaX3htuFw30xoeL2zfMtbLWBwX9g0K19zXy83VzWOZdDvWjL9yU480bU+Ww8c45Ehi2j53E/8ZcQSFm7e7/H7+4mISNXWs2dPvvvuO06cOAHAxo0b2bp1K1988QXp6ek0bNiQp59+utB9ExMT2b17NwDPPfcc9evX5/LLL2fVqlW5bQYMGMAll1xC06ZN6dWrF0ePHmXGjBl8++23PPHEEzRr1ox169bRv39/hg0bBsD48eNJS0ujcePG3HHHHbm1JSYm8vTTT9O8eXMaN27MypUrS+QcKOhJftGpcO078PASZ0bvxmnwURf48HJY9g1kn87XPDbEn/s71mHS4xkMubs1XRrGMGL+Fq59Zzpd/jOFAVPWs/vwCc98FhERqdIiIiJo2bIlY8eOBZzevJtuuonnnnuOuXPnsnjxYiZPnszixYuLPMa8efMYPHgwCxYsYPjw4cyZMyf3teuvv545c+awaNEiGjRowMCBA2nbti09evTglVdeYeHChSQnn70ydvz4cfr378+QIUNYsmQJWVlZvPvuu7mvR0ZGMn/+fO67775fvTx8vnTpVgoXHAOd/w/aPwYLv4CZ78BX/SC0FrT+A6TdCr5Buc2NMbRKiqBVUgTP9Ejl+8XbGDp3M8+NXsFLY1fSKSWKG9MTyKhfHS+3fr8QEalyxjwJ25eU7DFjGsOVLxbbpE+fPgwePJiePXsyePBgPvroI4YOHcoHH3xAVlYW27ZtY/ny5TRp0qTQ/adOncp1111HQEAAAD169Mh9benSpfztb39j//79HD58ON8l4sKsWrWK2rVrU69ePQD69evHO++8w8MPPww4wRGgRYsWDB8+/LxOwa9R0JPi+QRCy7sg/Q5YNRpmvAVj/wyTnne2tbwHqsXm2yXYz5ubW9bk5pY1WbvzEF/NzeTr+Vv4cfkOqgf7cn3zOG5okUCdqKAi3lRERKRkXHvttTz66KPMnz+fY8eOERYWxquvvsqcOXMICwujf//+HD9+vNhjFHWb0P79+/PNN9/QtGlTBg0axKRJk4o9zq8NafL1dda6dbvdZGVlFdv2fCnoyflxuaHBNc5j8xyY+ZazKPOMt51JHW0egJhG5+xWJyqYp65qwONd6zNp1S6Gzt3Mh1M38P7k9bSoFcaN6fFc3aQGQb76qygiUqn9Ss9baQkKCiIjI4M77riDPn36cPDgQQIDAwkJCWHHjh2MGTOGjIyMIve/7LLL6N+/P08++SRZWVmMGjUq9161hw4dIjY2llOnTvH5558TF+fccz44OJhDhw6dc6yUlBQ2btzI2rVrqVOnDp999hkdOnQolc99hv53lQuXcAkkfAp7N8DP78KC/8GiLyGpI7R9AJI7O/frzcPb7eKK1GiuSI1m16ETfLNgC0PmbubPXy/hmW+Xc3WTWOq6T9PB2iJ/cxIREbkYffr04frrr2fw4MGkpKSQlpZGw4YNSUpKol27dsXu27x5c2666SaaNWtGrVq1aN++fe5rzz77LK1ataJWrVo0btw4N9zdfPPN3HXXXbz55pu5kzAA/Pz8+Pjjj7nhhhvIysrikksu4d577y2dD53DVKWZkcaYJOCvQIi1tndxbdPT0+2ZtXRKy6RJk4r9LaLCOLYP5n4Ms96Hw9udhZfb3A+Nb8i95VphrLUs3LyfoXMzGbVoK4dPZJEYEcAN6Qn0ah5PTIhfGX6I8qXS/N0oITof+el8nKVzkV95Ox8rVqygQYMGHnv/Q4cOERwc7LH3Ly2FnVdjzDxrbXrBtqU6Kt4YE2qMGWaMWWmMWWGMaXORx/nIGLPTGLO0kNe6GWNWGWPWGmOeLO441tr11to7L6YGKYZ/mDND9+ElcO17gIGR98PrjWHKq/kWYM7LGENazTBeuL4xc/56OXc19iEmxI9XflhF2xfH0//j2SzdcqBsP4uIiEglUtrTH98AxlprU4CmwIq8LxpjoowxwQW21SnkOIOAbgU3GmPcwDvAlUAq0McYk2qMaWyM+a7AI6pkPpIUycsHmvWB+6bDbSMguhFMeBb+0xC+fxz2ri9yV38fN+3ivBl8dxsmP5HB/R3rsHTLAXq+M53nR6/g2MnTRe4rIiIihSu1oGeMqQZcBgwEsNaetNbuL9CsAzDSGOOXs89dwJsFj2WtnQIU1i3UElib01N3EhgM9LTWLrHWdi/w2FliH06KZwwkd8q/APO8QfBmcxhy6zkLMBdUKyKQx7rUZ/yjGdzQIp4Ppqyny+uTmbpmV9nULyIiUkmUZo9eErAL+NgYs8AY86ExJjBvA2vtV8BYYLAxpi9wB3DjBbxHHLA5z/PMnG2FMsZEGGPeA9KMMU8V0eYaY8wHBw7okmGJOLMA8yNLncu7G6YWuwBzXiEB3rzYqwmD726Nt8vFbQNn8+jQhew9crLs6hcRkd+kKs0FKAsXej5LM+h5Ac2Bd621acAR4JwxdNbal4HjwLtAD2vt4Qt4j8KmZxZ5Bqy1e6y191prk621LxTRZpS19u6QkJALKEN+1ZkFmB9dDle9Ckd2OQswv5nmTOI4UfQfe+ukCEY/1J4/dqrDtwu3cvlrk/lmwRb98BARKef8/PzYs2ePfl6XEGste/bswc/v/CcrlubyKplAprX2zHW6YRQS9Iwx7YFGwAjgaeCBC3yPhDzP44GtF1WtlI28CzCv/B5mvg1j/gQTn6N21OXQvP45CzAD+Hm7eaxLfbo3qcGTwxfz8JCFDF+wheeubURCeIAHPoiIiPya+Ph4MjMz2bXLM0Nvjh8/fkGhqCLw8/MjPj7+vNuXWtCz1m43xmw2xtS31q4COgPL87YxxqQBA4CrgQ3A/4wx/7LW/u0832YOUNcYUxvYAtwM3FJiH0JKj8sNqT2cR84CzDWXj4DXv3WWZWlzf6ELMNePCWbYvW3538+/8PLYlXT5zxQe61KP/m0TdWs1EZFyxtvbm9q1a3vs/SdNmkRaWprH3r88KO3/Gf8IfG6MWQw0A54v8HoAcIO1dp21NhvoB/xS8CDGmC+BmUB9Y0ymMeZOAGttFk4P4A84M3qHWmuXldaHkVKScAnc+CmzWr3r9PQtHwnvtYNPr4WdK89p7nYZ+rVNZNyjHWhXJ4J/fb+C6/47Q0uxiIiIFFCqQc9au9Bam26tbWKtvdZau6/A69OttUvyPD9lrR1QyHH6WGtjrbXe1tp4a+3APK+NttbWyxl391xpfh4pXcf9Y+Cql+HRZdD5adi+GD7oAHMGQiHjO2qE+jPg9nTeuaU52w4cp+c703lBS7GIiIjk0rUuKX/OLMB830yo1Ra+fxQG94Uje85paozh6iaxjH+0Aze0iOf9Kevp+voUpq3Z7YHCRUREyhcFPSm/gqOh79fQ9XlY8yO82xbWTyq06ZmlWL68qzVul+HWgbN4dOhC9mkpFhERqcIU9KR8c7mciRl3jQe/as64vXFPQ1bhAa5NcgRjHmrPAx2dpVg6aykWERGpwhT0pGKIbQp3T4IW/WD6686iy3vWFdrUz9vN413r892Dl1IzPICHhyyk/8dz2Lz3aJmWLCIi4mkKelJx+ATCNW/AjZ/B3g3wXntY8L9CJ2oApMRU4+v72vL0NanM2biXLv+ZwodT15N1OruMCxcREfEMBT2peFJ7wH0zIK45jLwfhv0Oju0vtKnbZfhdu9qMe7QDbZLPLsWybKuWYhERkcpPQU8qppA4uH2kswzLilHw3qXwy8wim8eF+jOwXzpv9Ulj24Fj9Hh7Oi+M0VIsIiJSuSnoScXlcjvLsNzxI7i8YNBVMPF5OJ1VaHNjDNc0rcFPj3agV/M43p+8nm5vTGH6Wi3FIiIilZOCnlR88S3g3qnQ5CaY/JIT+PZtLLJ5aIAPL/duyhd3tcJlDH0/nMVjQxdpKRYREal0FPSkcvANhuveg14DYecKZ6LGkmHF7tI2OZIxD7Xn/o7JjFy4hctfm8zIhVqKRUREKg8FPalcGveGe6dBVAP4+k4YcS+cOFRkcz9vN090TWHUHy8lPjyAhwZrKRYREak8FPSk8gmrBf1HQ4cnYfEQp3cvc16xuzSIrcbwQpZiOZ2t3j0REam4FPSkcnJ7QcennMCXneUssDz135Bd9Czbwpdimc7yrQfLsHAREZGSo6AnlVutNs6l3AbXwPh/wqc94cCWYnfJuxTL1v3HuObtabw4ZiXHT2kpFhERqVgU9KTy8w+F3h9Dz//ClvnwbltY/m2xuxRciuW9yevo+voUFmzaVzY1i4iIlAAFPakajIG0vs4yLOG1YehtMOohOHmk2N3yLsWSbS23DJildfdERKTCUNCTqiUi2Vlgud3DMO8T+CADti361d3aJkfy9X1tqRkewO8GzWH8ih2lXqqIiMhvpaAnVY+XD1zxD7j9G2fplQ8vh5nvQHZ2sbtFBfsx+O7W1I8O5p7P5vH94m1lU6+IiMhFUtCTqispA+6dDnWugB/+Ap/3hkPF99SFBfrw+V2taJYQyh+/nM+weZllU6uIiMhFUNCTqi0wAm7+HK5+DX6Z4UzUWP1jsbtU8/Pm0ztb0jY5kse/WsRnMzeWTa0iIiIXSEFPxBi45E64exIEx8AXN8DoP8Gp40XuEuDjxYf90rm8QRR/H7mM9yevK7t6RUREzpOCnsgZUSnw+/HQ+g8w+30Y0Mm5b24R/LzdvHtrC7o3ieWFMSt5bdxq3SdXRETKFQU9kby8/aDbC9B3GBzZ6czKnT0Aighw3m4Xb9ycxg0t4nlz/Bqe+36Fwp6IiJQbCnoihal7Bdw3AxIvhdGPw+Bb4MieQpu6XYaXejWhf9tEPpy2gb9+s5Rs3SNXRETKAQU9kaIERcEtX0G3F2HtT85EjV9mFNrU5TI8fU0q92Uk88WsTTz+1SKyThe/XIuIiEhpU9ArwBhzjTHmgwMHDni6FCkPXC5ofR/cNQF8g+DTa2HFqEKbGmP4c7cUHu9Sj+ELtvDAFws4maWwJyIinqOgV4C1dpS19u6QkBBPlyLlSUxjuHMcxDaBobc7d9UowgOd6vL37qmMXbaduz+by/FTp8uwUBERkbMU9ETOV0A43D4SkjvDqAdhyitFTtK489LavHB9Yyav3kX/j2dz+ERWGRcrIiKioCdyYXwCoc+X0OQmmPAvGPtkkbdO69OyJq/f1Iw5G/dx28BZHDh6qoyLFRGRqk5BT+RCub3h2veg9f0w6z0YfhdknSy0ac9mcfy3b3OWbTlInwE/s+fwiTIuVkREqjIFPZGL4XJB1+fg8mdg6TD48iY4cbjQpl0bxjCgXzrrdh3mxvdnsv1A0XfcEBERKUkKeiIXyxi49BHo8TasnwSf9ihyrb0O9arzyR0t2X7gODe+P5PNe4+Wba0iIlIlKeiJ/FbNb4ObPocdy+CjrrB/U6HNWidF8Pldrdl/9CQ3vj+TdbsK7wEUEREpKQp6IiUh5Sq4bQQc3gkDuxZ5j9xmCaEMvrsNJ7Oyuen9mazYdrCMCxURkapEQU+kpNRqC78bDTYbPuoGm2YV2iy1RjWG3NMGL5eLmz/4mUWb95dtnSIiUmUo6ImUpJhGcOcPEBABn/aE1T8U2qxOVBBf3duGav5e9P1wFrM37C3jQkVEpCpQ0BMpaWGJcMcPUL0+fNkHFn5ZaLOE8ACG3tOGqGq+3P7RLKau2VW2dYqISKWnoCdSGoKqQ//vIPFS+OZemP5moc1iQ/wZek8bEiMCuXPQXH5ctr2MCxURkcpMQU+ktPgGQ9+vIPVaGPd3+PHvhd4yLTLIl8F3t6ZBjWrc9/l8Ri7cUva1iohIpaSgJ1KavHyh90dwye9hxpsw8n44fe59b0MDfPj8961oUSuMh4csZEqmbpcmIiK/nYKeSGlzueGqVyHjKVj4OQzpCyfPXTA5yNeLT37XkvZ1q/PR0pN8PH2DB4oVEZHKREFPpCwYAxlPwtWvOTNxP7sOju07p5m/j5sBt7egRbSbf4xazjsT13qgWBERqSwU9ETK0iV3wg2DYOt8+PgqOLj1nCa+Xm7+0NSXa5vV4JUfVvHKDyuxhYztExER+TUKeiJlreG10HcY7N/s3EVj95pzmrhdhn/f2Iw+LRN4Z+I6/jFqucKeiIhcMAU9EU9I6uAsv3LqqHN/3C3zzmnidhmev64xd7SrzaAZG3lq+BJOZyvsiYjI+VPQE/GUGs3gzh/BJxAGXQPrJpzTxBjD37s34I+d6jB4zmYeGbKQU6ezy75WERGpkBT0RDwpIhnuHAfhteHzG2HJsHOaGGN4rEt9/twthW8XbeUPn8/nRNZpDxQrIiIVjYKeiKcFx0D/7yGhJXz9e5j1QaHN7stI5h89GjJu+Q5+/8lcjp1U2BMRkeIp6ImUB/6hcOvXkHI1jHmCxA2fF3oXjX5tE3m5dxOmr91Nv49mc+i4FlYWEZGiKeiJlBfe/nDDJ5B2G4m/DIXvHobsc3vtbkxP4I2b05i/aR+3DVTYExGRoinoiZQnbi/o8Ra/1OwN8wbBV/3g1PFzml3TtAb/7ducpVsOcMegORw9ee5t1URERBT0RMobY9iQdBt0exFWjILPe8PxA+c069IwhjduTmPeL/v4/SdzOX5KY/ZERCQ/BT2R8qr1fXD9h7BpJgy6Gg7vPKfJ1U1i+feNTZm5fg/3fDZPs3FFRCQfBT2R8qzJDdBnCOxZBwO7wN4N5zS5Li2eF65rzOTVu7j/8wVaZ09ERHIp6ImUd3Uvh36jnMu3A7vAtsXnNLm5ZU3+2bMhP63YwcODF5KlsCciIijoiVQM8elwx1hw+ziXcTdMPafJ7W0S+dvVDfh+yTaeGLZYt0sTEREFPZEKo3p9uPMHCI6F//VyJmoU8Pv2STzRtT4jFmzhryOWkK2wJyJSpSnoiVQkIfFOz15sExh6u3MXjQILK9/fsQ4P5twb95lRy7CFLLwsIiJVQ5UKesaYJGPMQGPMuTcUFakoAsLh9pFQtwuMeQIG3wKHd+Vr8sgV9bjnsiQ+nfkLz32/QmFPRKSKKvWgZ4xxG2MWGGO++w3H+MgYs9MYs7SQ17oZY1YZY9YaY54s7jjW2vXW2jsvtg6RcsMnEG7+Erq+AGvHw7ttYPUPuS8bY3jyyhT6t03kw2kb+PePqz1YrIiIeEpZ9Og9BKwo7AVjTJQxJrjAtjqFNB0EdCtkfzfwDnAlkAr0McakGmMaG2O+K/CI+q0fRKRccbmgzR/g7kkQFA1f3AijHoaTRwAn7P1f91T6tEzg7YlreWv8Go+WKyIiZa9Ug54xJh64GviwiCYdgJHGGL+c9ncBbxZsZK2dAuwtZP+WwNqcnrqTwGCgp7V2ibW2e4HHuavNilQG0alw1wRo+6Bz27T32kPmPABcLsNz1zbm+rQ4/j1uNe9PXufZWkVEpEyVdo/e68CfgEIX9bLWfgWMBQYbY/oCdwA3XsDx44DNeZ5n5mwrlDEmwhjzHpBmjHmqiDbXGGM+OHDg3FtOiZRbXr7Q5Vno/x2cPgkDr4BJL8HpLFwuw8u9m9C9SSwvjFnJoOnnLrosIiKVU6kFPWNMd2CntXZece2stS8Dx4F3gR7W2sMX8jaFHbKY99pjrb3XWptsrX2hiDajrLV3h4SEXEAZIuVE4qVw7zRo1AsmPQ8fd4M96/Byu/jPTc3okhrNM6OW88WsTZ6uVEREykBp9ui1A3oYYzbiXFLtZIz5X8FGxpj2QCNgBPD0Bb5HJpCQ53k8sPWiqhWpLPxDodcA6P0R7F7tXMqd9wneLsNbt6TRsX51/vrNEobNy/R0pSIiUspKLehZa5+y1sZbaxOBm4EJ1tpb87YxxqQBA4CewO+AcGPMvy7gbeYAdY0xtY0xPjnv822JfACRiq5RL7hvJsS3gFEPwuBb8D2+l3dvbUG75Ej+NGwRoxbp9yIRkcrM0+voBQA3WGvXWWuzgX7ALwUbGWO+BGYC9Y0xmcaYOwGstVnAA8APODN7h1prl5VZ9SLlXUgc3DYSuj6fuwyL34af+OD2FqQnhvPwkIWMXbrd01WKiEgpKZOgZ62dZK3tXsj26dbaJXmen7LWDiikXR9rbay11junl3BgntdGW2vr5Yy7e670PoVIBeVyQZv78y3DEvDjE3x0SypN4kP445fzmbhSk9JFRCojT/foiUhZyV2G5Y8w92OCPu7IZ928qB8TzD3/m8e0Nbs9XaGIiJQwBT2RqsTLF7r8C/qNgqwTBH12JV+lTKNOhB+//3QOs9bv8XSFIiJSghT0RKqi2u3hvunQqBf+019kZMCzpFfbzx2D5jDvl32erk5ERErIRQU9Y4wW4RKp6M4sw9JrIN771vLpyUfp7zeF/h/NYnHmfk9XJyIiJeBie/QKW6hYRCqixr3hvhm44tN54uQ7vOP+Nw9+OI7lWw96ujIREfmNLjboFXn3CRGpgELic5dhac9CvuZx3v/wv6zZccjTlYmIyG/gVdQLxphHi3oJCCqdckTEY3KWYTFJGQQNvZM39rzAsPfn4X3Xf0mMre7p6kRE5CIU16MXXMQjCHij9EsTEY+IbojvfZPZ1/Qers8eh/ngMnasmOHpqkRE5CIU2aNnrf1HWRYiIuWIly9h173MxoRO+H73ByKGXM3BNo9T7fI/g7vIHxsiIlLOFNmjZ4wZmuf7lwq89mNpFiUi5UNiejd23zqRH2xbqs18mVMfdoW96z1dloiInKfiLt3WzfP9FQVe04AdkSqicZ1axNzxGY9nP8iJbSuw77aDeZ+A1ZwsEZHyrrigV9xPcf2EF6lCWtQKp3f/h+l++iUW2Tow6kEY3BeO6LZpIiLlWXFBL8AYk2aMaQH453zf/MzzMqpPRMqJ1kkRPHt7N246/iQfBvweu3Yc/LcNrNZIDhGR8qq4UdXbgdcK+f7McxGpYtrXrc5/+6Zz7/8My6LTeNX9Du4vboD0O5176PoEeLpEERHJo7hZtxllWIeIVBCdG0TzVp807v9iATtrvsigVj/gPesd2DAZrnsf4tM9XaKIiOQobtbtrcaY2wrZfpcx5pbSLUtEyrNujWJ57camzPjlML/b0pOTfb+BU8fgw84woBPMHgBH9ni6TBGRKq+4MXqPAd8Usn1IzmsiUoX1bBbHy72aMG3tbu6ZFsjJu6fDFc9C1gkY/Tj8ux582QeWj3S2iYhImStujJ7bWnvOjS6ttQeNMd6lWJOIVBA3pCdw8nQ2fx2xlD+OcPH2LQ/g3e5B2L4UFg+GxV/BqtHgFwINr4emN0NCKzDG06WLiFQJxfXoeRtjAgtuNMYEAz6lV5KIVCR9W9Xi/7qn8sOyHTw8ZCEnsk5DTCNncsajy+HW4VC3KyweAh91hTfTYOILWnhZRKqG06c8uu5ocT16A4Fhxpj7rLUbAYwxicA7Oa+JiABwx6W1ycrO5vnRK9my7xjv3dqCmBA/cLmhTmfnceIQrPgOFn0Jk1+CyS86vXtNboKG10FAuKc/hojIhbMWDu+Afb/A/l9yvm50vu77BQ5mwmOrIcgz95oobtbtq8aYw8BkY0wQziLJR4AXrbXvllWBIlIx3H1ZMglhATz21SKueXsa7/ZtTnpinvDmGwzN+jiPA1tgyVBYNAS+fxTGPgn1ukKTm6FuF/DSRQMRKUeOH8gJbhvzhLkzXzdB1rH87YOiIbQW1GwFYTeBKe4Caukq9u7k1tr3gPdygp4pbMyeiMgZVzaOJTkqiLs/nUufAT/z9DUN6duqJqbgmLyQOLj0EWj3MGxfDIsGw5KvYMUo8A+HRtc7oS8+XeP5RKT0nToOBzYX6I3beDbMHd+fv71vCITVhMi6UPcKCEt0gl1YLQitCd7l574SxQa9M6y1h0u7EBGpHOpFBzPy/kt5aMgC/vbNUpZuOcA/ejbE18t9bmNjILap87jiWVg3wZnEseB/MOdDCE+Gpjfjd6xW2X8QEak8sk/DoW1OeMvXG5cT6A5ty9/e7eMEtrBEiEvPCXA5QS4sEfzDyv4zXKTzCnoiIhciJMCbgf0u4bVxq3hn4jpW7TjEe7e2ILqaX9E7ub2gXhfncfwALP/WmcAx8TlaA2wb5MzaTe0J/qFl80FEpGLIzoYju+BAphPecgJck/ULYfFB2L8Zsk/l2cFAtTgnuCV1dMJb3jAXFAMuz11uLUkKeiJSKtwuwxNdU2hUI4THvlpE97em8d6tzWlR6zwmXfiFQPPbnMf+Tawf+RJJh2bDqAdh9BNQ/0po2seZ5OHWak8ild6pY87Y3gObnTB3ztdMOH0y/z4BEXi5w6FmU2jQ42xvXGgtCEmoMmOBzyvoGWPaAol521trPy2lmkSkErmycSxJ1YO4+7O53PzBzzzToyF9W13ApdjQmmyqdQNJHd6GrfOdCRxLh8HybyAgEhr3dmbu1kjTeD6RishaOLrHmdRwJrQdyIQDeZ4f2VVgJwPBsRAS7/zbb3CNE95C4p2vYbXAN5j5kyaRkZHhiU9Vbvxq0DPGfAYkAwuB0zmbLaCgJyLnpX5MMN/efykPDl7AX0c44/ae6VHEuL2iGANxLZxH1+dg7U/OJI65H8Os9yCynnNpt/GNEJpQeh9GRC5M1gk4uOVsaNu/+dweuazj+ffxDjgb3GKaOP+mc4NcvHPZVb355+V8evTSgVRrPbjan4hUeCEB3nzU/xL+/eMq/jtpHSu3n8e4vaK4vZ3Lt/WvhGP7nd69RYNh/D9h/LOQeKkT+hr0AL9qJf1RROQMa+HYvvyXUA9szglzOc8P78DpH8ojKNoJbNGNoF43J8SF5umR8w9TD30JOZ+gtxSIAbb9WkMRkeK4XYY/dUuhUVwIj1/ouL2i+IdCi/7OY99GWDzUWZR55P3w3SNQPQWiG0JUKkSnQlRDCI7RfyIiRbEWThyEw7vgyE4nqOV+v9O5jHp459nnBXvjvPzO9rzVvRxCap59fubh5euZz1YFnU/QiwSWG2NmA7l3JrfW9ii1qkSkUruqcSzJecbt/aNHI25pVfO3HzgsETr8CS57AjLnwopvYcdSWD/JCX9n+Ic5wS83/KVCVANnEohIZWStM5s9b0ArGNjybjt94txjGBcERDi9cYHVISLZ+VqtRv7xcYGR+kWqHDmfoPdMaRchIlVP3nF7fxmxhCVb9l/4uL2iGAMJlziPM47uhZ3LYcdy2LkMdq5wLveezLMOfEjC2dB3phcwsl6VmZ0nFcyZy6Z5Altc5gz4aXJOeNuV/2vBWanghLfA6hAY5dyiK7Ke8zUwygl0ud9HOSHPVQL/PqVM/WrQs9ZOLotCRKTqKThub9X2Q7x7seP2fk1AuDN2L/HSs9usdcYT7VjuhMAzQXDdhLNrbrm8IKJuTvjLufQbnepcjqok62xJGcjOdi5xnjoGp47m+Xo0z/MCr508Wnj7I7vP9r7lWxsO6gKsczvh7UxIq56S8zzaCWyB1XO+Rjn/LhTeKrXzmXXbGngLaAD4AG7giLVWI5xF5DcrlXF758sYZ/X70JpQv9vZ7adPwZ61sGPZ2fC3ZR4sG362jU9Qzvi/POEvKtW5bCUVW9aJs71kR/fCqSP5A9fJowXCV2HhrZDgdsEM+AQ6t9Py9ndmonr7O3/HolKdsHYmsOWEuumL1tDu8mv0S4jkOp9Lt28DNwNf4czAvZ2cXxpEREpKqY3buxhub6cHL6pB/u0nDsHOlc6l3zO9gCu/h/l5VpsKjDob/s70AlZvAD4BZfsZ5Kx8kwtyLmMe2VXg+Zlest1w4sCvH9O4Cw9h3gHOXRXybcv53icgf7vc1wILb+/le8Fj3U757FTIk3zO9163a40xbmvtaeBjY8yMUq5LRKqgM+P2/pg7bu8Az/RI9XRZZ/kGnzv2z1onIJwZ93dmDODcjyDrWE4jA+G1nV4Y/zDnP3C3rxMovXyd+2qe2eblk/+r2+ecbf5HM537dJ7Z98z+Lq+qMwj+dBYc3UPg4Y3Opfa8lzPPPM4EtyO7Cp9cAOAffvZSZmwTJ6jnXvas7izK7VMwiAVo3KZUGOcT9I4aY3yAhcaYl3GWWQks3bJEpKoKCfDm43zj9g5ye1K2p8sqmjEQHO08kjud3Z592lnuJd8EkJVOr+DpE5B10vla2AD5X9EKYHahxRQRGH2KCZM+Tu+Uy8sZq+XyyvO40Od5txV1zF85zulTBYJaYcEt55IqlksA5uY5BS7vnMuZkU5oi0o9G9oCc7afGacWEKFFd6XSO5+gdxvgAh4AHgESgF6lWZSIVG1nxu01rOGM23tmRzYJDfbRolaYp0s7fy63s/xERLJze6aiWOuEvawT+b+es+1sOFy+ZCGp9ZJzXjuZPzgWPE5hxzt+8Ow+2acgO8sJpoV+zTpnwH+Z8612NrhFJEOtNjnBrTrLNu6kYauOuc/xC6k6vZoi5+F8Zt3+YozxB2Kttf8og5pERAC4ukksyVGB3Pb+NG7+YCb/7NmIPi09NG6vtJicXrgLWEB2584QUtMySq2kQmVn5wl+WYWEwUKe29O/3qbgc5fX2dB25uFd9CzsXUcnQa22ZXceRCqY85l1ew3wKs6M29rGmGbAPyvigsnGmCTgr0CItba3p+sRkV+XElONp9v4MzQzkKeG54zbu6YhPl4acF6mXC5w+eD8VyAiFcX5/KR8BmgJ7Aew1i4EEn9tJ2OMnzFmtjFmkTFmmTHmonsDjTEfGWN2GmOWFvJaN2PMKmPMWmPMk8Udx1q73lp758XWISKeEeRj+Lj/JdyXkcwXszbRZ8DP7Dx4/Nd3FBGp4s4n6GVZa89jrvk5TgCdrLVNgWZAt5w1+XIZY6KMMcEFttUp5FiDgG4FNxpj3MA7wJVAKtDHGJNqjGlsjPmuwCPqIj6DiJQTbpfhz91SeOeW5izfepDub01j3i/7PF2WiEi5dj5Bb6kx5hbAbYypa4x5C/jV5VWs43DOU++chy3QrAMw0hjjB2CMuQt4s5BjTQH2FvI2LYG1OT11J4HBQE9r7RJrbfcCj53n8VkxxlxjjPngwIGLybYiUtqubhLLiPvb4uft5uYPZvLl7E2eLklEpNw6n6D3R6AhTg/dl8BB4OHzObgxxm2MWQjsBMZZa2flfd1a+xUwFhhsjOkL3AHceL7FA3HA5jzPM3O2FVVPhDHmPSDNGPNUYW2staOstXeHhOjm5iLlVUpMNb59oB2tkyJ4avgS/jJiCSezyvESLCIiHnI+s26P4kxg+OuFHjxngeVmxphQYIQxppG1dmmBNi8bYwYD7wLJeXoBz0dhc+gL9hrmfa89wL0XcHwRKadCA3wY9LuWvPLDKt6bnHOf3L7NiSqN++SKiFRQRQY9Y8y3xe14IbNurbX7jTGTcMbZ5Qt6xpj2QCNgBPA0znp95ysTZ12/M+KBrRewv4hUYG6X4ckrU2gUV40nvlrMNW9P491bW9C8ZgVab09EpBQVd+m2DU5wmoqzvMq/CzyKZYypntOTR846fJcDKwu0SQMGAD2B3wHhxph/XUD9c4C6xpjaOXfvuBkoNqCKSOXTvUkNhv+hLb5ebm5+/2cGa9yeiAhQfNCLAf6C09v2BnAFsNtaO9laO/k8jh0LTDTGLMYJZOOstd8VaBMA3GCtXWetzQb6Ab8UPJAx5ktgJlDfGJNpjLkTwFqbhdMD+AOwAhhqrV12HrWJSCXTINYZt9cqKZwnhy/hrxq3JyJS9KXbnPF1Y4GxxhhfoA8wyRjzT2vtW792YGvtYiDtV9pML/D8FE4PX8F2fYo5xmhg9K/VIyKVX8Fxeyu3H+LVG5pSO1K35xaRqqnYWbfGGF9jzPXA/4D7cZY+GV4WhYmIXIwz4/beviWNVdsP0eU/k3l+9AoOHvfw/VpFRDyguMkYn+Bcth0D/KPgbFkRkfKse5MatKwdzitjVzFg6nqGz8/kia716d0iAbdLN70XkaqhuB6924B6wEPADGPMwZzHIWPMwbIpT0Tk4kUF+/HKDU0ZeX87akUE8uevl9Dj7WnM3lDY+usiIpVPkUHPWuuy1gbnPKrleQRba6uVZZEiIr9Fk/hQht3bhjf7pLH3yElufH8mD3wxny37j3m6NBGRUnU+d8YQEanwjDH0aFqDCY9l8FDnuoxbvoNOr07itXGrOXoyy9PliYiUCgU9EalS/H3cPHJFPSY8nkGXhjG8OX4Nnf89mZELt2BtkTfWERGpkBT0RKRKigv1560+aQy9pw0RQT48NHghvd+byeLM/Z4uTUSkxCjoiUiV1rJ2OCPvv5SXezXhlz1H6PH2dB7/ahE7Dx73dGkiIr+Zgp6IVHlul+HGSxKY+HgG91yWxMiFW+j46iTenbSOE1mnPV2eiMhFU9ATEckR7OfNU1c14MdHOtAmOZKXxq6ky3+m8MOy7Rq/JyIVkoKeiEgBtSMD+bBfOp/d2RIft4t7PpvHrQNnsWr7IU+XJiJyQRT0RESK0L5udcY81J5/9GjI0i0HufKNKfz9m6XsO3LS06WJiJwXBT0RkWJ4uV30a5vIpMczuK11Lb6YvYmMVyfx8fQNnDqd7enyRESKpaAnInIewgJ9+EfPRox+sD2N4qrxj1HLufKNqUxZvcvTpYmIFElBT0TkAtSPCeZ/d7big9tacOp0Nrd/NJvffzKHDbuPeLo0EZFzKOiJiFwgYwxdGsbw4yOX8eSVKcxct4cu/5nM86NXcPD4KU+XJyKSS0FPROQi+Xq5ubdDMhOfyOC6tDgGTF1Pp1cnMXj2Jk5nazkWEfE8BT0Rkd8oKtiPl3s3ZeT97agVEciTw5fQ4+1pzN6w19OliUgVp6AnIlJCmsSHMuzeNrzZJ429R05y4/szeeCL+WzZf8zTpYlIFaWgJyJSgowx9GhagwmPZfBQ57r8tGIHnV6dxGvjVnP0ZJanyxORKkZBT0SkFPj7uHnkinqMfyyDLg1jeHP8Gjq9Opn//fwLx07q/rkiUjYU9ERESlFcqD9v9Unjq3vbEBPix9++WUrbF8fz2rjV7D58wtPliUglp6AnIlIGLkkMZ8Qf2jLk7ta0qBXOWxPW0PbFCTw1fDFrdx72dHkiUkl5eboAEZGqwhhDq6QIWiVFsG7XYQZO28DX8zL5cvZmOqVEcVf7JFonhWOM8XSpIlJJqEdPRMQDkqsH8fx1jZnxZCceubweizbvp8+An7nm7WmMXLhF99EVkRKhoCci4kERQb48dHldpj/ZiReub8zRk6d5aPBCOrw8kQFT1nNId9oQkd9AQU9EpBzw83bTp2VNfnqkAwP7pVMzIoDnRq+g7QsTeO775WzVWnwichE0Rk9EpBxxuQydG0TTuUE0SzIPMGDqej6avpGPpm+ke5NY7mqf5OkSRaQCUdATESmnGseH8GafNP58ZQofT9vA4DmbGblwKynhLrJjdpBRLwqXSxM3RKRounQrIlLOxYX687fuqcx4qhN/uSqFnUctdwyayxX/mczg2Zs4fkoLMItI4RT0REQqiGp+3tx9WTIvX+bPGzc3w8/bzZPDl3DpSxN446c17D1y0tMlikg5o0u3IiIVjJfL0LNZHD2a1mDm+j0MmLKe//y0mncnr6VX83juvLQ2SdWDPF2miJQDCnoiIhWUMYa2yZG0TY5kzY5DfDh1A1/NzeSL2Zu4vEE0d7VP4pLEMC3ALFKFKeiJiFQCdaODeal3Ex7rWo/PZv7CZz//wrjlO2iaEMpd7WvTrWEMXm6N1hGpavSvXkSkEokK9uOxLvWZ+WRnnr22EQeOnuSBLxaQ8eokPpq2gcMnsjxdooiUIQU9EZFKyN/HzW2tazH+sQzev60FsSF+/PO75bR5YTwvjlnJ9gPHPV2iiJQBXboVEanE3C5D14YxdG0Yw4JN+/hw6gY+mLKOD6eup0fTGvRvl0jjuBCN4xOppBT0RESqiLSaYbzTN4zNe48ycNoGhs7dzPAFW6gbFUSvFvFclxZHdDU/T5cpIiVIl25FRKqYhPAAnunRkJlPdea56xoR7OfFi2NW0uaF8dz+0WxGLtzCsZNahFmkMlCPnohIFRXi703fVrXo26oW63cdZsSCLQyfv4WHBi8k2NeLqxrH0qtFvJZoEanAFPRERISk6kE81qU+j1xej5837OHreVsYtXgrQ+ZupmZ4ANc3j6NX83gSwgM8XaqIXAAFPRERyeVynV2E+Z89GzJ26Xa+np/JG+PX8PpPa2hZO5zezeO5snEMwX7eni5XRH6Fgp6IiBQq0NeLXi3i6dUini37jzFifiZfz9/Cn75ezP99u5SuDWPo1TyednUicbt0aVekPFLQExGRXxUX6s8Dnepyf8c6LNi8n6/nZTJq0VZGLtxKTDU/rk2Lo3eLOOpEBXu6VBHJQ0FPRETOmzGG5jXDaF4zjL93T2X8ip18PT+TAVPX897kdTSND+H65vH0aFqDsEAfT5crUuUp6ImIyEXx83ZzdZNYrm4Sy85Dx/l24VaGzcvk6W+X8a/vl9MpJYpezePpmBKFt+6zK+IRCnoiIvKbRQX78fv2Sfy+fRLLtx7k6/mZjFy4hR+W7SA80IceTWvQu0U8DWtU01ItImVIQU9EREpUao1qpNZI5ckrU5iyehdfz8/ki1mbGDRjI/Wjg7m+eRzXpcURpbtwiJQ6BT0RESkV3m4XnRtE07lBNPuPnuS7xdv4en4mL4xZyUtjV9K+bnV6tYinS2o0ft5uT5crUikp6ImISKkLDfDh1ta1uLV1LdbtOszw+ZmMmL+FB79cQLCfF92bxNKreTwtaukuHCIlSUFPRETKVHL1IJ7omsJjV9Rn5vo9fD0vk28WbOXL2c5dOK5sHMNVjWJpEh+i0CfyGynoiYiIR7hchnZ1ImlXJ5J/XpvFmCXbGLV4GwOnbuD9yeuJC/XnykYxXNUklmbxobi0KLPIBVPQExERjwvy9eKG9ARuSE9g/9GTjFu+gzFLt/PJzI18OG0DsSF+dGsUw1WNY2lRM0yhT+Q8KeiJiEi5Ehrgkxv6Dhw7xYSVOxi9ZDufz9rEx9M3EhXsS7dGMVzZKJZsaz1drki5pqAnIiLlVoi/N9elxXNdWjyHT2QxYeVOxizZxtC5m/l05i9U84Hu+5dwVaNYWieF46WFmUXyUdATEZEKIcjXix5Na9CjaQ2OnMhi0qpdfDJhESPmb+GLWZsIC/CmS2oMVzaOoV2dSN2NQwQFPRERqYACfb24ukksgXtX0apteyav3sWYpdv4fsk2hszdTIi/N1ekRnNVTujz9dI6fVI1VamgZ4xJAv4KhFhre3u6HhER+e38fdx0axRDt0YxHD91mmlrdjN66TZ+WLadYfMyCfb14vLUaK5sFMNl9aprcWapUkot6BljEoBPgRggG/jAWvvGRR7rI6A7sNNa26jAa92ANwA38KG19sWijmOtXQ/caYwZdjF1iIhI+ebn7eby1GguT43mZFY209ftZsySbfy4fAcjFmwh0MdNpwbRXNUohoz6Ufj7KPRJ5VaaPXpZwGPW2vnGmGBgnjFmnLV2+ZkGxpgo4Ji19lCebXWstWsLHGsQ8DZOcCRPWzfwDnAFkAnMMcZ8ixP6XihwjDustTtL5qOJiEh55+PlomP9KDrWj+K509n8vH4Po5ds54dl2xm1aCv+3m46plTnykaxdEqJItC3Sl3kkiqi1P5WW2u3Adtyvj9kjFkBxAHL8zTrANxnjLnKWnvcGHMXcB1wVYFjTTHGJBbyNi2BtTk9dRhjBgM9rbUv4PQAioiI4O120b5uddrXrc6zPRsye8NeRi/dxtilztItvl4uMupX56rGTugL9vP2dMkiJaJMfn3JCWlpwKy82621XxljagODjTFfAXfg9M6drzhgc57nmUCrYuqIAJ4D0owxT+UEwoJtrgGuqVOnzgWUISIiFYWX20XbOpG0rRPJP3o0Yu7GvYxZup0xS7fxw7Id+LhdXFYvkisbxXJ5ajQh/gp9UnGVetAzxgQBXwMPW2sPFnzdWvtyTk/cu0CytfbwhRy+kG1Frp5prd0D3FvcAa21o4BR6enpd11AHSIiUgG5XYZWSRG0Sorg/7qnsmDzPkYv2c6YJdv4acVOvFyGlrXD6ZQSRaeUKJKqB3m6ZJELUqpBzxjjjRPyPrfWDi+iTXugETACeBp44ALeIhNIyPM8Hth6cdWKiEhV5nIZWtQKp0WtcP52dQMWZR5g7NLtTFi5g399v4J/fb+C2pGBdEqJonNKFOmJ4fh4aa0+Kd9Kc9atAQYCK6y1rxXRJg0YAFwNbAD+Z4z5l7X2b+f5NnOAujmXf7cANwO3/ObiRUSkSjPG0CwhlGYJoTx5ZQqb9x5l4qqdjF+xk89+/oWB0zYQ5OvFZfUi6ZQSTUb96kQG+Xq6bJFzlGaPXjvgNmCJMWZhzra/WGtH52kTANxgrV0HYIzpB/QveCBjzJdABhBpjMkEnrbWDrTWZhljHgB+wJlp+5G1dlkpfR4REamiEsIDuL1NIre3SeToySymr93DhJU7GL9iJ6OXbMcYaJYQSqf6UXRqEEVqbDWc/g4RzyrNWbfTKHwMXd420ws8P4XTw1ewXZ9ijjEaGF3U6yIiIiUpwMeLK1KjuSI1Gmsty7YeZMLKnYxfuZN/j1vNv8etJqaaH50aRNGpfhTt6kRqvT7xGC0aJCIicpGMMTSKC6FRXAgPdq7LzkPHmbRqFxNX7mTkAucevL5eLtomR9ApJYqOKVHEhwV4umypQhT0RERESkhUsB83pidwY3oCJ7JOM2fDPsav3MGElTuZOHIZjFxGSkxw7izetJphuF26xCulR0FPRESkFPh6ubm0biSX1o3k/7qnsn73ESas2Mn4lTv4YMp6/jtpHWEB3mTUd0LfZfWqa80+KXEKeiIiIqXMGENy9SCSqwdx12VJHDh2iqlrdjFhxU4mrtrJiAVbcLsM6bXC6Nwgik4p0SRXD9SEDvnNFPRERETKWIi/N92b1KB7kxqczrYs3Lw/dxbv86NX8vzoldQMD3DW7GsQRcva4fh6aUKHXDgFPREREQ9yuwwtaoXRolYYT3RNYcv+Y0xcuZMJK3fy5exNDJqxkUAfN+3rVqdTShQd6lcnupqfp8uWCkJBT0REpByJC/Xn1ta1uLV1LY6dPM3M9bsZv8IJfmOXbQegfnQw7etG0r5edVomhmv5FimSgp6IiEg55e/jplNKNJ1SnDX7Vm4/xNQ1u5iyejef/vwLH07bgI+Xi5aJ4bSvG4n/wdNYazW2T3Ip6ImIiFQAxhgaxFajQWw17r4smWMnTzN7416mrt7F1DW7eWHMSgDeXDze6e3LmfEbFazLvFWZgp6IiEgF5O/jpkO96nSoVx2AHQePM2DUVHa5I5iyehcjFmwBoEFsNS6rG0n7utVJTwzDz1uXeasSBT0REZFKILqaH5fGeZORkUZ2tmX5toNMXbObqWt28fH0jbw/ZT2+Xi5aJUXkBr960UG6zFvJKeiJiIhUMi7X2Vuz3ZeRzNGTWczasJcpOZd5//X9CmAFUcG+tK9bncvqRdKuTiSRQb6eLl1KmIKeiIhIJRfg40XH+lF0rB8FwNb9x5i2ZjdT1uxi/ModfD0/E4CGNarlBr8WtcK0dl8loKAnIiJSxdQI9efGSxK48ZIETmdblm09wNQ1u5myehcfTl3Pe5PX4e/tpnVSeG7wS66uy7wVkYKeiIhIFeZ2GZrEh9IkPpT7O9bh8IksZq3fk3uZd+Kq5QDEhvjlzOatTrs6kYQH+ni4cjkfCnoiIiKSK8jXi84NouncIBqAzXuPMm2tM6lj7NLtDJ2biTHQOC6E9nWdsX3Na2o2b3mloCciIiJFSggPoE/LmvRpWZPT2ZbFmftzZ/O+N3k970xch4+Xi+Y1Q2mbHEmb5Aiaxofi4+XydOmCgp6IiIicJ7fLkFYzjLSaYTzYuS6Hjp9i9oa9zFy3h5nr9/Cfn1bz2jjw93aTnhhGm+QI2iRF0DguBC+3gp8nKOiJiIjIRQn28853mXf/0ZP8vH4vP6/fw8x1e3h57CrAuRx8SWJYbo9fg9hquF2a2FEWFPRERESkRIQG+NCtUQzdGsUAsPvwidzQN3P9HiauWgFANT8vWiVF0DY5gjbJEdSLCsal4FcqFPRERESkVEQG+dK9SQ26N6kBOLdpm7nubPAbt3wHAOGBPrROCqdNUgRtkiNJrh6opVxKiIKeiIiIlInoan5cmxbHtWlxAGzZf4yZ6/YwY91ufl63h9FLtgNQPdg3J/Q5vX41wwMU/C6Sgp6IiIh4RFyoP71bxNO7RTzWWjbtPZrb2zdj3R6+XbQVgBohfrTOmdjRJjmC+LAAD1decSjoiYiIiMcZY6gVEUitiEBublkTay3rdh1h5vo9/LxuD5NW7WL4/C0A1AwPyA19bZIjiK7m5+Hqyy8FPRERESl3jDHUiQqiTlQQt7WuRXa2ZfXOQ7lj/MYs3caQuZsBSKoemBv8WidFEBnk6+Hqyw8FPRERESn3XC5DSkw1UmKq8bt2tTmdbVmx7WDupd6RC7fy+axNACRXD6RVUgTBx7JIOXCcmJCq2+OnoCciIiIVjttlaBQXQqO4EO66LIms09ks2XKAWRv2Mmv9HkYt3MqhE1m8v3g8NcMDaFU7nJa1w2mdFEF8mH+VmdyhoCciIiIVnpfblXvXjns7JHM62/LZqAmcjkhi1vo9jFuxg6/mZQIQG+KXE/wiaJUUTlJk5V3ORUFPREREKh23y5AY4ibj0trceWltsrMta3YeZvaGPfy8YS/T1+3hm4XOrN7IIN/cHr9WSeGVagFnBT0RERGp9FwuQ/2YYOrHBHNbm0SstWzYfYTZG/bmXu79fsk2AEIDvLkkMZxWtcNpVTuCBrHBFfZevQp6IiIiUuUYY0iqHkRS9SBublkTgM17j+YEvz3M3rA3984dQb5etKgVRqskJ/w1jgvFx6tiBD8FPREREREgITyAhPAAerWIB2D7gePM3uj09s3esJeXx64CwM/bRfOaYbSqHUHL2uGk1QzFz9vtydKLpKAnIiIiUoiYED96NK1Bj6bOvXr3HD7BnI1nLvXu5fXxq7EWfNwumiaE5Aa/FrXCCPQtHxGrfFQhIiIiUs5FBPnSrVEs3RrFAnDg2CnmbtzL7A17+XnDXt6dvI63J67NXfqldc4Ej7bJkfj7eKbHT0FPRERE5CKE+HvTuUE0nRtEA3DkRBbzftmXO87v4+kbeX/Keqb+qSMJ4Z65P6+CnoiIiEgJCPT14rJ61bmsXnUAjp86zZItBzwW8gAqxpQRERERkQrGz9vNJYnhHq1BQU9ERESkklLQExEREamkFPREREREKikFPREREZFKSkFPREREpJJS0BMRERGppBT0RERERCopBT0RERGRSkpBT0RERKSSUtATERERqaSMtdbTNZRLxphdwC+l/DaRwO5Sfo+KROfjLJ2L/HQ+8tP5OEvnIj+dj/yq0vmoZa2tXnCjgp4HGWPmWmvTPV1HeaHzcZbORX46H/npfJylc5Gfzkd+Oh+6dCsiIiJSaSnoiYiIiFRSCnqe9YGnCyhndD7O0rnIT+cjP52Ps3Qu8tP5yK/Knw+N0RMRERGppNSjJyIiIlJJKeh5iDGmmzFmlTFmrTHmSU/X4ynGmARjzERjzApjzDJjzEOerqk8MMa4jTELjDHfeboWTzPGhBpjhhljVub8PWnj6Zo8xRjzSM6/k6XGmC+NMX6erqksGWM+MsbsNMYszbMt3BgzzhizJudrmCdrLEtFnI9Xcv6tLDbGjDDGhHqwxDJT2LnI89rjxhhrjIn0RG2epqDnAcYYN/AOcCWQCvQxxqR6tiqPyQIes9Y2AFoD91fhc5HXQ8AKTxdRTrwBjLXWpgBNqaLnxRgTBzwIpFtrGwFu4GbPVlXmBgHdCmx7Ehhvra0LjM95XlUM4tzzMQ5oZK1tAqwGnirrojxkEOeeC4wxCcAVwKayLqi8UNDzjJbAWmvtemvtSWAw0NPDNXmEtXabtXZ+zveHcP4Tj/NsVZ5ljIkHrgY+9HQtnmaMqQZcBgwEsNaetNbu92hRnuUF+BtjvIAAYKuH6ylT1topwN4Cm3sCn+R8/wlwbVnW5EmFnQ9r7Y/W2qycpz8D8WVemAcU8XcD4D/An4AqOyFBQc8z4oDNeZ5nUsXDDYAxJhFIA2Z5uBRPex3nB1O2h+soD5KAXcDHOZeyPzTGBHq6KE+w1m4BXsXpmdgGHLDW/ujZqsqFaGvtNnB+cQSiPFxPeXIHMMbTRXiKMaYHsMVau8jTtXiSgp5nmEK2VdnfNgCMMUHA18DD1tqDnq7HU4wx3YGd1tp5nq6lnPACmgPvWmvTgCNUrUtzuXLGnvUEagM1gEBjzK2erUrKK2PMX3GGxnzu6Vo8wRgTAPwV+D9P1+JpCnqekQkk5HkeTxW7BJOXMcYbJ+R9bq0d7ul6PKwd0MMYsxHnkn4nY8z/PFuSR2UCmdbaM728w3CCX1V0ObDBWrvLWnsKGA609XBN5cEOY0wsQM7XnR6ux+OMMf2A7kBfW3XXUEvG+aVoUc7P03hgvjEmxqNVeYCCnmfMAeoaY2obY3xwBlR/6+GaPMIYY3DGX62w1r7m6Xo8zVr7lLU23lqbiPP3YoK1tsr22lhrtwObjTH1czZ1BpZ7sCRP2gS0NsYE5Py76UwVnZhSwLdAv5zv+wEjPViLxxljugF/BnpYa496uh5PsdYusdZGWWsTc36eZgLNc36mVCkKeh6QM1D2AeAHnB/UQ621yzxblce0A27D6blamPO4ytNFSbnyR+BzY8xioBnwvGfL8YycXs1hwHxgCc7P7yq16r8x5ktgJlDfGJNpjLkTeBG4whizBmd25YuerLEsFXE+3gaCgXE5P0/f82iRZaSIcyHozhgiIiIilZZ69EREREQqKQU9ERERkUpKQU9ERESkklLQExEREamkFPREREREKikFPRGRi2CMOZ1nSaCFxpgSu2OHMSbRGLO0pI4nIlWXl6cLEBGpoI5Za5t5uggRkeKoR09EpAQZYzYaY14yxszOedTJ2V7LGDPeGLM452vNnO3RxpgRxphFOY8ztzVzG2MGGGOWGWN+NMb4e+xDiUiFpaAnInJx/Atcur0pz2sHrbUtce5S8HrOtreBT621TXBuNP9mzvY3gcnW2qY49/E9c5ecusA71tqGwH6gV6l+GhGplHRnDBGRi2CMOWytDSpk+0agk7V2vTHGG9hurY0wxuwGYq21p3K2b7PWRhpjdgHx1toTeY6RCIyz1tbNef5nwNta+68y+GgiUomoR09EpOTZIr4vqk1hTuT5/jQaUy0iF0FBT0Sk5N2U5+vMnO9nADfnfN8XmJbz/XjgPgBjjNsYU62sihSRyk+/IYqIXBx/Y8zCPM/HWmvPLLHia4yZhfPLdJ+cbQ8CHxljngB2Ab/L2f4Q8IEx5k6cnrv7gG2lXbyIVA0aoyciUoJyxuilW2t3e7oWERFduhURERGppNSjJyIiIlJJqUdPREREpJJS0BMRERGppBT0RERERCopBT0RERGRSkpBT0RERKSSUtATERERqaT+H0KwxxEyOg2vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the results\n",
    "x_axis = np.linspace(0, num_epochs, num_epochs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(x_axis, sgd_results, label='Training')\n",
    "ax.plot(x_axis, sgd_results_valid, label='Validation')\n",
    "ax.set_ylabel(\"Mean CEL\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.grid()\n",
    "\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "len(sgd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean CEL 0.15664341107694124\n",
      "Accuracy on training 0.9471142857142857\n",
      "Validation mean CEL 0.3418162960179054\n",
      "Accuracy on validation 0.8701333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Training mean CEL\", sgd_results[-1])\n",
    "print(\"Accuracy on training\", accuracies[-1])\n",
    "print(\"Validation mean CEL\", sgd_results_valid[-1])\n",
    "print(\"Accuracy on validation\", accuracies_valid[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mean CEL 0.38033656114043785\n",
      "Test accuracy 0.8565333333333334\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_CEL, test_accuracy = testing(model, criterion, test_loader, len(vocab))\n",
    "\n",
    "print(\"Test mean CEL\", test_CEL)\n",
    "print(\"Test accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.init_weights()\n",
    "\n",
    "classifier.ws_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight Parameter containing:\n",
      "tensor([[-0.0065,  0.0210, -0.0047,  ...,  0.0021, -0.0090,  0.0002],\n",
      "        [ 0.0038, -0.0103, -0.0030,  ..., -0.0002, -0.0070, -0.0035],\n",
      "        [ 0.0060,  0.0009, -0.0063,  ..., -0.0022,  0.0030, -0.0045],\n",
      "        ...,\n",
      "        [-0.0022, -0.0008, -0.0018,  ..., -0.0048,  0.0088,  0.0016],\n",
      "        [-0.0006,  0.0142,  0.0018,  ...,  0.0027, -0.0060,  0.0050],\n",
      "        [-0.0035, -0.0230,  0.0165,  ...,  0.0062,  0.0059, -0.0020]],\n",
      "       requires_grad=True)\n",
      "0.bias Parameter containing:\n",
      "tensor([ 0.0290,  0.0594,  0.0030,  0.0689,  0.0254,  0.0028,  0.0164,  0.0033,\n",
      "         0.0581,  0.0285, -0.0004,  0.0715,  0.0451,  0.0140,  0.0094,  0.0855],\n",
      "       requires_grad=True)\n",
      "2.weight Parameter containing:\n",
      "tensor([[ 5.7764e-02,  2.0102e-01, -1.7060e-01, -4.2006e-02, -7.8665e-02,\n",
      "          6.3256e-02,  6.6348e-02,  6.2663e-02,  2.8887e-01, -1.3245e-01,\n",
      "          2.3586e-02,  1.6659e-01,  9.7242e-02, -1.4332e-01, -1.1524e-01,\n",
      "          3.7746e-01],\n",
      "        [ 1.6566e-01,  1.7437e-01, -1.6710e-01,  2.5257e-01,  7.8475e-02,\n",
      "         -8.1486e-02, -1.8750e-01, -1.4075e-01,  1.9757e-01, -1.9270e-01,\n",
      "         -5.7360e-02, -1.0749e-01, -1.7804e-01,  1.5921e-01,  1.7752e-01,\n",
      "         -3.1537e-04],\n",
      "        [ 3.1607e-01, -6.4271e-02, -1.0987e-01,  6.2966e-01, -5.9082e-02,\n",
      "          1.2175e-01,  9.3758e-02,  9.4151e-02, -8.7914e-02, -2.6049e-01,\n",
      "          1.9596e-02,  1.4350e-01,  1.1461e-01,  4.4731e-02,  2.3976e-01,\n",
      "         -4.1670e-01],\n",
      "        [ 1.4255e-01, -5.1505e-02, -2.6956e-02,  2.2233e-01, -4.8828e-02,\n",
      "         -7.0108e-02,  1.6221e-01,  6.0779e-02, -2.1018e-01,  1.4816e-01,\n",
      "         -2.4165e-01,  4.0332e-03,  7.6032e-02, -1.0510e-01, -1.4155e-01,\n",
      "         -2.3549e-01],\n",
      "        [-3.1148e-01,  2.7956e-01,  2.3095e-01, -1.8744e-01, -1.4128e-02,\n",
      "         -1.9776e-01,  3.1462e-02,  6.3873e-03, -1.5189e-01,  2.4209e-01,\n",
      "         -3.4446e-02,  9.4573e-02,  2.9217e-01, -1.6677e-01,  4.2111e-02,\n",
      "          3.0330e-01],\n",
      "        [-1.4041e-01, -1.9833e-01, -1.2154e-01, -1.4635e-01,  2.6172e-02,\n",
      "         -1.7529e-01, -9.4719e-03, -1.6928e-01, -5.3043e-02, -1.8817e-01,\n",
      "          1.9282e-01,  1.4542e-01,  1.1262e-01, -1.1385e-01, -1.1478e-01,\n",
      "          1.1835e-01],\n",
      "        [-3.6710e-01,  4.0876e-01,  8.0080e-02, -2.8113e-01,  2.9580e-01,\n",
      "          1.2630e-01,  1.0102e-01,  2.1145e-02,  4.3499e-01,  2.3687e-01,\n",
      "         -7.4238e-02,  3.7905e-01,  2.2683e-01,  1.3953e-01, -3.7283e-01,\n",
      "          6.4225e-01],\n",
      "        [-2.3867e-01,  2.6519e-03,  1.8794e-01, -7.9713e-02,  5.6089e-02,\n",
      "          1.9840e-01, -1.6262e-02, -2.1199e-01, -2.1220e-02, -1.6988e-03,\n",
      "          1.1299e-01,  2.4934e-01,  3.7027e-02, -1.1624e-01,  7.8330e-02,\n",
      "         -1.2290e-01]], requires_grad=True)\n",
      "2.bias Parameter containing:\n",
      "tensor([-0.1950, -0.1070,  0.1502,  0.1440,  0.2458, -0.1092,  0.2208, -0.2430],\n",
      "       requires_grad=True)\n",
      "4.weight Parameter containing:\n",
      "tensor([[-0.2153,  0.0992,  0.4232,  0.3334, -0.0421,  0.2059, -0.7041, -0.2852],\n",
      "        [ 0.3135, -0.2945, -0.7425, -0.0322,  0.5832, -0.2931,  0.8983,  0.1368]],\n",
      "       requires_grad=True)\n",
      "4.bias Parameter containing:\n",
      "tensor([ 0.1330, -0.3636], requires_grad=True)\n",
      "[[-0.21532577  0.31346318]\n",
      " [ 0.09918025 -0.29454333]\n",
      " [ 0.4232431  -0.742456  ]\n",
      " [ 0.3333729  -0.03219656]\n",
      " [-0.04212745  0.5831935 ]\n",
      " [ 0.20594856 -0.29311886]\n",
      " [-0.7041183   0.89826316]\n",
      " [-0.28519708  0.13681473]]\n"
     ]
    }
   ],
   "source": [
    "paramslist = list()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    paramslist.append(param)\n",
    "    print(name, param)\n",
    "\n",
    "\n",
    "\n",
    "classifier.ws_1 = np.flip(np.rot90(paramslist[0].detach().numpy(), 1), 0)\n",
    "classifier.bias_1 = paramslist[1].detach().numpy()\n",
    "classifier.ws_2 = np.flip(np.rot90(paramslist[2].detach().numpy(), 1), 0)\n",
    "classifier.bias_2 = paramslist[3].detach().numpy()\n",
    "classifier.ws_3 = np.flip(np.rot90(paramslist[4].detach().numpy(), 1), 0)\n",
    "classifier.bias_3 = paramslist[5].detach().numpy()\n",
    "\n",
    "print(classifier.ws_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3782505169689974\n",
      "0.8562666666666666\n",
      "[[0.73459379 0.26540621]\n",
      " [0.8910143  0.1089857 ]\n",
      " [0.7260892  0.2739108 ]\n",
      " ...\n",
      " [0.3698329  0.6301671 ]\n",
      " [0.02046042 0.97953958]\n",
      " [0.82691791 0.17308209]]\n"
     ]
    }
   ],
   "source": [
    "cel_test2, output2, acc_test2 = classifier.predict_whole_set(X_test, Y_test)\n",
    "print(cel_test2)\n",
    "print(acc_test2)\n",
    "\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-148-6c8cc75b5bd7>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  nparr3 = np.array([arr1, arr2, arr3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = [1,2,3]\n",
    "arr2 = [4,5]\n",
    "arr3 = [7,8, 9,10]\n",
    "\n",
    "nparr3 = np.array([arr1, arr2, arr3])\n",
    "nparr = np.array([[1,2,3],[3,4,5],[4,5,6]])\n",
    "nparr2 = np.array([[1, 3],[3, 4],[3,4],[3,2],[2,1],[9,3]])\n",
    "\n",
    "npbias = np.array([1,1,1,1,1,1])\n",
    "\n",
    "np.argmax(nparr2, axis=1)\n",
    "npbias.reshape((6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43320269 0.56679731]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "ws_1 = np.random.rand(32, 16) - 0.5\n",
    "ws_2 = np.random.rand(16, 8) - 0.5\n",
    "ws_3 = np.random.rand(8, 2) - 0.5\n",
    "bias_1 = np.random.rand(16)\n",
    "bias_2 = np.random.rand(8)\n",
    "bias_3 = np.random.rand(2)\n",
    "\n",
    "\n",
    "layer_1 = classifier.relu(x.dot(ws_1) - bias_1)\n",
    "layer_2 = classifier.relu(layer_1.dot(ws_2))\n",
    "layer_3 = layer_2.dot(ws_3)\n",
    "layer_out = classifier.softmax(layer_3)\n",
    "\n",
    "print(layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa  uu\n",
      "aa  uu\n"
     ]
    }
   ],
   "source": [
    "string1 = \"aa \\x96 uu\"\n",
    "\n",
    "print(string1)\n",
    "\n",
    "text = re.sub(r'[\\\\x]', '', string1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 2 1 3 2 2 3] [4 4 2 0 0 0 3 4 0 0]\n",
      "[[  1   2   3   4 999]\n",
      " [999   7 999   9  10]\n",
      " [999  12  13  14 999]\n",
      " [999  17  18 999  20]]\n",
      "[978   1   1 282 599]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]])\n",
    "arr2 = np.array([[99,98,97,96,95],[89,88,87,86,85],[79,78,77,76,75],[69,68,67,66,65]])\n",
    "#np.flip(np.rot90(arr1, 1), 0)\n",
    "\n",
    "arr6 = np.array([1,2,3,4,5])\n",
    "arr7 = np.array([1,1,1,1,1])\n",
    "\n",
    "# arr1 = arr1 + np.random.uniform(-0.001, 0.001, size=arr1.shape)*np.random.randint(0, 2, size=arr1.shape)\n",
    "arr3 = np.zeros((4,5))\n",
    "\n",
    "\n",
    "#arr3[0::2] = arr1[0::2]\n",
    "#arr3\n",
    "\n",
    "index1 = np.random.choice(arr1.shape[0], 10, replace=True)\n",
    "index2 = np.random.choice(arr1.shape[1], 10, replace=True)\n",
    "\n",
    "index3 = random.sample(range(arr7.size), 3)\n",
    "\n",
    "print(index1, index2)\n",
    "\n",
    "arr1[index1, index2] = 999\n",
    "print(arr1)\n",
    "arr7[index3] = np.random.randint(100, 1000, 3)\n",
    "print(arr7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
