{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded assignment 1 - text classification using Genetic Algorithms\n",
    "## By Abdullah Karagøz\n",
    "\n",
    "In this assignmetn we'll make a binary text classifier using genetic algorithms. We will classify movie reviews from IMDB as either negative or positive. This task consists of several steps:\n",
    "\n",
    "1. Preprocessing of the text\n",
    "2. Genetich Algorithm\n",
    "3. Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Upload the text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"stayed and drinking\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in word_tokenize(sentence)]\n",
    "lemmatizer.lemmatize(\"stayed\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File directories\n",
    "corpus_train_pos_root = 'aclImdb/train/pos/'\n",
    "corpus_train_neg_root = 'aclImdb/train/neg/'\n",
    "corpus_test_pos_root = 'aclImdb/test/pos/'\n",
    "corpus_test_neg_root = 'aclImdb/test/neg/'\n",
    "\n",
    "# Corpus file objects\n",
    "files_train_pos = PlaintextCorpusReader(corpus_train_pos_root, '.*')\n",
    "files_train_neg = PlaintextCorpusReader(corpus_train_neg_root, '.*')\n",
    "files_test_pos = PlaintextCorpusReader(corpus_test_pos_root, '.*')\n",
    "files_test_neg = PlaintextCorpusReader(corpus_test_neg_root, '.*')\n",
    "\n",
    "\n",
    "# Getting review texts, labels and rates all in arrays\n",
    "reviews_train_pos = [files_train_pos.open(n).read() for n in files_train_pos.fileids()]\n",
    "rates_train_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_pos.fileids()]\n",
    "labels_train_pos = [1] * len(reviews_train_pos)\n",
    "\n",
    "reviews_train_neg = [files_train_neg.open(n).read() for n in files_train_neg.fileids()]\n",
    "rates_train_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_neg.fileids()]\n",
    "labels_train_neg = [-1] * len(reviews_train_neg)\n",
    "\n",
    "reviews_test_pos = [files_test_pos.open(n).read() for n in files_test_pos.fileids()]\n",
    "rates_test_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_pos.fileids()]\n",
    "labels_test_pos = [1] * len(reviews_test_pos)\n",
    "\n",
    "reviews_test_neg = [files_test_neg.open(n).read() for n in files_test_neg.fileids()]\n",
    "rates_test_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_neg.fileids()]\n",
    "labels_test_neg = [-1] * len(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all into two Pandas dataframes - training set and testing set\n",
    "train_set = pd.DataFrame()\n",
    "test_set = pd.DataFrame()\n",
    "\n",
    "train_set['review'] = reviews_train_pos + reviews_train_neg\n",
    "train_set['rate'] = rates_train_pos + rates_train_neg\n",
    "train_set['label'] = labels_train_pos + labels_train_neg\n",
    "\n",
    "test_set['review'] = reviews_test_pos + reviews_test_neg\n",
    "test_set['rate'] = rates_test_pos + rates_test_neg\n",
    "test_set['label'] = labels_test_pos + labels_test_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think to put this into own .PY file and import from there\n",
    "class text_preprocessor():\n",
    "    def __init__(self):\n",
    "        import nltk\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.punctuation = string.punctuation\n",
    "        \n",
    "        self.emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        self.emoticons = {\n",
    "            u\":‑\\)\":\"Happy face or smiley\",\n",
    "            u\":\\)\":\"Happy face or smiley\",\n",
    "            u\":-\\]\":\"Happy face or smiley\",\n",
    "            u\":\\]\":\"Happy face or smiley\",\n",
    "            u\":-3\":\"Happy face smiley\",\n",
    "            u\":3\":\"Happy face smiley\",\n",
    "            u\":->\":\"Happy face smiley\",\n",
    "            u\":>\":\"Happy face smiley\",\n",
    "            u\"8-\\)\":\"Happy face smiley\",\n",
    "            u\":o\\)\":\"Happy face smiley\",\n",
    "            u\":-\\}\":\"Happy face smiley\",\n",
    "            u\":\\}\":\"Happy face smiley\",\n",
    "            u\":-\\)\":\"Happy face smiley\",\n",
    "            u\":c\\)\":\"Happy face smiley\",\n",
    "            u\":\\^\\)\":\"Happy face smiley\",\n",
    "            u\"=\\]\":\"Happy face smiley\",\n",
    "            u\"=\\)\":\"Happy face smiley\"\n",
    "        }\n",
    "\n",
    "    def lower_case(self, text):\n",
    "        return str.lower(text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans('', '', self.punctuation))\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join([word for word in str(text).split() if word not in self.stop_words])\n",
    "    \n",
    "    def remove_freqwords(self, text, freq_words):\n",
    "        return \" \".join([word for word in str(text).split() if word not in freq_words])\n",
    "    \n",
    "    def remove_rarewords(self, text, rare_words):\n",
    "        return \" \".join([word for word in str(text).split() if word not in rare_words])\n",
    "    \n",
    "    def remove_emoji(self, text):\n",
    "        # src: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "        return self.emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    def remove_emoticons(self, text):\n",
    "        import re\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in self.emoticons) + u')')\n",
    "        return emoticon_pattern.sub(r'', text)\n",
    "    \n",
    "    def convert_emoticons(self, text):\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        for emot in self.emoticons:\n",
    "            text = re.sub(u'('+emot+')', \"_\".join(self.emoticons[emot].replace(\",\",\"\").split()), text)\n",
    "        return text\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url_pattern.sub(r'', text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocess_imdb_reviews(preprocessor, df):\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.lower_case(text))\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_punctuation(text))\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_stopwords(text))\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_urls(text))\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_emoji(text))\n",
    "\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for text in df[\"review\"].values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "\n",
    "    freq_words = set([w for (w, wc) in cnt.most_common(10)])\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_freqwords(text, freq_words))\n",
    "\n",
    "    n_rare_words = 10\n",
    "    rare_words = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "    df['review'] = df['review'].apply(lambda text: preprocessor.remove_rarewords(text, rare_words))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "preprocessor = text_preprocessor()\n",
    "train_set_processed = preprocess_imdb_reviews(preprocessor, train_set)\n",
    "test_set_processed = preprocess_imdb_reviews(preprocessor, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high cartoon comedy ran programs scho...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brilliant overacting lesley ann warren best dr...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>easily underrated inn brooks cannon sure flawe...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typical mel brooks much less slapstick movies ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>towards end felt technical felt classroom watc...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>kind enemies content watch bloody true watch m...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>saw descent last night stockholm festival huge...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>films pick pound turn rather 23rd century film...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>dumbest films ive ever seen rips nearly ever t...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label\n",
       "0      bromwell high cartoon comedy ran programs scho...     9      1\n",
       "1      homelessness houselessness george carlin state...     8      1\n",
       "2      brilliant overacting lesley ann warren best dr...    10      1\n",
       "3      easily underrated inn brooks cannon sure flawe...     7      1\n",
       "4      typical mel brooks much less slapstick movies ...     8      1\n",
       "...                                                  ...   ...    ...\n",
       "24995  towards end felt technical felt classroom watc...     4     -1\n",
       "24996  kind enemies content watch bloody true watch m...     3     -1\n",
       "24997  saw descent last night stockholm festival huge...     3     -1\n",
       "24998  films pick pound turn rather 23rd century film...     1     -1\n",
       "24999  dumbest films ive ever seen rips nearly ever t...     1     -1\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "There are a lot of classifiers. I haven't any experience in most of them, just a little familiar with Neural Network. When assigning fitness in GA I have to test-run all the 25k samples. If I have X number of classifiers and I'll run in N number of generations, then the complexity of running the GA will be at least like:\n",
    "X * N * 25,000. That'll be a big number, and my computer is not that fast.\n",
    "\n",
    "Taking hardware limitations into account, I wanted to choose a model that's quick to run. NN with multilayers may be too complicated and too slow to run.\n",
    "\n",
    "After taking a quick look at this link:\n",
    "https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d\n",
    "\n",
    "I decided Support Vector Machine (SVM) would be best to try using as classifier as it's quick to run.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0046381128849322245\n",
      "review    brief prologue showing masked man stalking sla...\n",
      "rate                                                      7\n",
      "label                                                     1\n",
      "Name: 55, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_words_pos = Counter()\n",
    "count_words_neg = Counter()\n",
    "\n",
    "sum_words_pos = len(\" \".join(train_set_processed.loc[train_set_processed['label'] == 1, \"review\"].values).split())\n",
    "sum_words_neg = len(\" \".join(train_set_processed.loc[train_set_processed['label'] == -1, \"review\"].values).split())\n",
    "\n",
    "count_reviews_pos = Counter()\n",
    "count_reviews_neg = Counter()\n",
    "\n",
    "words_set = set((\" \".join(train_set_processed['review'].values)).split())\n",
    "\n",
    "word_freq_pos = nltk.FreqDist(\" \".join(train_set_processed.loc[train_set_processed['label'] == 1, \"review\"].values).split())\n",
    "\n",
    "word_freq_neg = nltk.FreqDist(\" \".join(train_set_processed.loc[train_set_processed['label'] == -1, \"review\"].values).split())\n",
    "\n",
    "\n",
    "\n",
    "word_weights_1 = {word: (word_freq_pos[word] / sum_words_pos) - (word_freq_neg[word] / sum_words_pos) for word in words_set}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for text in train_set_processed.loc[train_set_processed['label'] == 1, \"review\"].values\n",
    "#     sum_words_pos += len(text.split())\n",
    "#     for word in text.split():\n",
    "#         count_words_pos[word] += 1\n",
    "\n",
    "# for text in train_set_processed.loc[train_set_processed['label'] == -1, \"review\"].values\n",
    "#     sum_words_neg += len(text.split())\n",
    "#     for word in text.split():\n",
    "#         count_words_neg[word] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19934\n",
      "25000\n",
      "0.79736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19934"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(text, word_weights):\n",
    "    val = 0\n",
    "    for word in text.split():\n",
    "        if word in word_weights:\n",
    "            val += word_weights[word]    \n",
    "    if val > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "test_set_processed['preds'] = test_set_processed['review'].apply(lambda text: predict(text, word_weights_1))\n",
    "\n",
    "correct = test_set_processed.loc[test_set_processed['preds'] == test_set_processed['label']].count()[0]\n",
    "\n",
    "print(correct) \n",
    "print(test_set_processed['review'].size)\n",
    "print(correct / test_set_processed['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
