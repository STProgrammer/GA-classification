{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded assignment 1 - text classification using Genetic Algorithms\n",
    "\n",
    "In this assignmetn we'll make a binary text classifier using genetic algorithms. We will classify movie reviews from IMDB as either negative or positive. \n",
    "\n",
    "The assignment text contain 3 steps, so I divided the assignment in 3 sections:\n",
    "\n",
    "1. Preprocessing of the text\n",
    "2. Genetich Algorithm\n",
    "3. Testing\n",
    "\n",
    "**Preprocessing text**\n",
    "\n",
    "First I upload all data, put them together and split them into training, validation and testing set. Then I preprocess the text using different methods like removing stop words, lemmaitzation etc.\n",
    "\n",
    "Training set will be used to build vocabulary and IDF values. Validation set will be used when assigning fitness in Genetic Algorithm which I'll get into more. Testing set will be used at the end in validation part.\n",
    "\n",
    "What we basically do here is to try to predict what class a text belongs to by making some calculations based on how many of which words the text containts. But when we do this calculation we don't want to consider all the words. That's because some words don't help with predictions. That's why we do preprocessing and turn all the reviews into equally sized vectors containing numerical values which can be proceeded by our neural network. How we do the preprocessing will be futher explained later.\n",
    "\n",
    "\n",
    "I have built two classes for preprocessing stage. One is TextPreprocessor which processes contents with methods like removing stop words, lemmatization etc.\n",
    "\n",
    "The other is TF-IDF vectorizer class which is used to build a Numpy array of TF-IDF vectors. Each document is turned into a TF-IDF vector. TF\n",
    "\n",
    "**Genetic Algorithm**\n",
    "Genetic algorithm  (often referred as GA) is an algorithm inspired by evolution theory. Here rather than species we have solutions to a problem. We call those solutions for Chromosomes. In this case Chromosomes are just weights for a neural network. We use methods like \"cross-over\" and \"mutation\" to make changes in those Chromosomes, and we try to pick the best Chromosome. In here the best Chromosome is \n",
    "\n",
    "The classifier I used is a simple neural network. I considered decision tree, but I'm not familiar with decision trees while I'm a bit familiar with neural network. Thus I decided to go with neural network. \n",
    "\n",
    "The neural net has only 2 hidden layers, 16 and 8 hidden nodes. I kept it simple because I had to take hardware limitations into account too. In the Genetic Algorithm every time we assign fitness we are going to predict the whole validation set for each classifier. So I didn't want to make it very slow.\n",
    "\n",
    "**Testing**\n",
    "Validation is simply using the optimizied weights to predict all reviews in the testing set to see how our GA performs. In the assignment text they suggested to use testing set for assigning fitness in GA and in validation part. But I thought that could make weights a bit biased. Thus other than training set I used two seperate sets: validation set and testing set. Validation set will be used when assigning fitness while optimizing weights in GA. Testing set will be used at the end.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "The preprocessing consists of two parts. Preproceesing the texts in the dataframe, and then converting the texts into vectors that can be used in classifier.\n",
    "\n",
    "I have built twp seperate classes for each one. TextPreprocessor class and TF-IDF vectorizer.\n",
    "\n",
    "I searched online about different vectorizartion methods in text classifying. I found TF-IDF vectorization contains more data than others, which could help in classifying. With TF-IDF vectors one can see which words are more \"special\" than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Uploading data\n",
    "\n",
    "All the data is placed in four seperate folders. I found PlainTextCorpusReader from nlkt.corpus to be more effective in reading multiple text files. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Upload the text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from scipy import special\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used for loop iteration to extract the info as array. Then I made a dataframe where I put all the samples together.\n",
    "\n",
    "The data contains 3 columns (among index): reviews (the content of TXT files), labels (positive or negative, 1 or 0), and rates (extracted from file names).\n",
    "\n",
    "I haven't used rate values anywhere in the assignment. I planned to use them but didn't use them. But still I didn't remove them so the code can be further developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File directories\n",
    "corpus_train_pos_root = 'aclImdb/train/pos/'\n",
    "corpus_train_neg_root = 'aclImdb/train/neg/'\n",
    "corpus_test_pos_root = 'aclImdb/test/pos/'\n",
    "corpus_test_neg_root = 'aclImdb/test/neg/'\n",
    "\n",
    "# Corpus file objects\n",
    "files_train_pos = PlaintextCorpusReader(corpus_train_pos_root, '.*')\n",
    "files_train_neg = PlaintextCorpusReader(corpus_train_neg_root, '.*')\n",
    "files_test_pos = PlaintextCorpusReader(corpus_test_pos_root, '.*')\n",
    "files_test_neg = PlaintextCorpusReader(corpus_test_neg_root, '.*')\n",
    "\n",
    "\n",
    "# Getting review texts, labels and rates all in arrays\n",
    "reviews_train_pos = [files_train_pos.open(n).read() for n in files_train_pos.fileids()]\n",
    "rates_train_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_pos.fileids()]\n",
    "labels_train_pos = [1] * len(reviews_train_pos)\n",
    "\n",
    "reviews_train_neg = [files_train_neg.open(n).read() for n in files_train_neg.fileids()]\n",
    "rates_train_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_train_neg.fileids()]\n",
    "labels_train_neg = [0] * len(reviews_train_neg)\n",
    "\n",
    "reviews_test_pos = [files_test_pos.open(n).read() for n in files_test_pos.fileids()]\n",
    "rates_test_pos = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_pos.fileids()]\n",
    "labels_test_pos = [1] * len(reviews_test_pos)\n",
    "\n",
    "reviews_test_neg = [files_test_neg.open(n).read() for n in files_test_neg.fileids()]\n",
    "rates_test_neg = [int(re.split(\"_|\\.\", n)[-2]) for n in files_test_neg.fileids()]\n",
    "labels_test_neg = [0] * len(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_set = pd.DataFrame()\n",
    "\n",
    "# Puttin all data into whole set\n",
    "whole_set['review'] = reviews_train_pos + reviews_test_pos + reviews_train_neg + reviews_test_neg\n",
    "whole_set['rate'] = rates_train_pos + rates_test_pos + rates_train_neg + rates_test_neg\n",
    "whole_set['label'] = labels_train_pos + labels_test_pos + labels_train_neg + labels_test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Splitting data\n",
    "\n",
    "The original sample was like 50% was testing and 50% for training. That's too little for training and too big for testing. And there's no validation.\n",
    "\n",
    "Thus I made the split into three: \n",
    "* 70 % on training, \n",
    "* 15 % on testing\n",
    "* 15 % on validation.\n",
    "\n",
    "Splitting the data into three sets where training set is significantly bigger is common in machine learning.\n",
    "\n",
    "TF-IDF vectors will be based on training set. Thus we need training set to be in bigger size.\n",
    "\n",
    "Validation set will be used in Genetic Algorithm when assigning fitness to classifiers.\n",
    "Too big validation set could slow it the GA much.\n",
    "\n",
    "Testing set will be used at the end to test how much the \"trained classifier\" performs.\n",
    "\n",
    "Then I shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's a soap-opera drawing upon an applied ethi...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have to say this is better than most SyFy ou...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is a waste of film stock. Do you be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clark Gable plays a con man who busts into the...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I normally don't like romantic films, but I lo...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>The idea of In the Name of the People is good,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>The Outsiders is undoubtedly a classic Austral...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>Admittedly, I tuned into this in the hopes of ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>In the late 1940s there was a short film serie...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>I agree with other users comments in that the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label\n",
       "0      It's a soap-opera drawing upon an applied ethi...     7      1\n",
       "1      I have to say this is better than most SyFy ou...     3      0\n",
       "2      This movie is a waste of film stock. Do you be...     1      0\n",
       "3      Clark Gable plays a con man who busts into the...     8      1\n",
       "4      I normally don't like romantic films, but I lo...    10      1\n",
       "...                                                  ...   ...    ...\n",
       "34995  The idea of In the Name of the People is good,...     2      0\n",
       "34996  The Outsiders is undoubtedly a classic Austral...     8      1\n",
       "34997  Admittedly, I tuned into this in the hopes of ...     3      0\n",
       "34998  In the late 1940s there was a short film serie...     9      1\n",
       "34999  I agree with other users comments in that the ...     2      0\n",
       "\n",
       "[35000 rows x 3 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting all into two Pandas dataframes - training set and testing set\n",
    "train_set = pd.DataFrame()\n",
    "test_set = pd.DataFrame()\n",
    "valid_set = pd.DataFrame()\n",
    "\n",
    "# Dividing reviews into negative and positive to make sure data is always balanced\n",
    "negatives = whole_set.loc[whole_set['label'] == 0]\n",
    "positives = whole_set.loc[whole_set['label'] == 1]\n",
    "\n",
    "# Splitting positive and negative reviews\n",
    "train_set, valid_set, test_set = np.split(positives, [int(0.7*len(positives)), int(0.85*len(positives))])\n",
    "tr_neg, vl_neg, ts_neg = np.split(negatives, [int(0.7*len(negatives)), int(0.85*len(negatives))])\n",
    "\n",
    "# Appending negatives to positives\n",
    "train_set = train_set.append(tr_neg)\n",
    "valid_set = valid_set.append(vl_neg)\n",
    "test_set = test_set.append(ts_neg)\n",
    "\n",
    "# Shuffle and reset the index\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "valid_set = valid_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data exploration\n",
    "\n",
    "Here we explore how the data looks like. That is to figure out how to preprocess the data, what to eliminate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 20 counted by appearance in nr of reviews:  [('the', 34512), ('a', 33657), ('and', 33527), ('of', 33109), ('to', 32761), ('is', 31043), ('in', 30224), ('this', 29017), ('that', 27272), ('it', 26552), ('I', 25151), ('for', 24184), ('with', 23784), ('but', 22444), ('was', 22333), ('The', 21788), ('as', 21391), ('on', 20873), ('/><br', 20460), ('have', 19648)]\n",
      "\n",
      "Most common 20 counted by word count total:  [('the', 398431), ('a', 215543), ('and', 211791), ('of', 198887), ('to', 183609), ('is', 142852), ('in', 119064), ('I', 92383), ('that', 89328), ('this', 79871), ('it', 75658), ('/><br', 71081), ('was', 64813), ('as', 58564), ('with', 57992), ('for', 56780), ('The', 46966), ('but', 46488), ('on', 42896), ('movie', 42730)]\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "# Most common words\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "cnt2 = Counter()\n",
    "for text in train_set[\"review\"].values:\n",
    "    # Counting the words\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "    # Counting in how many reviews the word appears\n",
    "    for word in set(text.split()):\n",
    "        cnt2[word] += 1\n",
    "\n",
    "print(\"Most common 20 counted by appearance in nr of reviews: \", cnt2.most_common(20))\n",
    "print(\"\\nMost common 20 counted by word count total: \", cnt.most_common(20))\n",
    "\n",
    "# print(\"Least common 20 counted by appearance in nr of reviews: \", cnt2.most_common()[:-20])\n",
    "# print(\"\\nLeast common 20 counted by word count total: \", cnt.most_common()[:-20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen there are a lot of stop words. There are HTML tags in the set too. The stop words and HTML tags will be removed in with TextPreprocessor class.\n",
    "\n",
    "Vocabulary here means all the words used in the training set. The vocabulary will be based on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '% of documents')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What % of the reviews use what % of the vocab\n",
    "vocab_size = len(cnt2)\n",
    "sample_size = len(train_set)\n",
    "\n",
    "y = [c/sample_size * 100 for (w, c) in cnt2.most_common()]\n",
    "x = [c/vocab_size * 100 for c in range(1, vocab_size+1)]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"% of the words used in % of the documents\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"% of vocabolary\")\n",
    "ax.set_ylabel(\"% of documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows how much % of the vocabulary is used by how much % of the documents.\n",
    "\n",
    "We see that only a tiny fraction of the vocubulary are used in most of the documents. \n",
    "Most of the words in vocabulary are rare words, also they're used only in a small fraction of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length distribution of the reviews\n",
    "train_set['rev_lens_raw'] = train_set['review'].str.len()\n",
    "train_set['rev_lens_words'] = train_set['review'].str.split().apply(len)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "ax[0].hist(train_set['rev_lens_raw'], bins='auto')\n",
    "ax[0].set_xlabel(\"Review length in characters\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[1].hist(train_set['rev_lens_words'], bins='auto')\n",
    "ax[1].set_xlabel(\"Review length in words\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above shows distribution of frequency and review length. The longest reviews are 1000 words which is not very unusual. \n",
    "\n",
    "We want to see if short reviews contain garbage, and see they look like normal movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I hope this group of film-makers never re-unites.',\n",
       "       'Brilliant and moving performances by Tom Courtenay and Peter Finch.',\n",
       "       'This movie is terrible but it has some good effects.',\n",
       "       \"I wouldn't rent this one even on dollar rental night.\",\n",
       "       \"You'd better choose Paul Verhoeven's even if you have watched it.\",\n",
       "       'Adrian Pasdar is excellent is this film. He makes a fascinating woman.',\n",
       "       'Ming The Merciless does a little Bardwork and a movie most foul!',\n",
       "       'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.',\n",
       "       'This is a great movie. Too bad it is not available on home video.',\n",
       "       'Comment this movie is impossible. Is terrible, very improbable, bad interpretation e direction. Not look!!!!!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shortest reviews\n",
    "train_set.nsmallest(10, 'rev_lens_words')['review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop reviews that are very long\n",
    "#train_set = train_set.loc[train_set['rev_lens_word'] < 1800]\n",
    "#train_set.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocessing and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerocessing and vectorization serves same goal. What we want to do is to buid\n",
    "\n",
    "What we basically want to do here is to try to predict what class a text belongs to by making some calculations based on how many of which words in the vocabulary that particular text contains. \n",
    "\n",
    "So we want to build a vocabulary first, also a list of words. But we don't want to include absolutely all words used in the training set to be included in the vocabulary. Thus we make some preprocessing, some filtering etc. and that's where TextPreprocessor comes in.\n",
    "\n",
    "E.g. stopwords like \"the\", \"a\", \"and\" etc. don't say anything about whether the review is positive or negative. Thus in the preprocessing stage we eliminate them. We eliminate numbers and other words too that don't give meaning.\n",
    "\n",
    "Very common words, words that are used in almost all reviews, don't say much about if the review is negative or positive. So we eliminate most common words too. But very rare words also don't say much about \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 TextPreprocessor class\n",
    "\n",
    "Most of the functions are in this class are taken from a code posted on Canvas. I just put hem into a class, made some changes and built some additional functions.\n",
    "\n",
    "Preprocessing is done with a function in TextProcessor class named def `preprocess_imdb_reviews()` which uses other functions in the class. It processes all the text in a dataframe under column \"review\" and returns a dataframe with additional column named \"processed\" where all processed text are saved. I keep both of them to be able to repeat code and to make a comparison later.\n",
    "\n",
    "The functions are applied by using .apply() function of the dataframe, which apply a function to each row in a chosen column.\n",
    "\n",
    "As mentioned we don't want to include all words used in the training set to be included in the vocabulary. Thus we make some changes. Here is how we process the text:\n",
    "1. We make all lower cases. Some people use upper case to highlight their words, and first letter of words are in upper case after periods. All all those words mean the same thing, so we don't want to discriminate between those. The only problem here is that movie names and person names would be processed just like some words. E.g. the movie \"Cars\" would be interpreted as noun in plural form cars, and eventually be lemmatized into car or removed. But it's very difficult to write a code to determine what is movie / person name or not, and they don't say much about whether a movie review is positive or negative. Thus for now we just ignore this problem. Lower casing all words also makes other processing steps easier.\n",
    "2. We remove HTML tags. They are not words with meaning, just some clutter to be removed.\n",
    "5. We remove URLs. They are not words with meaning, so just remove them.\n",
    "3. We convert emoticons into text. We do this before removing numbers or anything else as emoticons may contain numbers too. We don't want to remove them as they may contain useful information about the review.\n",
    "4. We remove numbers. They can be dates, ages etc. which don't give much relevant info about the review.\n",
    "6. We expand contractions. Alos \"don't\" means same as \"do not\". So we don't want to distinguish between those. Expanding contractions also will make removing stop words easier. It's important to do it early as it will generate new stop words.\n",
    "7. We remove emojis\n",
    "8. We remove punctuations, like dots and slashes. In computers \"something\" and \"something,\" are perceived as two different words, we don't want that.\n",
    "9. We lemmatize words. Lemmatizing means converting words to base forms.E.g. \"boring\" and \"bored\" are same words, \"child\" and \"children\" are actually same word. We want the computer to perceive them as same words too. Words with same base should be considered as same word.\n",
    "10. We remove the stop words. Words like \"the\", \"a\", \"and\" may be used in all reviews and don't give any information about review. So we just remove them.\n",
    "11. Curbing. We remove the words that exists in more than 85 % of the documents. They are some common words that don't give much context. And we remove the words that exists in less than 0.05 % of the documents. Rare words are words like special movie names, actor names, weirdo words, misspesllings etc. Words that are used only here and there or only in one review can't be used in calculating which class a reviews belongs to. The rates 0.85 and 0.0005 are paramaters to the function. I experimented with different values, I don't want to end up with vocabulary that's too little, but nor too big. Most of the words in vocabulary are rare words that are used in tiny fraction of the set. Thus the bottom curb is just 0.0005. Too little vocabulary give too little information. Too big vocabulary is too heavy for the neural network, and may contain a lot of words that give little information. A vocabulary of 10k-20k is enough. After experiemting with different values I did choose 0.85 and 0.0005 as curbing values.\n",
    "12. At the end I found some garbage words still existing in the set. I made an array of those and removed those from all the reviews. The array is: [\"□\", \"▢\", \"■\", \"\\x96\", \"st\", \"nd\", \"rd\", \"th\"]. \\x96 is just some triangle. st, nd, rd and th are remainders of numerical values like 1st, 2dn, 3rd, 4th etc. which is meaningless.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I think to put this into own .PY file and import from there\n",
    "class TextPreprocessor():\n",
    "    def __init__(self):\n",
    "        import nltk\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        from nltk.corpus import stopwords\n",
    "        \", \".join(stopwords.words('english'))\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer \n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.punctuation = string.punctuation\n",
    "        \n",
    "        self.emoji_pattern = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        self.emoticons = {\n",
    "            u\":‑\\)\":\"Happy face or smiley\",\n",
    "            u\":\\)\":\"Happy face or smiley\",\n",
    "            u\":-\\]\":\"Happy face or smiley\",\n",
    "            u\":\\]\":\"Happy face or smiley\",\n",
    "            u\":-3\":\"Happy face smiley\",\n",
    "            u\":3\":\"Happy face smiley\",\n",
    "            u\":->\":\"Happy face smiley\",\n",
    "            u\":>\":\"Happy face smiley\",\n",
    "            u\"8-\\)\":\"Happy face smiley\",\n",
    "            u\":o\\)\":\"Happy face smiley\",\n",
    "            u\":-\\}\":\"Happy face smiley\",\n",
    "            u\":\\}\":\"Happy face smiley\",\n",
    "            u\":-\\)\":\"Happy face smiley\",\n",
    "            u\":c\\)\":\"Happy face smiley\",\n",
    "            u\":\\^\\)\":\"Happy face smiley\",\n",
    "            u\"=\\]\":\"Happy face smiley\",\n",
    "            u\"=\\)\":\"Happy face smiley\"\n",
    "        }\n",
    "        \n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.html_pattern = re.compile('<.*?>')\n",
    "\n",
    "    def lower_case(self, text):\n",
    "        return str.lower(text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans('', '', self.punctuation))\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join([word for word in text.split() if word not in self.stop_words])\n",
    "    \n",
    "    def remove_words(self, text, freq_words):\n",
    "        return \" \".join([word for word in text.split() if word not in freq_words])\n",
    "    \n",
    "    def remove_emoji(self, text):\n",
    "        # src: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "        return self.emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    def remove_emoticons(self, text):\n",
    "        import re\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in self.emoticons) + u')')\n",
    "        return emoticon_pattern.sub(r'', text)\n",
    "    \n",
    "    def convert_emoticons(self, text):\n",
    "        # src : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "        for emot in self.emoticons:\n",
    "            text = re.sub(u'('+emot+')', \"_\".join(self.emoticons[emot].replace(\",\",\"\").split()), text)\n",
    "        return text\n",
    "    \n",
    "    def lemmatization(self, text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = word_tokenize(text)\n",
    "        for i in ['v','n','a']:\n",
    "            tokens = [lemmatizer.lemmatize(word, i) for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        text = re.sub(r\"i'm\", \" i am \", text)\n",
    "        text = re.sub(r\" im \", \" i am \", text)\n",
    "        text = re.sub(r\"\\: p\", \"\", text)\n",
    "        text = re.sub(r\" ive \", \" i have \", text)\n",
    "        text = re.sub(r\" he's \", \" he is \", text)\n",
    "        text = re.sub(r\" she's \", \" she is \", text)\n",
    "        text = re.sub(r\" that's \", \" that is \", text)\n",
    "        text = re.sub(r\" what's \", \" what is \", text)\n",
    "        text = re.sub(r\" where's \", \" where is \", text)\n",
    "        text = re.sub(r\" haven't \", \" have not \", text)\n",
    "        text = re.sub(r\" ur \", \" you are \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\" won't \", \" will not \", text)\n",
    "        text = re.sub(r\" wouldn't \", \" would not \", text)\n",
    "        text = re.sub(r\" can't \", \" cannot \", text)\n",
    "        text = re.sub(r\" couldn't \", \" could not \", text)\n",
    "        text = re.sub(r\" don't \", \" do not \", text)\n",
    "        text = re.sub(r\" didn't \", \" did not \", text)\n",
    "        text = re.sub(r\" doesn't \", \" does not \", text)\n",
    "        text = re.sub(r\" isn't \", \" is not \", text)\n",
    "        text = re.sub(r\" it's \", \" it is \", text)\n",
    "        text = re.sub(r\" who's \", \" who is \", text)\n",
    "        text = re.sub(r\" there's \", \" there is \", text)\n",
    "        text = re.sub(r\" weren't \", \" were not \", text)\n",
    "        text = re.sub(r\" wasn't \", \" was not \", text)\n",
    "        text = re.sub(r\" ok \", \" okay \", text)\n",
    "        text = re.sub(r\" you're \", \" you are \", text)\n",
    "        text = re.sub(r\" c'mon \", \" come on \", text)\n",
    "        text = re.sub(r\"\\'s\", \" s\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        text = re.sub(r'[0-9]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        return self.html_pattern.sub(r'', text)\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        return self.url_pattern.sub(r'', text)\n",
    "    \n",
    "    def process_text_vectorized(self, text):\n",
    "        text = self.lower_case(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.remove_numbers(text)\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.remove_emoji(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.lemmatization(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_2(self, corpus, max_df, min_df, n_freq_words = 10, n_rare_words = 10):\n",
    "        \n",
    "        vecpreprop = np.vectorize(self.process_text_vectorized)\n",
    "        \n",
    "        arr1 = vecpreprop(corpus)\n",
    "        \n",
    "        from collections import Counter\n",
    "        cnt = Counter()\n",
    "        cnt2 = Counter()\n",
    "        for text in corpus:\n",
    "            # Counting the words\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "            # Counting in how many reviews the word appears\n",
    "            for word in set(text.split()):\n",
    "                cnt2[word] += 1\n",
    "\n",
    "\n",
    "        # Removing most frequent words\n",
    "        freq_words = set([w for (w, wc) in cnt.most_common(n_freq_words)])\n",
    "        rem_freq_words = np.vectorize(self.remove_words)\n",
    "        arr2 = rem_freq_words(arr1, freq_words)\n",
    "\n",
    "        # Removing rarest words\n",
    "        rare_words = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "        \n",
    "        rem_rare_words = np.vectorize(self.remove_words)\n",
    "        arr3 = rem_rare_words(arr2, rare_words)\n",
    "\n",
    "        # Remove words used in >90% and <5% of the reviews\n",
    "        curb_max_amount = len(corpus) * max_df\n",
    "        curb_min_amount = len(corpus) * min_df\n",
    "\n",
    "        curb_words = set([w for (w, wc) in cnt2.most_common() if wc > curb_max_amount or wc < curb_min_amount])\n",
    "        print(len(cnt2))\n",
    "        print(len(curb_words))\n",
    "        rem_curb_words = np.vectorize(self.remove_words)\n",
    "        arr4 = rem_curb_words(arr3, curb_words)\n",
    "        \n",
    "        return arr4\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    # preprocessing IMDB reviews\n",
    "    def preprocess_imdb_reviews(self, df, max_df, min_df, n_freq_words = 10, n_rare_words = 10):\n",
    "        df['processed'] = df['review'].apply(lambda text: self.lower_case(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_html_tags(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_urls(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.convert_emoticons(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_numbers(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.expand_contractions(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_emoji(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_punctuation(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.lemmatization(text))\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_stopwords(text))\n",
    "\n",
    "\n",
    "        from collections import Counter\n",
    "        cnt = Counter()\n",
    "        cnt2 = Counter()\n",
    "        for text in df['processed'].values:\n",
    "            # Counting the words\n",
    "#             for word in text.split():\n",
    "#                 cnt[word] += 1\n",
    "            # Counting in how many reviews the word appears\n",
    "            for word in set(text.split()):\n",
    "                cnt2[word] += 1\n",
    "\n",
    "\n",
    "        # Removing most frequent words\n",
    "#         freq_words = set([w for (w, wc) in cnt.most_common(n_freq_words)])\n",
    "        \n",
    "#         df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, freq_words))\n",
    "\n",
    "#         # Removing rarest words\n",
    "#         rare_words = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "#         df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, rare_words))\n",
    "\n",
    "        # Remove words used in >90% and <5% of the reviews\n",
    "        curb_max_amount = len(df) * max_df\n",
    "        curb_min_amount = len(df) * min_df\n",
    "        curb_words = set([w for (w, wc) in cnt2.most_common() if wc > curb_max_amount or wc < curb_min_amount])\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, curb_words))\n",
    "        \n",
    "        # Other words to remove\n",
    "        rem_words = [\"\\x96\", \"st\", \"nd\", \"rd\", \"th\"]\n",
    "        df['processed'] = df['processed'].apply(lambda text: self.remove_words(text, rem_words))\n",
    "\n",
    "        return df, cnt2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 TF IDF Vectorizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF VECTORIZER CLASS\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "\n",
    "class TfIdfVectorizer:\n",
    "    def __init__(self, df):\n",
    "        self._idfs = self.prepare_idfs(df)\n",
    "        self._vocab = self.prepare_vocab(df)\n",
    "    \n",
    "    @property\n",
    "    def idfs(self):\n",
    "        return self._idfs\n",
    "    \n",
    "    @idfs.setter\n",
    "    def idfs(self, idfs):\n",
    "        self._idfs = idfs\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "    \n",
    "    @vocab.setter\n",
    "    def vocab(self, vocab):\n",
    "        self._vocab = vocab\n",
    "    \n",
    "    # Prepare and return vocab out of corpus\n",
    "    def prepare_vocab(self, df):\n",
    "        # Prepare the vocab\n",
    "        self.vocab = set(\" \".join(df['processed'].values).split())\n",
    "        self.vocab = dict.fromkeys(self.vocab, 0)\n",
    "        self.vocab.update((k, i) for i, k in enumerate(self.vocab))\n",
    "        return self.vocab\n",
    "    \n",
    "    # Prepare and return idfs out of corpus\n",
    "    def prepare_idfs(self, df):\n",
    "        # Counting how many reviews a word appears ins\n",
    "        cnt = Counter()\n",
    "        for text in df[\"processed\"].values:\n",
    "            for word in set(text.split()):\n",
    "                cnt[word] += 1\n",
    "        # Preparing the IDF vector\n",
    "        size = len(df)\n",
    "        self.idfs = dict()\n",
    "        for w, c in cnt.items():\n",
    "            self.idfs[w] = math.log(size / (1 + c))\n",
    "        return self.idfs\n",
    "\n",
    "\n",
    "    # TF-IDF vectorize a single text, returning an np.array\n",
    "    def tf_idf_vectorize(self, text):\n",
    "        freq_dist = FreqDist(text.split())\n",
    "        vector = np.zeros(len(self.vocab))\n",
    "        for w, c in freq_dist.items():\n",
    "            if w in self.vocab:\n",
    "                vector[self.vocab[w]] = c * self.idfs[w]\n",
    "        return vector\n",
    "\n",
    "\n",
    "    # One hot encode labels\n",
    "    def one_hot_encode(self, label, nr_of_labels):\n",
    "        arr = np.zeros(nr_of_labels, dtype=int)\n",
    "        arr[label] = 1\n",
    "        return arr\n",
    "\n",
    "    # Vectorize all in the dataset\n",
    "    def tf_idf_vectorize_all(self, df):\n",
    "        vectors = np.array(df['processed'].apply(lambda text: self.tf_idf_vectorize(text)).values.tolist())\n",
    "        return vectors\n",
    "    \n",
    "    # Turn all labels into one hot encoded arrays\n",
    "    def one_hot_encode_all(self, df, nr_of_labels):\n",
    "        vector = np.array(df['label'].apply(lambda label: self.one_hot_encode(label, nr_of_labels)).values.tolist())\n",
    "        return vector\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.4.3 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abdka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "      <th>rev_lens_raw</th>\n",
       "      <th>rev_lens_words</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's a soap-opera drawing upon an applied ethi...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1279</td>\n",
       "      <td>235</td>\n",
       "      <td>draw upon apply ethic idea movie human suffer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have to say this is better than most SyFy ou...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>132</td>\n",
       "      <td>say good say muchthe plot someone buy game mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is a waste of film stock. Do you be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1758</td>\n",
       "      <td>327</td>\n",
       "      <td>movie waste film stock believe map plan milita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clark Gable plays a con man who busts into the...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1113</td>\n",
       "      <td>188</td>\n",
       "      <td>clark gable play con man bust life hardboiled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I normally don't like romantic films, but I lo...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1313</td>\n",
       "      <td>234</td>\n",
       "      <td>normally like romantic film love film much sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>The idea of In the Name of the People is good,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>113</td>\n",
       "      <td>idea name people good murderer want daughter e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>The Outsiders is undoubtedly a classic Austral...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>646</td>\n",
       "      <td>110</td>\n",
       "      <td>outsider undoubtedly classic australian tv ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>Admittedly, I tuned into this in the hopes of ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2147</td>\n",
       "      <td>365</td>\n",
       "      <td>admittedly tune hop see beefcake shot jam brol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>In the late 1940s there was a short film serie...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4848</td>\n",
       "      <td>788</td>\n",
       "      <td>late short film series entitle flicker flashba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>I agree with other users comments in that the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>110</td>\n",
       "      <td>agree user comment two main role well act guy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label  \\\n",
       "0      It's a soap-opera drawing upon an applied ethi...     7      1   \n",
       "1      I have to say this is better than most SyFy ou...     3      0   \n",
       "2      This movie is a waste of film stock. Do you be...     1      0   \n",
       "3      Clark Gable plays a con man who busts into the...     8      1   \n",
       "4      I normally don't like romantic films, but I lo...    10      1   \n",
       "...                                                  ...   ...    ...   \n",
       "34995  The idea of In the Name of the People is good,...     2      0   \n",
       "34996  The Outsiders is undoubtedly a classic Austral...     8      1   \n",
       "34997  Admittedly, I tuned into this in the hopes of ...     3      0   \n",
       "34998  In the late 1940s there was a short film serie...     9      1   \n",
       "34999  I agree with other users comments in that the ...     2      0   \n",
       "\n",
       "       rev_lens_raw  rev_lens_words  \\\n",
       "0              1279             235   \n",
       "1               700             132   \n",
       "2              1758             327   \n",
       "3              1113             188   \n",
       "4              1313             234   \n",
       "...             ...             ...   \n",
       "34995           596             113   \n",
       "34996           646             110   \n",
       "34997          2147             365   \n",
       "34998          4848             788   \n",
       "34999           586             110   \n",
       "\n",
       "                                               processed  \n",
       "0      draw upon apply ethic idea movie human suffer ...  \n",
       "1      say good say muchthe plot someone buy game mak...  \n",
       "2      movie waste film stock believe map plan milita...  \n",
       "3      clark gable play con man bust life hardboiled ...  \n",
       "4      normally like romantic film love film much sto...  \n",
       "...                                                  ...  \n",
       "34995  idea name people good murderer want daughter e...  \n",
       "34996  outsider undoubtedly classic australian tv ser...  \n",
       "34997  admittedly tune hop see beefcake shot jam brol...  \n",
       "34998  late short film series entitle flicker flashba...  \n",
       "34999  agree user comment two main role well act guy ...  \n",
       "\n",
       "[35000 rows x 6 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "train_set_processed, cnt1 = preprocessor.preprocess_imdb_reviews(train_set, 0.85, 0.0005)\n",
    "test_set_processed, cnt2 = preprocessor.preprocess_imdb_reviews(test_set, 0.85, 0.0005)\n",
    "valid_set_processed, cnt3 = preprocessor.preprocess_imdb_reviews(valid_set, 0.85, 0.0005)\n",
    "\n",
    "train_set_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 Checking preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "# Most common words\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "cnt2 = Counter()\n",
    "for text in train_set_processed[\"processed\"].values:\n",
    "    # Counting the words\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "    # Counting in how many reviews the word appears\n",
    "    for word in set(text.split()):\n",
    "        cnt2[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 20 counted by appearance in nr of reviews:  [('movie', 22411), ('film', 20510), ('one', 19663), ('make', 17703), ('like', 16998), ('see', 16925), ('good', 16352), ('get', 14864), ('time', 13671), ('would', 13272), ('watch', 12599), ('go', 12362), ('even', 11706), ('think', 11388), ('character', 11383), ('story', 10745), ('really', 10538), ('bad', 10468), ('much', 9697), ('say', 9418), ('look', 9410), ('well', 9400), ('could', 9380), ('great', 9321), ('know', 9248), ('act', 9211), ('give', 9140), ('scene', 9043), ('end', 9029), ('also', 8730), ('take', 8725), ('way', 8668), ('come', 8666), ('people', 8580), ('first', 8553), ('thing', 8280), ('show', 8202), ('find', 8153), ('play', 7844), ('love', 7718), ('seem', 7207), ('want', 7086), ('many', 7018), ('plot', 7003), ('work', 6965), ('never', 6943), ('actor', 6839), ('try', 6680), ('two', 6660), ('best', 6658), ('little', 6628), ('year', 6609), ('ever', 6566), ('still', 6063), ('life', 5941), ('something', 5579), ('feel', 5481), ('part', 5396), ('interest', 5322), ('use', 5316), ('lot', 5304), ('man', 5277), ('back', 5272), ('old', 4977), ('real', 4960), ('director', 4948), ('cast', 4893), ('another', 4818), ('performance', 4783), ('leave', 4778), ('though', 4736), ('actually', 4717), ('nothing', 4676), ('funny', 4585), ('start', 4578), ('big', 4571), ('tell', 4554), ('every', 4546), ('write', 4515), ('point', 4454), ('long', 4423), ('star', 4403), ('new', 4393), ('turn', 4373), ('live', 4336), ('become', 4261), ('fact', 4228), ('role', 4224), ('young', 4196), ('set', 4195), ('guy', 4191), ('quite', 4133), ('woman', 4132), ('u', 4097), ('around', 4092), ('day', 4056), ('pretty', 4020), ('minute', 4008), ('happen', 3937), ('enough', 3919), ('mean', 3911), ('right', 3882), ('world', 3856), ('late', 3840), ('need', 3834), ('without', 3822), ('however', 3818), ('saw', 3814), ('keep', 3811), ('must', 3768), ('bite', 3759), ('put', 3759), ('enjoy', 3749), ('least', 3725), ('may', 3704), ('line', 3690), ('script', 3677), ('always', 3658), ('fan', 3653), ('last', 3645), ('almost', 3635), ('begin', 3626), ('girl', 3620), ('whole', 3617), ('believe', 3611), ('anything', 3532), ('lead', 3485), ('friend', 3475), ('far', 3441), ('kill', 3420), ('kind', 3406), ('reason', 3397), ('might', 3394), ('place', 3387), ('since', 3354), ('comedy', 3329), ('probably', 3327), ('action', 3302), ('away', 3243), ('music', 3237), ('anyone', 3213), ('shoot', 3212), ('let', 3201), ('sure', 3194), ('call', 3189), ('laugh', 3183), ('original', 3141), ('run', 3137), ('effect', 3130), ('hard', 3128), ('help', 3116), ('yet', 3103), ('rather', 3094), ('name', 3075), ('especially', 3029), ('moment', 3021), ('read', 2995), ('family', 2973), ('course', 2969), ('expect', 2938), ('worth', 2931), ('idea', 2919), ('screen', 2909), ('recommend', 2886), ('tv', 2880), ('fun', 2868), ('bring', 2853), ('horror', 2853), ('although', 2848), ('audience', 2836), ('bore', 2829), ('kid', 2806), ('everything', 2795), ('move', 2790), ('high', 2772), ('job', 2770), ('sense', 2769), ('someone', 2752), ('main', 2678), ('different', 2675), ('maybe', 2661), ('true', 2655), ('everyone', 2651), ('direct', 2651), ('dvd', 2644), ('together', 2617), ('early', 2605), ('waste', 2604), ('mind', 2586), ('money', 2584), ('lose', 2583), ('understand', 2576), ('second', 2569), ('hear', 2568), ('instead', 2560), ('problem', 2559), ('fall', 2549), ('follow', 2527), ('series', 2518), ('sound', 2500), ('child', 2500), ('else', 2489), ('miss', 2484), ('view', 2468), ('beautiful', 2464), ('special', 2451), ('surprise', 2434), ('le', 2433), ('face', 2432), ('excellent', 2423), ('night', 2416), ('three', 2416), ('american', 2404), ('hour', 2390), ('include', 2366), ('classic', 2360), ('talk', 2349), ('remember', 2335), ('piece', 2334), ('open', 2322), ('viewer', 2306), ('appear', 2303), ('simply', 2291), ('short', 2288), ('rest', 2283), ('completely', 2282), ('change', 2268), ('attempt', 2260), ('head', 2260), ('nice', 2251), ('entertain', 2250), ('along', 2240), ('suppose', 2239), ('involve', 2229), ('poor', 2222), ('eye', 2214), ('couple', 2213), ('add', 2193), ('death', 2184), ('word', 2182), ('care', 2177), ('either', 2175), ('lack', 2169), ('book', 2162), ('meet', 2139), ('production', 2133), ('die', 2129), ('decide', 2126), ('truly', 2125), ('hand', 2123), ('home', 2114), ('wrong', 2111), ('version', 2110), ('picture', 2109), ('release', 2107), ('boy', 2104), ('john', 2102), ('wonder', 2097), ('feature', 2093), ('close', 2088), ('full', 2086), ('next', 2083), ('small', 2067), ('save', 2067), ('half', 2050), ('wife', 2022), ('camera', 2021), ('hollywood', 2017), ('top', 1992), ('fight', 1988), ('sit', 1983), ('sort', 1973), ('black', 1969), ('title', 1969), ('definitely', 1965), ('low', 1957), ('case', 1951), ('wonderful', 1946), ('consider', 1945), ('create', 1940), ('perhaps', 1940), ('dialogue', 1939), ('base', 1936), ('others', 1933), ('guess', 1931), ('person', 1929), ('absolutely', 1924), ('house', 1910), ('human', 1897), ('stupid', 1892), ('review', 1883), ('awful', 1883), ('dead', 1882), ('men', 1876), ('fine', 1872), ('comment', 1865), ('often', 1863), ('father', 1861), ('mention', 1860), ('perfect', 1859), ('light', 1854), ('video', 1845), ('experience', 1840), ('example', 1832), ('stand', 1827), ('certainly', 1824), ('terrible', 1821), ('episode', 1818), ('flick', 1809), ('style', 1807), ('stop', 1799), ('budget', 1794), ('entire', 1790), ('rat', 1786), ('war', 1782), ('felt', 1781), ('quality', 1779), ('hope', 1776), ('direction', 1774), ('spend', 1765), ('amaze', 1764), ('force', 1762), ('ask', 1757), ('buy', 1755), ('school', 1754), ('fail', 1744), ('finally', 1737), ('several', 1733), ('disappoint', 1731), ('age', 1729), ('song', 1718), ('matter', 1717), ('deal', 1716), ('hold', 1716), ('break', 1716), ('actress', 1712), ('writer', 1707), ('sequence', 1699), ('cinema', 1684), ('already', 1678), ('learn', 1674), ('throughout', 1673), ('favorite', 1672), ('support', 1663), ('forget', 1663), ('dark', 1655), ('drama', 1654), ('sex', 1645), ('hit', 1644), ('wait', 1637), ('joke', 1635), ('side', 1630), ('yes', 1618), ('final', 1617), ('cut', 1616), ('able', 1613), ('totally', 1608), ('manage', 1603), ('murder', 1591), ('throw', 1584), ('wish', 1578), ('speak', 1577), ('mother', 1574), ('pay', 1565), ('touch', 1561), ('portray', 1560), ('stay', 1560), ('realize', 1555), ('walk', 1549), ('despite', 1547), ('deserve', 1535), ('type', 1533), ('art', 1533), ('unfortunately', 1532), ('brother', 1531), ('credit', 1529), ('rent', 1520), ('heart', 1517), ('oh', 1516), ('win', 1513), ('mr', 1505), ('past', 1494), ('twist', 1490), ('present', 1489), ('history', 1486), ('humor', 1482), ('relationship', 1481), ('score', 1480), ('strong', 1479), ('white', 1477), ('catch', 1472), ('complete', 1471), ('chance', 1468), ('behind', 1460), ('today', 1458), ('talent', 1456), ('overall', 1456), ('stuff', 1456), ('obviously', 1455), ('genre', 1447), ('evil', 1433), ('except', 1420), ('annoy', 1414), ('car', 1414), ('order', 1406), ('theme', 1405), ('highly', 1404), ('horrible', 1402), ('situation', 1396), ('number', 1388), ('question', 1387), ('sometimes', 1384), ('strange', 1383), ('soon', 1380), ('grow', 1379), ('son', 1376), ('please', 1374), ('decent', 1374), ('figure', 1372), ('level', 1370), ('group', 1369), ('brilliant', 1356), ('event', 1355), ('element', 1349), ('hero', 1344), ('voice', 1343), ('slow', 1337), ('extremely', 1325), ('killer', 1317), ('return', 1312), ('power', 1309), ('simple', 1309), ('pick', 1308), ('cinematography', 1305), ('body', 1304), ('obvious', 1301), ('particularly', 1292), ('cause', 1292), ('result', 1291), ('ago', 1289), ('edit', 1286), ('build', 1284), ('pace', 1283), ('opinion', 1283), ('effort', 1276), ('offer', 1276), ('none', 1264), ('etc', 1257), ('deliver', 1256), ('exactly', 1256), ('serious', 1253), ('career', 1252), ('sad', 1251), ('city', 1250), ('state', 1247), ('explain', 1244), ('value', 1242), ('across', 1239), ('daughter', 1235), ('town', 1234), ('okay', 1233), ('god', 1232), ('possible', 1232), ('usually', 1230), ('compare', 1230), ('convince', 1229), ('allow', 1224), ('drive', 1221), ('alone', 1218), ('check', 1211), ('huge', 1208), ('stick', 1205), ('somewhat', 1201), ('blood', 1200), ('cool', 1196), ('theater', 1194), ('michael', 1194), ('pull', 1193), ('hell', 1190), ('country', 1189), ('robert', 1189), ('hilarious', 1188), ('female', 1184), ('provide', 1183), ('ridiculous', 1181), ('mostly', 1180), ('easy', 1179), ('note', 1178), ('seriously', 1173), ('prove', 1172), ('usual', 1166), ('whose', 1166), ('attention', 1164), ('reality', 1161), ('focus', 1160), ('single', 1156), ('violence', 1156), ('beyond', 1151), ('game', 1148), ('pas', 1146), ('avoid', 1146), ('anyway', 1145), ('due', 1141), ('draw', 1140), ('imagine', 1139), ('important', 1138), ('cheap', 1136), ('silly', 1135), ('basically', 1132), ('happy', 1131), ('arent', 1129), ('dialog', 1127), ('hop', 1127), ('shock', 1125), ('filmmaker', 1124), ('major', 1124), ('produce', 1119), ('crap', 1118), ('local', 1113), ('form', 1107), ('apparently', 1106), ('doubt', 1105), ('lady', 1104), ('charm', 1100), ('subject', 1098), ('room', 1095), ('clearly', 1094), ('shot', 1093), ('dream', 1092), ('dance', 1092), ('jam', 1091), ('cover', 1088), ('scary', 1087), ('remind', 1085), ('easily', 1085), ('steal', 1084), ('upon', 1081), ('detail', 1081), ('husband', 1081), ('remain', 1079), ('deep', 1076), ('sister', 1075), ('clear', 1073), ('fit', 1073), ('fill', 1073), ('thank', 1072), ('near', 1070), ('plan', 1067), ('discover', 1066), ('police', 1063), ('aspect', 1058), ('whether', 1057), ('similar', 1056), ('weak', 1056), ('agree', 1047), ('producer', 1044), ('member', 1044), ('carry', 1043), ('entertainment', 1042), ('capture', 1034), ('develop', 1034), ('within', 1031), ('enjoyable', 1029), ('english', 1028), ('middle', 1025), ('message', 1022), ('marry', 1020), ('david', 1019), ('nearly', 1017), ('predictable', 1017), ('modern', 1016), ('novel', 1016), ('certain', 1015), ('sing', 1014), ('tale', 1013), ('choose', 1010), ('escape', 1006), ('confuse', 1005), ('storyline', 1002), ('ten', 1002), ('soundtrack', 1001), ('bunch', 999), ('among', 997), ('date', 995), ('image', 995), ('future', 993), ('wear', 992), ('gore', 991), ('finish', 988), ('send', 985), ('fast', 978), ('parent', 978), ('hate', 975), ('sequel', 975), ('street', 975), ('five', 974), ('musical', 973), ('class', 973), ('standard', 971), ('four', 971), ('continue', 963), ('describe', 962), ('typical', 961), ('television', 961), ('dull', 956), ('romantic', 955), ('comic', 954), ('air', 954), ('mark', 953), ('sorry', 950), ('actual', 947), ('material', 946), ('contain', 945), ('emotion', 938), ('somehow', 937), ('thriller', 936), ('mess', 935), ('fantastic', 933), ('excite', 933), ('admit', 932), ('premise', 931), ('adult', 931), ('particular', 930), ('straight', 929), ('general', 928), ('rock', 928), ('famous', 927), ('suffer', 925), ('period', 924), ('appreciate', 919), ('mystery', 917), ('monster', 913), ('trouble', 912), ('documentary', 910), ('hide', 907), ('notice', 906), ('background', 906), ('cry', 904), ('oscar', 903), ('cop', 902), ('team', 901), ('treat', 900), ('copy', 898), ('possibly', 898), ('gun', 896), ('mix', 893), ('list', 892), ('realistic', 891), ('crime', 891), ('appeal', 890), ('bother', 890), ('reveal', 889), ('large', 889), ('fire', 888), ('spoiler', 888), ('bear', 885), ('eventually', 883), ('difficult', 883), ('believable', 881), ('lame', 878), ('earth', 877), ('masterpiece', 876), ('victim', 873), ('whatever', 873), ('villain', 871), ('blow', 870), ('beat', 867), ('issue', 864), ('george', 863), ('romance', 860), ('choice', 859), ('otherwise', 858), ('mistake', 858), ('chase', 858), ('attack', 857), ('suggest', 857), ('british', 856), ('hat', 853), ('average', 852), ('poorly', 848), ('unless', 845), ('third', 845), ('truth', 844), ('atmosphere', 843), ('forward', 843), ('screenplay', 839), ('suck', 839), ('indeed', 836), ('memorable', 836), ('free', 834), ('emotional', 831), ('america', 831), ('box', 829), ('dog', 828), ('suspense', 826), ('peter', 825), ('battle', 824), ('superb', 822), ('warn', 820), ('student', 819), ('male', 819), ('struggle', 814), ('cheesy', 813), ('space', 812), ('total', 812), ('personal', 812), ('previous', 809), ('imdb', 807), ('towards', 806), ('fear', 804), ('quickly', 803), ('front', 802), ('development', 802), ('incredibly', 801), ('weird', 800), ('beauty', 800), ('b', 800), ('amount', 798), ('king', 797), ('train', 797), ('french', 797), ('perfectly', 796), ('richard', 796), ('paul', 796), ('nature', 795), ('respect', 794), ('costume', 794), ('unique', 794), ('shame', 793), ('hot', 787), ('york', 786), ('crazy', 786), ('flaw', 786), ('remake', 785), ('eat', 783), ('season', 782), ('rich', 781), ('drug', 780), ('society', 780), ('secret', 777), ('lie', 774), ('inside', 772), ('promise', 772), ('fly', 772), ('project', 770), ('concern', 769), ('amuse', 769), ('party', 768), ('dumb', 767), ('plain', 766), ('control', 764), ('sexual', 764), ('scream', 762), ('plus', 762), ('girlfriend', 762), ('water', 762), ('tear', 761), ('week', 759), ('recently', 758), ('various', 758), ('badly', 757), ('accent', 757), ('term', 757), ('drink', 756), ('apart', 754), ('hardly', 753), ('ok', 753), ('jump', 753), ('fairly', 752), ('powerful', 751), ('lover', 751), ('dramatic', 750), ('award', 749), ('plenty', 747), ('location', 744), ('pure', 742), ('business', 742), ('jack', 740), ('bill', 739), ('serve', 737), ('portrayal', 737), ('success', 736), ('color', 736), ('track', 734), ('studio', 733), ('share', 732), ('roll', 731), ('adventure', 726), ('talented', 725), ('exist', 722), ('listen', 720), ('potential', 720), ('stage', 717), ('baby', 717), ('slightly', 715), ('flat', 715), ('western', 714), ('spirit', 713), ('unlike', 713), ('disturb', 713), ('cute', 712), ('outside', 712), ('store', 711), ('design', 708), ('land', 708), ('company', 707), ('concept', 706), ('answer', 706), ('appearance', 705), ('creepy', 704), ('red', 700), ('introduce', 699), ('tom', 698), ('perform', 696), ('clever', 695), ('rise', 695), ('dress', 694), ('animation', 691), ('reach', 689), ('fantasy', 689), ('sleep', 689), ('doctor', 689), ('footage', 685), ('crew', 685), ('memory', 685), ('ability', 684), ('taste', 684), ('claim', 683), ('odd', 683), ('depth', 682), ('destroy', 681), ('intrigue', 681), ('sweet', 681), ('neither', 680), ('entirely', 679), ('public', 678), ('accept', 678), ('incredible', 677), ('scifi', 676), ('kick', 676), ('ruin', 675), ('spot', 674), ('era', 674), ('record', 674), ('channel', 671), ('la', 670), ('popular', 670), ('tension', 668), ('nudity', 668), ('brain', 668), ('sadly', 668), ('language', 666), ('familiar', 666), ('visual', 666), ('positive', 665), ('inspire', 665), ('rate', 664), ('sell', 664), ('purpose', 662), ('receive', 662), ('master', 662), ('approach', 661), ('scar', 661), ('intelligent', 659), ('suddenly', 659), ('william', 655), ('political', 651), ('engage', 651), ('impress', 650), ('basic', 650), ('hang', 648), ('player', 648), ('recent', 647), ('successful', 647), ('drag', 647), ('maker', 645), ('de', 644), ('animal', 643), ('tone', 642), ('stun', 642), ('dr', 642), ('search', 642), ('common', 640), ('handle', 640), ('century', 640), ('extra', 638), ('step', 637), ('strike', 634), ('fake', 634), ('soldier', 632), ('ultimately', 632), ('intend', 632), ('wood', 631), ('barely', 630), ('excuse', 629), ('former', 629), ('trip', 629), ('relate', 628), ('solid', 628), ('social', 625), ('conclusion', 625), ('million', 623), ('hole', 623), ('hair', 619), ('soul', 617), ('somewhere', 617), ('door', 617), ('impossible', 616), ('reviewer', 616), ('trash', 614), ('mood', 614), ('travel', 612), ('suit', 612), ('lee', 611), ('difference', 610), ('fascinate', 610), ('raise', 609), ('cold', 608), ('fair', 607), ('match', 606), ('sick', 605), ('hurt', 605), ('pop', 603), ('literally', 603), ('suspect', 602), ('limit', 601), ('glad', 601), ('genius', 601), ('pointless', 601), ('sign', 600), ('honest', 600), ('impressive', 599), ('pathetic', 598), ('burn', 597), ('violent', 597), ('culture', 596), ('wind', 596), ('repeat', 596), ('clothe', 596), ('tire', 595), ('display', 594), ('compel', 594), ('effective', 593), ('visit', 593), ('cartoon', 593), ('immediately', 593), ('count', 592), ('seek', 592), ('drop', 591), ('teen', 591), ('dont', 590), ('office', 588), ('awesome', 588), ('critic', 587), ('survive', 585), ('redeem', 585), ('opportunity', 584), ('chemistry', 584), ('personality', 583), ('exception', 583), ('surprisingly', 581), ('ring', 580), ('bizarre', 579), ('cost', 578), ('haunt', 577), ('nobody', 575), ('alive', 575), ('wild', 575), ('aside', 574), ('blue', 573), ('paint', 572), ('adaptation', 571), ('expectation', 570), ('study', 569), ('personally', 568), ('beautifully', 568), ('impression', 568), ('subtle', 568), ('smile', 568), ('park', 567), ('normal', 567), ('desire', 566), ('band', 566), ('folk', 565), ('likely', 565), ('fashion', 565), ('cat', 564), ('cinematic', 564), ('edge', 564), ('rare', 563), ('utterly', 563), ('bar', 563), ('honestly', 561), ('regard', 560), ('smart', 560), ('yeah', 560), ('creature', 558), ('ride', 558), ('gem', 558), ('sexy', 558)]\n",
      "\n",
      "Most common 20 counted by word count total:  [('movie', 69535), ('film', 64702), ('one', 36942), ('make', 30977), ('like', 30232), ('see', 28413), ('good', 27829), ('get', 24913), ('would', 21692), ('time', 20819)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Most common 20 counted by appearance in nr of reviews: \", cnt2.most_common(1000))\n",
    "print(\"\\nMost common 20 counted by word count total: \", cnt.most_common(10))\n",
    "\n",
    "#print(\"Most common 50 counted by appearance in nr of reviews: \", cnt1.most_common(50))\n",
    "\n",
    "vocab_size = len(cnt2)\n",
    "sample_size = len(train_set)\n",
    "\n",
    "x = [c/sample_size * 100 for (w, c) in cnt2.most_common()]\n",
    "y = [c/vocab_size * 100 for c in range(1, vocab_size+1)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_title(\"% of the words used in % of the documents\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"% of vocabolary\")\n",
    "ax.set_ylabel(\"% of documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>label</th>\n",
       "      <th>rev_lens_raw</th>\n",
       "      <th>rev_lens_words</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11423</th>\n",
       "      <td>The funniest show ever on TV, albeit the humor...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3791</td>\n",
       "      <td>661</td>\n",
       "      <td>funny show ever tv albeit humor everyone reali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>Every now and again you hear radio djs invitin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1479</td>\n",
       "      <td>251</td>\n",
       "      <td>every hear radio dj invite listener nominate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7342</th>\n",
       "      <td>I love the music of the Clash and I love the m...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1642</td>\n",
       "      <td>302</td>\n",
       "      <td>love music clash love music joe go movie hop l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>\"The Planet\" is an astounding piece of film ma...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1842</td>\n",
       "      <td>322</td>\n",
       "      <td>planet astound piece film make mere £ producti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20923</th>\n",
       "      <td>I saw this movie in 1959 when I was 11 years o...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1196</td>\n",
       "      <td>215</td>\n",
       "      <td>saw movie year old drivein theater back think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34315</th>\n",
       "      <td>I would not consider myself as one of Leonard ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>344</td>\n",
       "      <td>would consider one leonard cohen great fan how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21382</th>\n",
       "      <td>I knew full well when I rented this DVD that i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1037</td>\n",
       "      <td>185</td>\n",
       "      <td>know full well rent dvd could well one bad mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32484</th>\n",
       "      <td>I had heard this film was a study of a landsca...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2891</td>\n",
       "      <td>484</td>\n",
       "      <td>hear film study landscape photographer art pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>I would give this a zero if they had that rati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>789</td>\n",
       "      <td>153</td>\n",
       "      <td>would give zero rat fun fun grow tire movie te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25122</th>\n",
       "      <td>A patchwork about 911. The 11 stories from 11 ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>160</td>\n",
       "      <td>story director country sometimes sometimes bor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rate  label  \\\n",
       "11423  The funniest show ever on TV, albeit the humor...    10      1   \n",
       "2927   Every now and again you hear radio djs invitin...     1      0   \n",
       "7342   I love the music of the Clash and I love the m...     3      0   \n",
       "24544  \"The Planet\" is an astounding piece of film ma...     7      1   \n",
       "20923  I saw this movie in 1959 when I was 11 years o...    10      1   \n",
       "34315  I would not consider myself as one of Leonard ...     2      0   \n",
       "21382  I knew full well when I rented this DVD that i...     1      0   \n",
       "32484  I had heard this film was a study of a landsca...    10      1   \n",
       "10381  I would give this a zero if they had that rati...     1      0   \n",
       "25122  A patchwork about 911. The 11 stories from 11 ...     9      1   \n",
       "\n",
       "       rev_lens_raw  rev_lens_words  \\\n",
       "11423          3791             661   \n",
       "2927           1479             251   \n",
       "7342           1642             302   \n",
       "24544          1842             322   \n",
       "20923          1196             215   \n",
       "34315          1990             344   \n",
       "21382          1037             185   \n",
       "32484          2891             484   \n",
       "10381           789             153   \n",
       "25122          1008             160   \n",
       "\n",
       "                                               processed  \n",
       "11423  funny show ever tv albeit humor everyone reali...  \n",
       "2927   every hear radio dj invite listener nominate m...  \n",
       "7342   love music clash love music joe go movie hop l...  \n",
       "24544  planet astound piece film make mere £ producti...  \n",
       "20923  saw movie year old drivein theater back think ...  \n",
       "34315  would consider one leonard cohen great fan how...  \n",
       "21382  know full well rent dvd could well one bad mov...  \n",
       "32484  hear film study landscape photographer art pre...  \n",
       "10381  would give zero rat fun fun grow tire movie te...  \n",
       "25122  story director country sometimes sometimes bor...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_processed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 Vectorizing data sets using TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IDF values and VOCAB vector\n",
    "vectorizer = TfIdfVectorizer(train_set_processed)\n",
    "idfs = vectorizer.idfs\n",
    "vocab = vectorizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing train set and test set\n",
    "X_train = vectorizer.tf_idf_vectorize_all(train_set_processed)\n",
    "\n",
    "X_test = vectorizer.tf_idf_vectorize_all(test_set_processed)\n",
    "\n",
    "X_valid = vectorizer.tf_idf_vectorize_all(valid_set_processed)\n",
    "\n",
    "Y_train = train_set_processed['label'].to_numpy()\n",
    "\n",
    "Y_test = test_set_processed['label'].to_numpy()\n",
    "\n",
    "Y_valid = valid_set_processed['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "12050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vomit': 0,\n",
       " 'hippie': 1,\n",
       " 'min': 2,\n",
       " 'skateboard': 3,\n",
       " 'anita': 4,\n",
       " 'consent': 5,\n",
       " 'biological': 6,\n",
       " 'fake': 7,\n",
       " 'gielgud': 8,\n",
       " 'booth': 9,\n",
       " 'spacey': 10,\n",
       " 'versatile': 11,\n",
       " 'assistance': 12,\n",
       " 'heart': 13,\n",
       " 'subtly': 14,\n",
       " 'illustrate': 15,\n",
       " 'dir': 16,\n",
       " 'journey': 17,\n",
       " 'infinitely': 18,\n",
       " 'himthe': 19,\n",
       " 'newman': 20,\n",
       " 'pause': 21,\n",
       " 'custer': 22,\n",
       " 'establishment': 23,\n",
       " 'yacht': 24,\n",
       " 'inherit': 25,\n",
       " 'doom': 26,\n",
       " 'lowbrow': 27,\n",
       " 'reservoir': 28,\n",
       " 'fonda': 29,\n",
       " 'loosen': 30,\n",
       " 'suspension': 31,\n",
       " 'korean': 32,\n",
       " 'previous': 33,\n",
       " 'great': 34,\n",
       " 'thug': 35,\n",
       " 'chunk': 36,\n",
       " 'intestine': 37,\n",
       " 'evade': 38,\n",
       " 'snake': 39,\n",
       " 'burr': 40,\n",
       " 'quite': 41,\n",
       " 'poke': 42,\n",
       " 'undemanding': 43,\n",
       " 'template': 44,\n",
       " 'since': 45,\n",
       " 'plea': 46,\n",
       " 'attitude': 47,\n",
       " 'radha': 48,\n",
       " 'stomach': 49,\n",
       " 'slop': 50,\n",
       " 'maniac': 51,\n",
       " 'due': 52,\n",
       " 'detroit': 53,\n",
       " 'ogle': 54,\n",
       " 'margin': 55,\n",
       " 'ambitious': 56,\n",
       " 'impale': 57,\n",
       " 'town': 58,\n",
       " 'imo': 59,\n",
       " 'stereotypical': 60,\n",
       " 'magic': 61,\n",
       " 'moviegoing': 62,\n",
       " 'boost': 63,\n",
       " 'syrupy': 64,\n",
       " 'barrymore': 65,\n",
       " 'vcr': 66,\n",
       " 'briefly': 67,\n",
       " 'ad': 68,\n",
       " 'billy': 69,\n",
       " 'shea': 70,\n",
       " 'sombre': 71,\n",
       " 'topless': 72,\n",
       " 'diverse': 73,\n",
       " 'collette': 74,\n",
       " 'crack': 75,\n",
       " 'infamous': 76,\n",
       " 'opus': 77,\n",
       " 'kansa': 78,\n",
       " 'asshole': 79,\n",
       " 'inflict': 80,\n",
       " 'rom': 81,\n",
       " 'fanatical': 82,\n",
       " 'disgust': 83,\n",
       " 'crumble': 84,\n",
       " 'inch': 85,\n",
       " 'pacific': 86,\n",
       " 'maudlin': 87,\n",
       " 'hairstyle': 88,\n",
       " 'device': 89,\n",
       " 'soprano': 90,\n",
       " 'exception': 91,\n",
       " 'obligate': 92,\n",
       " 'wolf': 93,\n",
       " 'nonstop': 94,\n",
       " 'eighty': 95,\n",
       " 'unexpected': 96,\n",
       " 'perpetual': 97,\n",
       " 'politically': 98,\n",
       " 'spread': 99,\n",
       " 'book': 100,\n",
       " 'jones': 101,\n",
       " 'undeveloped': 102,\n",
       " 'mold': 103,\n",
       " 'mostly': 104,\n",
       " 'worldthe': 105,\n",
       " 'eventually': 106,\n",
       " 'horribly': 107,\n",
       " 'tho': 108,\n",
       " 'whale': 109,\n",
       " 'criminal': 110,\n",
       " 'wideeyed': 111,\n",
       " 'unfamiliar': 112,\n",
       " 'elton': 113,\n",
       " 'champ': 114,\n",
       " 'resemble': 115,\n",
       " 'bobby': 116,\n",
       " 'treat': 117,\n",
       " 'pond': 118,\n",
       " 'los': 119,\n",
       " 'mythic': 120,\n",
       " 'project': 121,\n",
       " 'blackmail': 122,\n",
       " 'illustrious': 123,\n",
       " 'hogan': 124,\n",
       " 'whip': 125,\n",
       " 'ho': 126,\n",
       " 'deceit': 127,\n",
       " 'reputation': 128,\n",
       " 'christensen': 129,\n",
       " 'lucid': 130,\n",
       " 'reuse': 131,\n",
       " 'colony': 132,\n",
       " 'groovy': 133,\n",
       " 'heaven': 134,\n",
       " 'operate': 135,\n",
       " 'tot': 136,\n",
       " 'housewife': 137,\n",
       " 'reduce': 138,\n",
       " 'itbut': 139,\n",
       " 'calibre': 140,\n",
       " 'prompt': 141,\n",
       " 'twentysomething': 142,\n",
       " 'detail': 143,\n",
       " 'hobgoblin': 144,\n",
       " 'versatility': 145,\n",
       " 'gorilla': 146,\n",
       " 'spout': 147,\n",
       " 'examine': 148,\n",
       " 'shirt': 149,\n",
       " 'massive': 150,\n",
       " 'foreign': 151,\n",
       " 'sparkle': 152,\n",
       " 'pino': 153,\n",
       " 'kickass': 154,\n",
       " 'decrepit': 155,\n",
       " 'claus': 156,\n",
       " 'improve': 157,\n",
       " 'behindthescenes': 158,\n",
       " 'difference': 159,\n",
       " 'eddy': 160,\n",
       " 'glass': 161,\n",
       " 'forgiveness': 162,\n",
       " 'hopefully': 163,\n",
       " 'droll': 164,\n",
       " 'hawke': 165,\n",
       " 'goofy': 166,\n",
       " 'novel': 167,\n",
       " 'competent': 168,\n",
       " 'fiend': 169,\n",
       " 'competitor': 170,\n",
       " 'infant': 171,\n",
       " 'skin': 172,\n",
       " 'cushing': 173,\n",
       " 'accomplish': 174,\n",
       " 'poppins': 175,\n",
       " 'olivier': 176,\n",
       " 'end': 177,\n",
       " 'sideline': 178,\n",
       " 'sadden': 179,\n",
       " 'freakish': 180,\n",
       " 'vu': 181,\n",
       " 'solve': 182,\n",
       " 'complex': 183,\n",
       " 'boorish': 184,\n",
       " 'ransom': 185,\n",
       " 'sleepy': 186,\n",
       " 'tourdeforce': 187,\n",
       " 'impressively': 188,\n",
       " 'chad': 189,\n",
       " 'perish': 190,\n",
       " '½': 191,\n",
       " 'concert': 192,\n",
       " 'suitably': 193,\n",
       " 'impersonator': 194,\n",
       " 'consult': 195,\n",
       " 'sondra': 196,\n",
       " 'enforcement': 197,\n",
       " 'picnic': 198,\n",
       " 'archetype': 199,\n",
       " 'repetitive': 200,\n",
       " 'humility': 201,\n",
       " 'pancake': 202,\n",
       " 'ceiling': 203,\n",
       " 'surreal': 204,\n",
       " 'loretta': 205,\n",
       " 'ralph': 206,\n",
       " 'rejoice': 207,\n",
       " 'astaire': 208,\n",
       " 'visit': 209,\n",
       " 'pesky': 210,\n",
       " 'edgar': 211,\n",
       " 'seize': 212,\n",
       " 'payne': 213,\n",
       " 'frederic': 214,\n",
       " 'lavish': 215,\n",
       " 'grinch': 216,\n",
       " 'actress': 217,\n",
       " 'rea': 218,\n",
       " 'farther': 219,\n",
       " 'disappointment': 220,\n",
       " 'hairdo': 221,\n",
       " 'crane': 222,\n",
       " 'pessimistic': 223,\n",
       " 'advertise': 224,\n",
       " 'informative': 225,\n",
       " 'irresponsible': 226,\n",
       " 'tyler': 227,\n",
       " 'implausibility': 228,\n",
       " 'bernsen': 229,\n",
       " 'grasshopper': 230,\n",
       " 'upgrade': 231,\n",
       " 'fathom': 232,\n",
       " 'scooby': 233,\n",
       " 'rightfully': 234,\n",
       " 'jury': 235,\n",
       " 'longtime': 236,\n",
       " 'setback': 237,\n",
       " 'slit': 238,\n",
       " 'breathtaking': 239,\n",
       " 'lash': 240,\n",
       " 'rightly': 241,\n",
       " 'libido': 242,\n",
       " 'disc': 243,\n",
       " 'lowe': 244,\n",
       " 'englund': 245,\n",
       " 'friction': 246,\n",
       " 'fraternity': 247,\n",
       " 'gesture': 248,\n",
       " 'technically': 249,\n",
       " 'stevenson': 250,\n",
       " 'west': 251,\n",
       " 'eccleston': 252,\n",
       " 'digitally': 253,\n",
       " 'ruin': 254,\n",
       " 'big': 255,\n",
       " 'archer': 256,\n",
       " 'ang': 257,\n",
       " 'foe': 258,\n",
       " 'etcthe': 259,\n",
       " 'poetic': 260,\n",
       " 'increase': 261,\n",
       " 'bound': 262,\n",
       " 'omen': 263,\n",
       " 'uneasy': 264,\n",
       " 'minor': 265,\n",
       " 'stepsister': 266,\n",
       " 'todd': 267,\n",
       " 'importantly': 268,\n",
       " 'fearful': 269,\n",
       " 'courage': 270,\n",
       " 'cliché': 271,\n",
       " 'hammerhead': 272,\n",
       " 'scriptwriter': 273,\n",
       " 'politely': 274,\n",
       " 'dramatize': 275,\n",
       " 'honestly': 276,\n",
       " 'motivate': 277,\n",
       " 'replete': 278,\n",
       " 'literary': 279,\n",
       " 'tribulation': 280,\n",
       " 'secure': 281,\n",
       " 'repression': 282,\n",
       " 'descent': 283,\n",
       " 'controversy': 284,\n",
       " 'swat': 285,\n",
       " 'fish': 286,\n",
       " 'lunatic': 287,\n",
       " 'sorority': 288,\n",
       " 'selection': 289,\n",
       " 'rhetoric': 290,\n",
       " 'behalf': 291,\n",
       " 'belgium': 292,\n",
       " 'hai': 293,\n",
       " 'forensic': 294,\n",
       " 'recorder': 295,\n",
       " 'solely': 296,\n",
       " 'bitch': 297,\n",
       " 'absurdity': 298,\n",
       " 'cheer': 299,\n",
       " 'mystical': 300,\n",
       " 'compassion': 301,\n",
       " 'fleet': 302,\n",
       " 'marshall': 303,\n",
       " 'figure': 304,\n",
       " 'montage': 305,\n",
       " 'easily': 306,\n",
       " 'flesh': 307,\n",
       " 'champagne': 308,\n",
       " 'choreography': 309,\n",
       " 'secondary': 310,\n",
       " 'medical': 311,\n",
       " 'unfulfilled': 312,\n",
       " 'toddler': 313,\n",
       " 'askey': 314,\n",
       " 'outlet': 315,\n",
       " 'downtown': 316,\n",
       " 'youthe': 317,\n",
       " 'march': 318,\n",
       " 'skeleton': 319,\n",
       " 'confess': 320,\n",
       " 'arrangement': 321,\n",
       " 'sam': 322,\n",
       " 'kelly': 323,\n",
       " 'feelgood': 324,\n",
       " 'standouts': 325,\n",
       " 'smuggle': 326,\n",
       " 'dawson': 327,\n",
       " 'attorney': 328,\n",
       " 'unhealthy': 329,\n",
       " 'fundamental': 330,\n",
       " 'culkin': 331,\n",
       " 'bonnie': 332,\n",
       " 'heel': 333,\n",
       " 'month': 334,\n",
       " 'nowhere': 335,\n",
       " 'stall': 336,\n",
       " 'special': 337,\n",
       " 'hate': 338,\n",
       " 'cringeworthy': 339,\n",
       " 'peg': 340,\n",
       " 'slat': 341,\n",
       " 'paint': 342,\n",
       " 'helm': 343,\n",
       " 'enamor': 344,\n",
       " 'goodi': 345,\n",
       " 'happily': 346,\n",
       " 'feminism': 347,\n",
       " 'patterson': 348,\n",
       " 'scrawny': 349,\n",
       " 'stun': 350,\n",
       " 'maggot': 351,\n",
       " 'hbo': 352,\n",
       " 'soderbergh': 353,\n",
       " 'hellraiser': 354,\n",
       " 'super': 355,\n",
       " 'gossett': 356,\n",
       " 'proposal': 357,\n",
       " 'crapfest': 358,\n",
       " 'natasha': 359,\n",
       " 'lex': 360,\n",
       " 'bikers': 361,\n",
       " 'carbon': 362,\n",
       " 'boringly': 363,\n",
       " 'lens': 364,\n",
       " 'overshadow': 365,\n",
       " 'dandy': 366,\n",
       " 'maverick': 367,\n",
       " 'majesty': 368,\n",
       " 'ohara': 369,\n",
       " 'command': 370,\n",
       " 'carroll': 371,\n",
       " 'jude': 372,\n",
       " 'hybrid': 373,\n",
       " 'star': 374,\n",
       " 'patrick': 375,\n",
       " 'quo': 376,\n",
       " 'latter': 377,\n",
       " 'emergence': 378,\n",
       " 'martino': 379,\n",
       " 'cover': 380,\n",
       " 'underwhelming': 381,\n",
       " 'ryan': 382,\n",
       " 'bythenumbers': 383,\n",
       " 'coarse': 384,\n",
       " 'amiable': 385,\n",
       " 'classy': 386,\n",
       " 'wrench': 387,\n",
       " 'acid': 388,\n",
       " 'connery': 389,\n",
       " 'pope': 390,\n",
       " 'susannah': 391,\n",
       " 'privy': 392,\n",
       " 'bradford': 393,\n",
       " 'seemingly': 394,\n",
       " 'lillian': 395,\n",
       " 'makeshift': 396,\n",
       " 'article': 397,\n",
       " 'franchise': 398,\n",
       " 'sweaty': 399,\n",
       " 'ramon': 400,\n",
       " 'overpower': 401,\n",
       " 'roeg': 402,\n",
       " 'package': 403,\n",
       " 'inherent': 404,\n",
       " 'oscarnominated': 405,\n",
       " 'screenthe': 406,\n",
       " 'song': 407,\n",
       " 'visionary': 408,\n",
       " 'flood': 409,\n",
       " 'flashy': 410,\n",
       " 'issue': 411,\n",
       " 'reaction': 412,\n",
       " 'bodily': 413,\n",
       " 'sensitively': 414,\n",
       " 'considerable': 415,\n",
       " 'smarmy': 416,\n",
       " 'artfully': 417,\n",
       " 'walmart': 418,\n",
       " 'valerie': 419,\n",
       " 'second': 420,\n",
       " 'paresh': 421,\n",
       " 'mug': 422,\n",
       " 'troop': 423,\n",
       " 'doris': 424,\n",
       " 'earl': 425,\n",
       " 'success': 426,\n",
       " 'convent': 427,\n",
       " 'chuck': 428,\n",
       " 'hacker': 429,\n",
       " 'disintegrate': 430,\n",
       " 'te': 431,\n",
       " 'seriousness': 432,\n",
       " 'concentrate': 433,\n",
       " 'groan': 434,\n",
       " 'pleasure': 435,\n",
       " 'devos': 436,\n",
       " 'shag': 437,\n",
       " 'culminate': 438,\n",
       " 'deft': 439,\n",
       " 'outthe': 440,\n",
       " 'belt': 441,\n",
       " 'kingsley': 442,\n",
       " 'chow': 443,\n",
       " 'rerun': 444,\n",
       " 'anthem': 445,\n",
       " 'antique': 446,\n",
       " 'fritz': 447,\n",
       " 'china': 448,\n",
       " 'se': 449,\n",
       " 'alexis': 450,\n",
       " 'drip': 451,\n",
       " 'muni': 452,\n",
       " 'mario': 453,\n",
       " 'afterwards': 454,\n",
       " 'knight': 455,\n",
       " 'squander': 456,\n",
       " 'sierra': 457,\n",
       " 'renew': 458,\n",
       " 'wwf': 459,\n",
       " 'femme': 460,\n",
       " 'mistreat': 461,\n",
       " 'fanciful': 462,\n",
       " 'pm': 463,\n",
       " 'dub': 464,\n",
       " 'tit': 465,\n",
       " 'spray': 466,\n",
       " 'casey': 467,\n",
       " 'nefarious': 468,\n",
       " 'inside': 469,\n",
       " 'generic': 470,\n",
       " 'paulie': 471,\n",
       " 'independence': 472,\n",
       " 'curtiz': 473,\n",
       " 'surprisingly': 474,\n",
       " 'existent': 475,\n",
       " 'pas': 476,\n",
       " 'cannibalism': 477,\n",
       " 'exactly': 478,\n",
       " 'etc': 479,\n",
       " 'lugosi': 480,\n",
       " 'wildly': 481,\n",
       " 'laundry': 482,\n",
       " 'hamill': 483,\n",
       " 'unthinkable': 484,\n",
       " 'filma': 485,\n",
       " 'fictionalize': 486,\n",
       " 'tara': 487,\n",
       " 'qa': 488,\n",
       " 'avid': 489,\n",
       " 'filmmy': 490,\n",
       " 'mcclure': 491,\n",
       " 'empathise': 492,\n",
       " 'chick': 493,\n",
       " 'ninth': 494,\n",
       " 'indulgent': 495,\n",
       " 'stockwell': 496,\n",
       " 'inspire': 497,\n",
       " 'indeed': 498,\n",
       " 'quaid': 499,\n",
       " 'harris': 500,\n",
       " 'tvseries': 501,\n",
       " 'gunga': 502,\n",
       " 'xmen': 503,\n",
       " 'bitterness': 504,\n",
       " 'offthewall': 505,\n",
       " 'elegant': 506,\n",
       " 'squarely': 507,\n",
       " 'whimper': 508,\n",
       " 'ghastly': 509,\n",
       " 'nicky': 510,\n",
       " 'utterly': 511,\n",
       " 'outlaw': 512,\n",
       " 'champion': 513,\n",
       " 'rich': 514,\n",
       " 'adventurous': 515,\n",
       " 'cease': 516,\n",
       " 'kudos': 517,\n",
       " 'animate': 518,\n",
       " 'delightful': 519,\n",
       " 'prague': 520,\n",
       " 'offensive': 521,\n",
       " 'cocky': 522,\n",
       " 'mismatch': 523,\n",
       " 'perfection': 524,\n",
       " 'elisha': 525,\n",
       " 'regal': 526,\n",
       " 'meld': 527,\n",
       " 'reel': 528,\n",
       " 'rite': 529,\n",
       " 'hardwicke': 530,\n",
       " 'resonance': 531,\n",
       " 'nugget': 532,\n",
       " 'normally': 533,\n",
       " 'societal': 534,\n",
       " 'roommate': 535,\n",
       " 'vulgar': 536,\n",
       " 'bike': 537,\n",
       " 'lotr': 538,\n",
       " 'humorless': 539,\n",
       " 'typically': 540,\n",
       " 'dedication': 541,\n",
       " 'reality': 542,\n",
       " 'brosnan': 543,\n",
       " 'gage': 544,\n",
       " 'war': 545,\n",
       " 'hindi': 546,\n",
       " 'excruciate': 547,\n",
       " 'opposite': 548,\n",
       " 'olympic': 549,\n",
       " 'office': 550,\n",
       " 'orleans': 551,\n",
       " 'wash': 552,\n",
       " 'cycle': 553,\n",
       " 'existenz': 554,\n",
       " 'seclude': 555,\n",
       " 'interracial': 556,\n",
       " 'neville': 557,\n",
       " 'clumsily': 558,\n",
       " 'joy': 559,\n",
       " 'badly': 560,\n",
       " 'gratuitous': 561,\n",
       " 'bucket': 562,\n",
       " 'aside': 563,\n",
       " 'prank': 564,\n",
       " 'willie': 565,\n",
       " 'ninja': 566,\n",
       " 'sammy': 567,\n",
       " 'dismiss': 568,\n",
       " 'frequency': 569,\n",
       " 'wonderful': 570,\n",
       " 'hull': 571,\n",
       " 'bargain': 572,\n",
       " 'strategy': 573,\n",
       " 'shefali': 574,\n",
       " 'olive': 575,\n",
       " 'saloon': 576,\n",
       " 'unbiased': 577,\n",
       " 'ingmar': 578,\n",
       " 'annual': 579,\n",
       " 'architect': 580,\n",
       " 'mastery': 581,\n",
       " 'pronounce': 582,\n",
       " 'herman': 583,\n",
       " 'shady': 584,\n",
       " 'bettany': 585,\n",
       " 'transfer': 586,\n",
       " 'moronic': 587,\n",
       " 'timethe': 588,\n",
       " 'nude': 589,\n",
       " 'girlfriend': 590,\n",
       " 'door': 591,\n",
       " 'defender': 592,\n",
       " 'overdose': 593,\n",
       " 'runner': 594,\n",
       " 'poltergeist': 595,\n",
       " 'magically': 596,\n",
       " 'denver': 597,\n",
       " 'outwit': 598,\n",
       " 'darren': 599,\n",
       " 'loosely': 600,\n",
       " 'indication': 601,\n",
       " 'enslave': 602,\n",
       " 'humanity': 603,\n",
       " 'satisfaction': 604,\n",
       " 'frustration': 605,\n",
       " 'contrivance': 606,\n",
       " 'awry': 607,\n",
       " 'exhaust': 608,\n",
       " 'premise': 609,\n",
       " 'skull': 610,\n",
       " 'travis': 611,\n",
       " 'harp': 612,\n",
       " 'isle': 613,\n",
       " 'thieve': 614,\n",
       " 'tube': 615,\n",
       " 'everybody': 616,\n",
       " 'lear': 617,\n",
       " 'particularly': 618,\n",
       " 'masculinity': 619,\n",
       " 'redford': 620,\n",
       " 'ivory': 621,\n",
       " 'adapt': 622,\n",
       " 'dim': 623,\n",
       " 'animation': 624,\n",
       " 'ease': 625,\n",
       " 'horrific': 626,\n",
       " 'cagney': 627,\n",
       " 'blurt': 628,\n",
       " 'crash': 629,\n",
       " 'vacant': 630,\n",
       " 'dan': 631,\n",
       " 'cara': 632,\n",
       " 'pretentious': 633,\n",
       " 'toothe': 634,\n",
       " 'foist': 635,\n",
       " 'immaculate': 636,\n",
       " 'outdated': 637,\n",
       " 'wendy': 638,\n",
       " 'ancient': 639,\n",
       " 'balcony': 640,\n",
       " 'epiphany': 641,\n",
       " 'biopic': 642,\n",
       " 'lifethe': 643,\n",
       " 'appal': 644,\n",
       " 'fourth': 645,\n",
       " 'undermine': 646,\n",
       " 'switch': 647,\n",
       " 'closely': 648,\n",
       " 'rene': 649,\n",
       " 'marginal': 650,\n",
       " 'successfully': 651,\n",
       " 'businessmen': 652,\n",
       " 'currently': 653,\n",
       " 'frame': 654,\n",
       " 'vibrant': 655,\n",
       " 'battleship': 656,\n",
       " 'drawl': 657,\n",
       " 'mill': 658,\n",
       " 'vicious': 659,\n",
       " 'rap': 660,\n",
       " 'shoddy': 661,\n",
       " 'fascist': 662,\n",
       " 'benign': 663,\n",
       " 'quincy': 664,\n",
       " 'undo': 665,\n",
       " 'interrupt': 666,\n",
       " 'sykes': 667,\n",
       " 'cheezy': 668,\n",
       " 'undertake': 669,\n",
       " 'interaction': 670,\n",
       " 'activity': 671,\n",
       " 'cargo': 672,\n",
       " 'noel': 673,\n",
       " 'contestant': 674,\n",
       " 'watchable': 675,\n",
       " 'locate': 676,\n",
       " 'theresa': 677,\n",
       " 'egypt': 678,\n",
       " 'frederick': 679,\n",
       " 'service': 680,\n",
       " 'consolation': 681,\n",
       " 'retro': 682,\n",
       " 'hopper': 683,\n",
       " 'retreat': 684,\n",
       " 'clutter': 685,\n",
       " 'burstyn': 686,\n",
       " 'host': 687,\n",
       " 'hijinks': 688,\n",
       " 'misstep': 689,\n",
       " 'work': 690,\n",
       " 'classify': 691,\n",
       " 'wan': 692,\n",
       " 'blurb': 693,\n",
       " 'kapoor': 694,\n",
       " 'premiere': 695,\n",
       " 'dear': 696,\n",
       " 'fulfillment': 697,\n",
       " 'print': 698,\n",
       " 'mildred': 699,\n",
       " 'ooh': 700,\n",
       " 'phase': 701,\n",
       " 'dilemma': 702,\n",
       " 'firmly': 703,\n",
       " 'drive': 704,\n",
       " 'yikes': 705,\n",
       " 'focus': 706,\n",
       " 'hepburn': 707,\n",
       " 'union': 708,\n",
       " 'shoestring': 709,\n",
       " 'hip': 710,\n",
       " 'randomly': 711,\n",
       " 'explode': 712,\n",
       " 'construct': 713,\n",
       " 'boo': 714,\n",
       " 'marilyn': 715,\n",
       " 'unwilling': 716,\n",
       " 'miraculous': 717,\n",
       " 'hind': 718,\n",
       " 'apparent': 719,\n",
       " 'pod': 720,\n",
       " 'aplenty': 721,\n",
       " 'altman': 722,\n",
       " 'technician': 723,\n",
       " 'carry': 724,\n",
       " 'away': 725,\n",
       " 'interweave': 726,\n",
       " 'teddy': 727,\n",
       " 'spellbind': 728,\n",
       " 'semblance': 729,\n",
       " 'benny': 730,\n",
       " 'subtitle': 731,\n",
       " 'alibi': 732,\n",
       " 'cruel': 733,\n",
       " 'iranian': 734,\n",
       " 'bridge': 735,\n",
       " 'mushroom': 736,\n",
       " 'nausea': 737,\n",
       " 'stance': 738,\n",
       " 'counterpart': 739,\n",
       " 'texan': 740,\n",
       " 'cock': 741,\n",
       " 'seuss': 742,\n",
       " 'caesar': 743,\n",
       " 'woe': 744,\n",
       " 'apply': 745,\n",
       " 'frustrate': 746,\n",
       " 'bunch': 747,\n",
       " 'gradual': 748,\n",
       " 'vent': 749,\n",
       " 'leia': 750,\n",
       " 'shiver': 751,\n",
       " 'correspond': 752,\n",
       " 'deck': 753,\n",
       " 'stalin': 754,\n",
       " 'swank': 755,\n",
       " 'thati': 756,\n",
       " 'nonexistent': 757,\n",
       " 'coppola': 758,\n",
       " 'professor': 759,\n",
       " 'caricature': 760,\n",
       " 'gladiator': 761,\n",
       " 'bruno': 762,\n",
       " 'spunky': 763,\n",
       " 'leung': 764,\n",
       " 'lane': 765,\n",
       " 'cameo': 766,\n",
       " 'brendan': 767,\n",
       " 'remake': 768,\n",
       " 'sickeningly': 769,\n",
       " 'airplane': 770,\n",
       " 'standout': 771,\n",
       " 'devgan': 772,\n",
       " 'eastern': 773,\n",
       " 'teenager': 774,\n",
       " 'hawaii': 775,\n",
       " 'elisabeth': 776,\n",
       " 'audacity': 777,\n",
       " 'somewhere': 778,\n",
       " 'goodbye': 779,\n",
       " 'award': 780,\n",
       " 'apathetic': 781,\n",
       " 'morality': 782,\n",
       " 'precursor': 783,\n",
       " 'superb': 784,\n",
       " 'gel': 785,\n",
       " 'clarify': 786,\n",
       " 'hide': 787,\n",
       " 'invisibility': 788,\n",
       " 'hugo': 789,\n",
       " 'wade': 790,\n",
       " 'pre': 791,\n",
       " 'hilariously': 792,\n",
       " 'recluse': 793,\n",
       " 'eat': 794,\n",
       " 'armstrong': 795,\n",
       " 'historical': 796,\n",
       " 'tax': 797,\n",
       " 'peckinpah': 798,\n",
       " 'moses': 799,\n",
       " 'skate': 800,\n",
       " 'barrel': 801,\n",
       " 'daily': 802,\n",
       " 'performer': 803,\n",
       " 'michelle': 804,\n",
       " 'fringe': 805,\n",
       " 'unarm': 806,\n",
       " 'murky': 807,\n",
       " 'abortion': 808,\n",
       " 'sappy': 809,\n",
       " 'physical': 810,\n",
       " 'unbalance': 811,\n",
       " 'transformer': 812,\n",
       " 'imperial': 813,\n",
       " 'meryl': 814,\n",
       " 'idealistic': 815,\n",
       " 'narrow': 816,\n",
       " 'jackson': 817,\n",
       " 'glorify': 818,\n",
       " 'turn': 819,\n",
       " 'loyalty': 820,\n",
       " 'respectable': 821,\n",
       " 'donovan': 822,\n",
       " 'intercut': 823,\n",
       " 'crud': 824,\n",
       " 'investigative': 825,\n",
       " 'occupant': 826,\n",
       " 'hasnt': 827,\n",
       " 'rumble': 828,\n",
       " 'prominently': 829,\n",
       " 'isabel': 830,\n",
       " 'campbell': 831,\n",
       " 'corrupt': 832,\n",
       " 'snuff': 833,\n",
       " 'forgive': 834,\n",
       " 'moustache': 835,\n",
       " 'turner': 836,\n",
       " 'persuade': 837,\n",
       " 'console': 838,\n",
       " 'nikki': 839,\n",
       " 'dennis': 840,\n",
       " 'recently': 841,\n",
       " 'presley': 842,\n",
       " 'trainer': 843,\n",
       " 'perilous': 844,\n",
       " 'besides': 845,\n",
       " 'evocative': 846,\n",
       " 'eyecandy': 847,\n",
       " 'taboo': 848,\n",
       " 'frenetic': 849,\n",
       " 'slayer': 850,\n",
       " 'relevant': 851,\n",
       " 'announcement': 852,\n",
       " 'gloomy': 853,\n",
       " 'psychosis': 854,\n",
       " 'search': 855,\n",
       " 'specie': 856,\n",
       " 'malcolm': 857,\n",
       " 'beneath': 858,\n",
       " 'kirk': 859,\n",
       " 'expend': 860,\n",
       " 'wisdom': 861,\n",
       " 'refusal': 862,\n",
       " 'abrupt': 863,\n",
       " 'hindu': 864,\n",
       " 'bless': 865,\n",
       " 'featurelength': 866,\n",
       " 'crave': 867,\n",
       " 'mamet': 868,\n",
       " 'faint': 869,\n",
       " 'unappealing': 870,\n",
       " 'neo': 871,\n",
       " 'lori': 872,\n",
       " 'san': 873,\n",
       " 'prospective': 874,\n",
       " 'polite': 875,\n",
       " 'rereleased': 876,\n",
       " 'nationalist': 877,\n",
       " 'plausibility': 878,\n",
       " 'gold': 879,\n",
       " 'failure': 880,\n",
       " 'sweep': 881,\n",
       " 'saw': 882,\n",
       " 'rival': 883,\n",
       " 'cinematographer': 884,\n",
       " 'psychological': 885,\n",
       " 'dustin': 886,\n",
       " 'typical': 887,\n",
       " 'medicine': 888,\n",
       " 'flute': 889,\n",
       " 'hopkins': 890,\n",
       " 'industrial': 891,\n",
       " 'jaffe': 892,\n",
       " 'gandolfini': 893,\n",
       " 'help': 894,\n",
       " 'certificate': 895,\n",
       " 'conjure': 896,\n",
       " 'college': 897,\n",
       " 'cuteness': 898,\n",
       " 'closeup': 899,\n",
       " 'phyllis': 900,\n",
       " 'watch': 901,\n",
       " 'mumble': 902,\n",
       " 'crank': 903,\n",
       " 'exploitive': 904,\n",
       " 'peasant': 905,\n",
       " 'goo': 906,\n",
       " 'perceptive': 907,\n",
       " 'lovable': 908,\n",
       " 'label': 909,\n",
       " 'lull': 910,\n",
       " 'uninterested': 911,\n",
       " 'chairman': 912,\n",
       " 'cheesiness': 913,\n",
       " 'recommendation': 914,\n",
       " 'invasion': 915,\n",
       " 'prolong': 916,\n",
       " 'reprise': 917,\n",
       " 'restroom': 918,\n",
       " 'lily': 919,\n",
       " 'beach': 920,\n",
       " 'function': 921,\n",
       " 'slowmoving': 922,\n",
       " 'bailey': 923,\n",
       " 'overdramatic': 924,\n",
       " 'inherently': 925,\n",
       " 'coaster': 926,\n",
       " 'bullet': 927,\n",
       " 'flawless': 928,\n",
       " 'rhyme': 929,\n",
       " 'go': 930,\n",
       " 'slob': 931,\n",
       " 'scorsese': 932,\n",
       " 'nevertheless': 933,\n",
       " 'showi': 934,\n",
       " 'hopelessly': 935,\n",
       " 'face': 936,\n",
       " 'mann': 937,\n",
       " 'rosa': 938,\n",
       " 'sleazy': 939,\n",
       " 'occasional': 940,\n",
       " 'stunningly': 941,\n",
       " 'every': 942,\n",
       " 'bin': 943,\n",
       " 'professional': 944,\n",
       " 'fiona': 945,\n",
       " 'baldwin': 946,\n",
       " 'temple': 947,\n",
       " 'prey': 948,\n",
       " 'norman': 949,\n",
       " 'strongly': 950,\n",
       " 'coproduction': 951,\n",
       " 'seminal': 952,\n",
       " 'willard': 953,\n",
       " 'kristofferson': 954,\n",
       " 'shortly': 955,\n",
       " 'motley': 956,\n",
       " 'whenever': 957,\n",
       " 'crisp': 958,\n",
       " 'eddie': 959,\n",
       " 'baffle': 960,\n",
       " 'villainess': 961,\n",
       " 'nonactors': 962,\n",
       " 'screen': 963,\n",
       " 'inadequate': 964,\n",
       " 'commonplace': 965,\n",
       " 'zap': 966,\n",
       " 'fifth': 967,\n",
       " 'doorway': 968,\n",
       " 'amok': 969,\n",
       " 'mow': 970,\n",
       " 'clara': 971,\n",
       " 'heri': 972,\n",
       " 'popularity': 973,\n",
       " 'fragile': 974,\n",
       " 'scenery': 975,\n",
       " 'investigate': 976,\n",
       " 'unpretentious': 977,\n",
       " 'skip': 978,\n",
       " 'cottage': 979,\n",
       " 'motherdaughter': 980,\n",
       " 'spot': 981,\n",
       " 'gi': 982,\n",
       " 'awaken': 983,\n",
       " 'perhaps': 984,\n",
       " 'gypsy': 985,\n",
       " 'unify': 986,\n",
       " 'visceral': 987,\n",
       " 'era': 988,\n",
       " 'doodle': 989,\n",
       " 'vagina': 990,\n",
       " 'come': 991,\n",
       " 'robby': 992,\n",
       " 'feminine': 993,\n",
       " 'swoon': 994,\n",
       " 'waste': 995,\n",
       " 'tree': 996,\n",
       " 'cole': 997,\n",
       " 'disgustingly': 998,\n",
       " 'fantasy': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if labels fit\n",
    "index = random.randint(0, len(train_set))\n",
    "print(Y_train[index])\n",
    "print(train_set.loc[index, 'label'])\n",
    "\n",
    "print(len(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up genetic algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classifier\n",
    "There are a lot of classifiers. I haven't any experience in most of them, just a little familiar with Neural Network thus this is what I did choose.\n",
    "\n",
    "The acitecture:\n",
    "len(vector) input Relu() --> 64 nodes --> softmax() --> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, idfs, vocab):\n",
    "        self.idfs = idfs\n",
    "        self.vocab = vocab\n",
    "        self.ws_1 = 0\n",
    "        self.ws_2 = 0\n",
    "        self.ws_3 = 0\n",
    "        self.np_predictor = np.vectorize(lambda vec: self.predict(vec))\n",
    "        self.hn_1 = 0\n",
    "        self.hn_2 = 0\n",
    "        self.in_vector = 0\n",
    "\n",
    "    # Initializing weights\n",
    "    def init_weights(self):\n",
    "        self.ws_1 = np.random.rand(self.in_vector, self.hn_1) - 0.5\n",
    "        self.ws_2 = np.random.rand(self.hn_1, self.hn_2) - 0.5\n",
    "        self.ws_3 = np.random.rand(self.hn_2, 2) - 0.5\n",
    "        return self.ws_1, self.ws_2, self.ws_3\n",
    "        \n",
    "    def set_weights(self, ws_1, ws_2, ws_3):\n",
    "        self.ws_1 = ws_1\n",
    "        self.ws_2 = ws_2\n",
    "        self.ws_3 = ws_3\n",
    "    \n",
    "    def set_hidden_nodes(self, in_vector, hn_1, hn_2):\n",
    "        self.hn_1 = hn_1\n",
    "        self.hn_2 = hn_2\n",
    "        self.in_vector = in_vector\n",
    "\n",
    "    def relu(self, x):\n",
    "        return (x > 0) * x\n",
    "    \n",
    "    def relu_grad(self, x):\n",
    "        return x > 0\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        from scipy import special\n",
    "        return special.expit(x)\n",
    "    \n",
    "    def sigmoid_grad(self, x):\n",
    "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        import numpy as np\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "        \n",
    "    def train_with_SGD(self, epochs, lr, x, y):\n",
    "        error_list = list()\n",
    "        mse_list = list()\n",
    "        \n",
    "        for n in range(epochs):\n",
    "            for i in range(x.shape[0]):\n",
    "                l_in = x[i:i+1]\n",
    "                l_o, l_4, l_3, l_2, l_1, s_1 = self.predict(l_in, True)           \n",
    "                \n",
    "                delta_2 = l_2 - y[i]\n",
    "                delta_1 = delta_2.dot(self.ws_2.T) * self.relu_grad(l_1)\n",
    "                \n",
    "                self.ws_2 -= lr * (l_1.T.reshape(self.hn_1,1).dot(delta_2))\n",
    "                self.ws_1 -= lr * (l_in.T.reshape(x.shape[1],1).dot(delta_1))\n",
    "\n",
    "                error = delta_2**2\n",
    "                    \n",
    "                error_list.append(error[0][0])\n",
    "            \n",
    "            mse_list.append(sum(error_list) / x.shape[0])\n",
    "            error_list = list()\n",
    "        \n",
    "        cel, output, acc = self.predict_whole_set(x, y)\n",
    "        \n",
    "        return mse_list, cel, output, acc\n",
    "                \n",
    "\n",
    "    def cross_entropy(self, p, y):\n",
    "        import numpy as np\n",
    "        return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n",
    "    \n",
    "    def predict(self, x, get_all=False):\n",
    "        #forward pass/prediction\n",
    "        layer_1 = self.relu(x.dot(self.ws_1))\n",
    "        layer_2 = self.relu(layer_1.dot(self.ws_2))\n",
    "        layer_3 = layer_2.dot(self.ws_3)\n",
    "        layer_out = self.softmax(layer_3)\n",
    "        if get_all:\n",
    "            return layer_out, layer_3, layer_2, layer_1\n",
    "        else:\n",
    "            return layer_out\n",
    "    \n",
    "    def get_accuracy(self, y, p):\n",
    "        acc = np.sum((y == np.argmax(p, axis=1))) / len(y)\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_whole_set(self, x, y, get_acc = True):\n",
    "        output = np.apply_along_axis(self.predict, 1, x)\n",
    "        cel = self.cross_entropy(output, y)\n",
    "        if get_acc:\n",
    "            acc = self.get_accuracy(y, output)\n",
    "        if get_acc:\n",
    "            acc = self.get_accuracy(y, output)\n",
    "            return cel, output, acc\n",
    "        else:\n",
    "            return cel, output\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def training(num_epochs, model, optimizer, criterion, train_loader, valid_loader, vocab_length):\n",
    "    results = list()\n",
    "    accuracies = list()\n",
    "    valid_results = list()\n",
    "    valid_accuracies = list()\n",
    "\n",
    "    #model.float()\n",
    "    for ep in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        valid_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_valid = 0\n",
    "        correct_valid = 0\n",
    "        model.train() # Set model into training mode\n",
    "        for batch in train_loader:\n",
    "            # We extract the images and labels from the batch\n",
    "            vector = batch[:, :vocab_length]\n",
    "            labels = batch[:, vocab_length]\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            # This will prevent the gradient descents from the previous batches to accumulate. Without this the weight will get updated with the sum of\n",
    "            # all previos gradient descents, instead of the gradient descents on the current batch.\n",
    "            optimizer.zero_grad() \n",
    "            output = model(vector) # prediction / output from the model\n",
    "\n",
    "            loss = criterion(output, labels)  # We calculate the loss here\n",
    "            loss.backward() # Computes the derivative of the loss using backpropagation.\n",
    "            optimizer.step() # We update the weights\n",
    "\n",
    "            running_loss += loss.item() # Sum the loss here\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        else:\n",
    "            results.append(running_loss / len(train_loader))  # We append the mean loss into a list\n",
    "            accuracies.append(correct / total)\n",
    "    \n",
    "        # Validation part\n",
    "        model.eval()\n",
    "        with torch.no_grad(): # This will prevent calculation of gradient descents\n",
    "            for batch in valid_loader:\n",
    "                vector = batch[:, :vocab_length]\n",
    "                labels = batch[:, vocab_length]\n",
    "                valid_output = model(vector) # Prediciton\n",
    "                loss = criterion(valid_output, labels.long()) # Calculation of loss\n",
    "                valid_loss += loss.item() # Sum the loss\n",
    "\n",
    "                _, predicted = torch.max(valid_output.data, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "            else:\n",
    "                valid_results.append(valid_loss / len(valid_loader)) # Here we calculate the mean loss\n",
    "                valid_accuracies.append(correct_valid / total_valid)\n",
    "    return results, accuracies, valid_results, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00003\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-181-b33fb47361d0>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_torch = torch.tensor(train_torch, dtype=torch.float)\n",
      "<ipython-input-181-b33fb47361d0>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valid_torch = torch.tensor(valid_torch, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "train_torch = np.c_[X_train, Y_train]\n",
    "\n",
    "train_torch = torch.from_numpy(train_torch)\n",
    "train_torch = torch.tensor(train_torch, dtype=torch.float)\n",
    "\n",
    "valid_torch = np.c_[X_valid, Y_valid]\n",
    "\n",
    "valid_torch = torch.from_numpy(valid_torch)\n",
    "valid_torch = torch.tensor(valid_torch, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_torch, batch_size=1, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_torch, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(len(vocab), 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, 8),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(8,2)\n",
    "                     )\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-179-14579e24792d>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "sgd_results, accuracies, sgd_results_valid, accuracies_valid = training(num_epochs, model, optimizer, criterion, train_loader, valid_loader, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sgd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0, num_epochs, num_epochs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(x_axis, sgd_results, label='Training')\n",
    "ax.plot(x_axis, sgd_results_valid, label='Validation')\n",
    "ax.set_ylabel(\"Mean CEL\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.grid()\n",
    "\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "len(sgd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgd_results[0])\n",
    "print(sgd_results[-1])\n",
    "print(accuracies[0])\n",
    "print(accuracies[-1])\n",
    "print(sgd_results_valid[0])\n",
    "print(sgd_results_valid[-1])\n",
    "print(accuracies_valid[0])\n",
    "print(accuracies_valid[-1])\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "classifier = Classifier(idfs, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.049602179689973\n",
      "0.4968\n",
      "7364\n"
     ]
    }
   ],
   "source": [
    "classifier.set_hidden_nodes(len(vocab), 16, 8)\n",
    "classifier.init_weights()\n",
    "cel_pre, output, acc_pre = classifier.predict_whole_set(X_valid, Y_valid)\n",
    "print(cel_pre)\n",
    "print(acc_pre)\n",
    "\n",
    "print(sum(np.argmax(output, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Chromosome\n",
    "It's just some weights that'll be used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chromosome:\n",
    "    # x_pos and y_pos are the features of our chromosome\n",
    "    def __init__(self, ws_1, ws_2, ws_3):\n",
    "        self._fitness = 0\n",
    "        self._ws_1 = ws_1\n",
    "        self._ws_2 = ws_2\n",
    "        self._ws_3 = ws_3\n",
    "    \n",
    "    # Getters and setters\n",
    "    @property\n",
    "    def ws_1(self):\n",
    "        return self._ws_1\n",
    "    \n",
    "    @ws_1.setter\n",
    "    def ws_1(self, value):\n",
    "        self._ws_1 = value\n",
    "    \n",
    "    @property\n",
    "    def ws_2(self):\n",
    "        return self._ws_2\n",
    "    \n",
    "    @ws_2.setter\n",
    "    def ws_2(self, value):\n",
    "        self._ws_2 = value\n",
    "    \n",
    "    @property\n",
    "    def ws_3(self):\n",
    "        return self._ws_3\n",
    "    \n",
    "    @ws_3.setter\n",
    "    def ws_3(self, value):\n",
    "        self._ws_3 = value\n",
    "    \n",
    "    @property\n",
    "    def fitness(self):\n",
    "        return self._fitness\n",
    "    \n",
    "    @fitness.setter\n",
    "    def fitness(self, value):\n",
    "        self._fitness = value\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.fitness < other.fitness\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.fitness <= other.fitness\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.fitness == other.fitness\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return self.fitness != other.fitness\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.fitness >= other.fitness\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.fitness > other.fitness\n",
    "    \n",
    "    def assign_fitness(self, classifier, x, y):\n",
    "        import math\n",
    "        classifier.set_weights(self.ws_1, self.ws_2, self.ws_3)\n",
    "        loss, _, acc = classifier.predict_whole_set(x, y, True)\n",
    "        self.fitness = acc #0 if loss <= 0 or loss == float('inf') else -math.log(1 / loss)\n",
    "    \n",
    "    # produce a new offspring from 2 parents\n",
    "    def crossover(self, other):\n",
    "        r = 2\n",
    "        \n",
    "        min_mat_1 = np.minimum(self.ws_1, other.ws_1)\n",
    "        max_mat_1 = np.maximum(self.ws_1, other.ws_1)\n",
    "        min_mat_2 = np.minimum(self.ws_2, other.ws_2)\n",
    "        max_mat_2 = np.maximum(self.ws_2, other.ws_2)\n",
    "        min_mat_3 = np.minimum(self.ws_3, other.ws_3)\n",
    "        max_mat_3 = np.maximum(self.ws_3, other.ws_3)\n",
    "        \n",
    "        ws_1 = np.random.uniform(min_mat_1-r, max_mat_1+r)\n",
    "        ws_2 = np.random.uniform(min_mat_2-r, max_mat_2+r)\n",
    "        ws_3 = np.random.uniform(min_mat_3-r, max_mat_3+r) \n",
    "        \n",
    "        offspring = Chromosome(ws_1, ws_2, ws_3)\n",
    "        return offspring\n",
    "\n",
    "    # mutate the individual\n",
    "    def mutate(self):\n",
    "        np.random.shuffle(self.ws_1)\n",
    "        self.ws_1 = self.ws_1 + np.random.uniform(-5, 5, size=self.ws_1.shape)\n",
    "        np.random.shuffle(self.ws_2)\n",
    "        self.ws_2 = self.ws_2 + np.random.uniform(-5, 5, size=self.ws_2.shape)\n",
    "        np.random.shuffle(self.ws_3)\n",
    "        self.ws_3 = self.ws_3 + np.random.uniform(-5, 5, size=self.ws_3.shape)\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Genetic Algorithm Engine\n",
    "\n",
    "Here I have divided the population in two: elite and population.\n",
    "Elite contains the population with highest fitness. This is to make sure if we find a very fit chromosome we don't lose it again because of cross overs or mutations.\n",
    "\n",
    "And we make the mutations first. We make mutations on the least fit chromosomes. Mutations are here are nothing more than just randomly generating a new chromosome.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAEngine:\n",
    "    def __init__(self, classifier):\n",
    "        self._population = []\n",
    "        self._generations = 0\n",
    "        self._classifier = classifier\n",
    "        self.elite = []\n",
    "        self.nr_of_elites = 0\n",
    "        self.to_mutate = []\n",
    "\n",
    "    def make_initial_population(self, population_size):       \n",
    "        for i in range(population_size):\n",
    "            ws_1, ws_2, ws_3 = self.classifier.init_weights()\n",
    "            self.population.append(Chromosome(ws_1, ws_2, ws_3))\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def generations(self):\n",
    "        return self._generations\n",
    "    \n",
    "    @generations.setter\n",
    "    def generations(self, g):\n",
    "        self._generations = g\n",
    "    \n",
    "    @property\n",
    "    def population(self):\n",
    "        return self._population\n",
    "    \n",
    "    @population.setter\n",
    "    def population(self, p):\n",
    "        self._population = p\n",
    "    \n",
    "    @property\n",
    "    def classifier(self):\n",
    "        return self._classifier\n",
    "    \n",
    "    @classifier.setter\n",
    "    def classifier(self, cl):\n",
    "        self._classifier = cl\n",
    "    \n",
    "    # selection code goes here...\n",
    "    def do_crossover(self, elite_crossover_rate):\n",
    "        import random\n",
    "        population_size = len(self.population)\n",
    "        \n",
    "                \n",
    "        # Here we combine elitism selection with roulette wheel\n",
    "        # We carry some of the most fit over to the next generation.\n",
    "        # We do cross over with both the elite and other population\n",
    "        # Then we use roulette wheel because we want diversity too.\n",
    "        # We want diversity because it's hard to predict optimal weights\n",
    "        \n",
    "        no_of_elite_crossovers = int(elite_crossover_rate * population_size)\n",
    "        \n",
    "        other_offspring = population_size - no_of_elite_crossovers\n",
    "\n",
    "        new_generation = list()\n",
    "        \n",
    "        # Offsprings from the elite\n",
    "        for i in range(no_of_elite_crossovers):\n",
    "            parent1, parent2 = random.choices(self.elite, k=2)\n",
    "            offspring = parent1.crossover(parent2)\n",
    "            new_generation.append(offspring)\n",
    "        \n",
    "        # Weighted random choice\n",
    "        fitness_values = [x.fitness**2 for x in self.population]\n",
    "        \n",
    "        # Offsprings from other population\n",
    "        for i in range(other_offspring):\n",
    "            parent1, parent2 = random.choices(self.population, weights=fitness_values, k=2)\n",
    "            offspring = parent1.crossover(parent2)\n",
    "            new_generation.append(offspring)\n",
    "        \n",
    "        # The population is the new generation\n",
    "        self.population = new_generation\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def do_mutation(self, no_of_mutation, x, y):\n",
    "        for i in range(no_of_mutation):\n",
    "            self.population[-i-1].mutate()\n",
    "            self.population[-i-1].assign_fitness(self.classifier, x, y)\n",
    "    \n",
    "    \n",
    "    # fitness calculation goes here...\n",
    "    def assign_fitness(self, x, y):\n",
    "        for ch in self.population:\n",
    "            ch.assign_fitness(self.classifier, x, y)\n",
    "        self.population = sorted(self.population, reverse=True)\n",
    "        self.update_elite()\n",
    "        return\n",
    "    \n",
    "    def get_population(self):\n",
    "        return self.population\n",
    "    \n",
    "    def update_elite(self):\n",
    "        if len(self.elite) == 0:\n",
    "            self.elite = self.population[-self.nr_of_elites:]\n",
    "            self.elite = sorted(self.elite, reverse=True)\n",
    "            return\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < self.nr_of_elites and j < len(self.population):\n",
    "            if self.elite[i] < self.population[j]:\n",
    "                self.elite[i] = self.population[j]\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        self.elite = sorted(self.elite, reverse=True)\n",
    "\n",
    "    \n",
    "    def get_best_chromosome(self):\n",
    "        return max(self.elite)\n",
    "    \n",
    "    # TRAINING ROUTINE\n",
    "    def training_routine(self, init_population, nr_of_generations, \n",
    "                        nr_of_mutation, nr_of_elites, elite_crossover_rate, x, y):\n",
    "        self.make_initial_population(init_population)\n",
    "        self.generations = nr_of_generations\n",
    "        self.nr_of_elites = nr_of_elites\n",
    "        \n",
    "        cels = list()\n",
    "\n",
    "\n",
    "        for i in range(self.generations):\n",
    "            self.assign_fitness(x, y)\n",
    "            self.update_elite()\n",
    "            cels.append(self.elite[-1].fitness)\n",
    "            \n",
    "            self.do_mutation(nr_of_mutation, x, y)\n",
    "            self.update_elite()\n",
    "            \n",
    "            self.do_crossover(elite_crossover_rate)\n",
    "            \n",
    "            \n",
    "\n",
    "        # Assign fitness last time before getting the best chromosome\n",
    "        self.assign_fitness(x, y)\n",
    "        self.update_elite()\n",
    "        cels.append(self.get_best_chromosome().fitness)\n",
    "        return self.get_best_chromosome(), cels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Optimizing classifier with Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-bd99c7f08a9d>:77: RuntimeWarning: divide by zero encountered in log\n",
      "  return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n",
      "<ipython-input-94-bd99c7f08a9d>:77: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 59min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5005333333333334,\n",
       " 0.5022666666666666,\n",
       " 0.5032,\n",
       " 0.5061333333333333,\n",
       " 0.5101333333333333,\n",
       " 0.5104,\n",
       " 0.5112,\n",
       " 0.5114666666666666,\n",
       " 0.5136,\n",
       " 0.5136,\n",
       " 0.5136,\n",
       " 0.5136,\n",
       " 0.5186666666666667,\n",
       " 0.524,\n",
       " 0.528,\n",
       " 0.5292,\n",
       " 0.5318666666666667,\n",
       " 0.536,\n",
       " 0.5388,\n",
       " 0.5421333333333334,\n",
       " 0.5469333333333334,\n",
       " 0.5489333333333334,\n",
       " 0.5553333333333333,\n",
       " 0.5586666666666666,\n",
       " 0.5646666666666667,\n",
       " 0.5697333333333333,\n",
       " 0.5716,\n",
       " 0.5744,\n",
       " 0.58,\n",
       " 0.5821333333333333,\n",
       " 0.5857333333333333,\n",
       " 0.5870666666666666,\n",
       " 0.5912,\n",
       " 0.5949333333333333,\n",
       " 0.6001333333333333,\n",
       " 0.6045333333333334,\n",
       " 0.6064,\n",
       " 0.6088,\n",
       " 0.614,\n",
       " 0.6145333333333334,\n",
       " 0.6176,\n",
       " 0.6194666666666667,\n",
       " 0.6233333333333333,\n",
       " 0.6246666666666667,\n",
       " 0.6278666666666667,\n",
       " 0.6308,\n",
       " 0.6365333333333333,\n",
       " 0.6394666666666666,\n",
       " 0.6437333333333334,\n",
       " 0.6485333333333333,\n",
       " 0.6492,\n",
       " 0.6538666666666667,\n",
       " 0.6557333333333333,\n",
       " 0.6568,\n",
       " 0.6605333333333333,\n",
       " 0.6638666666666667,\n",
       " 0.6684,\n",
       " 0.67,\n",
       " 0.6741333333333334,\n",
       " 0.6772,\n",
       " 0.6794666666666667,\n",
       " 0.6826666666666666,\n",
       " 0.6864,\n",
       " 0.6873333333333334,\n",
       " 0.6913333333333334,\n",
       " 0.6917333333333333,\n",
       " 0.6937333333333333,\n",
       " 0.6958666666666666,\n",
       " 0.6973333333333334,\n",
       " 0.6982666666666667,\n",
       " 0.6990666666666666,\n",
       " 0.7014666666666667,\n",
       " 0.704,\n",
       " 0.7089333333333333,\n",
       " 0.7097333333333333,\n",
       " 0.7141333333333333,\n",
       " 0.7146666666666667,\n",
       " 0.7150666666666666,\n",
       " 0.7161333333333333,\n",
       " 0.7168,\n",
       " 0.7178666666666667,\n",
       " 0.7189333333333333,\n",
       " 0.7216,\n",
       " 0.7237333333333333,\n",
       " 0.7246666666666667,\n",
       " 0.7262666666666666,\n",
       " 0.7285333333333334,\n",
       " 0.7293333333333333,\n",
       " 0.7305333333333334,\n",
       " 0.7324,\n",
       " 0.7338666666666667,\n",
       " 0.7352,\n",
       " 0.7361333333333333,\n",
       " 0.7376,\n",
       " 0.7401333333333333,\n",
       " 0.7406666666666667,\n",
       " 0.7422666666666666,\n",
       " 0.7433333333333333,\n",
       " 0.7437333333333334,\n",
       " 0.7444,\n",
       " 0.7572]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Here I run the GA engine\n",
    "classifier = Classifier(idfs, vocab)\n",
    "classifier.set_hidden_nodes(len(vocab), 16, 8)\n",
    "\n",
    "ga = GAEngine(classifier)\n",
    "\n",
    "init_population = 100\n",
    "nr_of_generations = 100 \n",
    "nr_of_mutation = 50\n",
    "nr_of_elites = 50\n",
    "elite_crossover_rate = 0.8\n",
    "\n",
    "ch, cels = ga.training_routine(init_population, \n",
    "                               nr_of_generations, \n",
    "                               nr_of_mutation,\n",
    "                               nr_of_elites, \n",
    "                               elite_crossover_rate, \n",
    "                               X_valid, Y_valid)\n",
    "cels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print(\"pop\", ga.population[i].fitness)\n",
    "#     print(\"elite\", ga.elite[i].fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting increase of fitness in training\n",
    "x = np.linspace(0, len(cels), len(cels))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, cels, label='fitness')\n",
    "\n",
    "plt.xlabel(\"Generations\")\n",
    "plt.ylabel(\"fitness\")\n",
    "plt.title(\"Change of fitness (fitness = accuracy)\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-bd99c7f08a9d>:77: RuntimeWarning: divide by zero encountered in log\n",
      "  return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n",
      "<ipython-input-94-bd99c7f08a9d>:77: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-np.nan_to_num(np.eye(2)[y]*np.log(p))).mean() * 2\n",
      "C:\\Users\\abdka\\anaconda3\\envs\\testing\\lib\\site-packages\\numpy\\core\\_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "classifier.set_weights(ch.ws_1, ch.ws_2, ch.ws_3)\n",
    "cel_post, output_post, acc_post = classifier.predict_whole_set(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training:  0.4968\n",
      "Accuracy after training:  0.6753333333333333\n",
      "CEL before training 5.049602179689973\n",
      "CEL after training inf\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy before training: \", acc_pre)\n",
    "print(\"Accuracy after training: \", acc_post)\n",
    "print(\"CEL before training\", cel_pre)\n",
    "print(\"CEL after training\", cel_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(output_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-b4c45037dd8a>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  nparr3 = np.array([arr1, arr2, arr3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [3, 4],\n",
       "       [3, 4],\n",
       "       [3, 2],\n",
       "       [2, 1],\n",
       "       [9, 3]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = [1,2,3]\n",
    "arr2 = [4,5]\n",
    "arr3 = [7,8, 9,10]\n",
    "\n",
    "nparr3 = np.array([arr1, arr2, arr3])\n",
    "nparr = np.array([[1,2,3],[3,4,5],[4,5,6]])\n",
    "nparr2 = np.array([[1, 3],[3, 4],[3,4],[3,2],[2,1],[9,3]])\n",
    "\n",
    "np.argmax(nparr2, axis=1)\n",
    "nparr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa  uu\n",
      "aa  uu\n"
     ]
    }
   ],
   "source": [
    "string1 = \"aa \\x92 uu\"\n",
    "\n",
    "print(string1)\n",
    "\n",
    "text = re.sub(r'[\\\\x]', '', string1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
